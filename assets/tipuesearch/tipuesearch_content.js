var tipuesearch = {"pages": [{
    "title": "도커를 응용하기 위한 작동 원리 및 베이스들",
    "text": "써보면 느끼겠지만 도커는 정말 편리하다. 하지만, 구성환경이 복잡해질수록 커스텀 하기 복잡해지며 다루기 어려워진다. 그래서 단순히 사용하는 방법을 넘어 동작 원리를 이해할 필요가 있어 정리한다. 해당 내용을 옛날에 처음 봤을 때는 진짜 이게 뭔 소리지 했던 내용이다. 근데 계속해서 부딪히며 체감하다 보니 아 이게 그때 그거구나 하고 조금씩 쌓여 막상 지금 다시 이론을 보니 잘 이해가 되니 신기하기도 하다. 확실히 야생형이 습득하는데 최고인 거 같기도 하고 나한테 잘 맞는 것 같다. 단점은 시간도 좀 걸릴 수 있고 많은 시행착오가 필요한 거? 그래도 나중에 깨달았을 때의 희열이란… 말로 표현할 수 없지 않나 도커(Dcoekr)란? Accelerate how you build, share, and run applications 애플리케이션을 더 쉽게 배포, 공유, 실행 할 수 있도록 설계된 컨테이너 기반의 오픈소스 가상화 플랫폼이다. 여기서 컨테이너라는 개념이 핵심이다. 일반적으로 컨테이너를 떠올리면 여러 개의 운송 상품들을 컨테이너에 넣고 쉽게 운송하는게 떠오를 것이다. 마찬가지로 도커에서도 여러 개의 프로그램, 실행환경을 컨테이너로 추상화하고 동일한 인터페이스를 제공하여 프로그램의 배포 및 관리를 단순하게 해준다. 즉, 도커 컨테이너는 코드와 모든 종속성을 패키지화하여 응용 프로그램이 한 컴퓨팅 환경에서 다른 컴퓨팅 환경으로 빠르고 안정적으로 실행되게 된다. 이렇게 되면 환경으로부터 격리되어 인프라에 관계없이 동일하게 실행할 수 있다는 장점이 있다. 이 도커 컨테이너를 생성하기 위해서는 도커 이미지라는 것이 필요한데 도커 이미지는 코드, 런타임, 시스템 도구, 시스템 라이브러리 및 설정과 같은 응용 프로그램을 실행하는 데 필요한 모든 것을 포함하는 가볍고 독립적이며 실행 가능한 소프트웨어 패키지이다. 도커 이미지가 도커 엔진에 의해 실행되면 도커 컨테이너가 생성되고 해당 도커 컨테이너를 이용하여 프로그램이 실행된다.(스냅샷과 실행 인스턴스라고 생각하면 될 것 같다.) 도커 사용 흐름 도커의 기본적인 사용 흐름은 다음과 같다. 도커 클라이언트(CLI)에서 명령어 입력( ex)docker run moomin ) 클라이언트에서 도커 서버(Daemon)로 요청 서버에서 moomin이라는 이미지가 로컬에 cache 되어 있는지 확인 있으면 그 이미지를 이용해서 컨테이너를 생성하고 없으면 Docker Hub라는 곳에 가서 이미지를 가져와 로컬에 Cache로 보관. 생성된 컨테이너가 이미지에서 받은 설정이나 조건에 따라 프로그램 실행 도커와 기존 가상화 기술과의 차이 가상화 기술이 나오기 전에는 한대의 서버에 하나의 운영체제 하나의 프로그램만을 운영하며 남는 공간은 그대로 방치되었다. 안정적이지만 비효율적이라 할 수 있다. 하이퍼 바이저 기반의 가상화가 출현하면서 논리적으로 공간을 분할하여 VM(Virtual Machine)이라는 독립적인 가상 환경의 서버를 이용 가능하게 되었다. 여기서 하이퍼 바이저란 호스트 시스템에서 다수의 게스트 OS를 구동할 수 있게 하는 소프트웨어이다. 그리고 하드웨어를 가상화하면서 하드웨어와 각각의 VM을 모니터링하는 중간 관리자라고 할 수 있다. 하이퍼 바이저에는 네이티브 하이퍼 바이저와 호스트형 하이퍼 바이저가 있다. 네이티브 하이퍼 바이저 같은 경우 하이퍼 바이저가 하드웨어를 직접 제어하기 때문에 자원을 효율적으로 사용 가능하며, 별도의 호스트 OS가 없으므로 오버헤드가 적다. 하지만, 여러 하드웨어 드라이버를 세팅해야 하므로 설치가 어렵다는 단점이 있다. 호스트형 하이퍼 바이저는 일반적인 소프트웨어처럼 호스트 OS 위에서 실행되며, 하드웨어 자원을 VM 내부의 게스트 OS에 에뮬레이트 하는 방식으로 오버헤드가 크다. 하지만, 게스트 OS 종류(window, linux 등)에 대한 제약이 없고 구현이 다소 쉬워 일반적으로 많이 이용하는 방법이다. 하이퍼 바이저 기반의 VM 구조 하이퍼바이저에 구동되는 VM은 각 VM마다 독립된 가상 하드웨어 자원을 할당받는다. 논리적으로 분리되어 있어 한 VM에 오류가 발생해도 다른 VM에 영향이 가지 않고 각 VM마다 다른 OS를 설치할 수 있다는 장점이 있다. 도커도 이러한 가상화 기술에서 나온 컨테이너 가상화 기술이다. 컨테이너 가상화 기술 vs 기존 가상화 기술(VM) VM과 비교했을 때 컨테이너는 하이퍼바이저와 게스트 OS가 필요하지 않기 때문에 더 가볍다. 또한, 애플리케이션을 실행할 때 컨테이너 방식에서는 호스트 OS 위에 애플리케이션의 실행 패키지인 이미지를 배포하기만 하면 되는데, VM 같은 경우 애플리케이션을 실행하기 위해 VM을 띄우고 자원을 할당한 다음, 게스트 OS를 부팅하여 애플리케이션을 실행해야 해서 훨씬 무겁게 실행된다. 도커 컨테이너와 가상 머신의 공통점으로는 기본 하드웨어를 격리된 환경내에서 애플리케이션을 배치한다는 것이고 가장 큰 차이점은 격리된 환경을 얼마나 격리 시키는지이다. 도커 컨테이너에서 돌아가는 애플리케이션은 컨테이너로 격리되어 있지만 여전히 같은 호스트의 다른 컨테이너와 동일한 커널을 공유한다. 결과적으로, 컨테이너 내부에서 실행되는 프로세스는 호스트 시스템에서 확인할 수 있다. 또한, 컨테이너가 전체 OS를 내장할 필요가 없는 결과 매우 가볍다. 가상 머신 같은 경우 호스트 운영 체제 또는 하피어바이저와 독립되어 있다. 그래서 이 특정 VM만을 위한 커널을 부팅하고 운영체제 프로세스를 시작해야 하기 때문에 응용 프로그램만 포함하는 컨테이너보다 VM의 크기를 훨씬 크게 만든다. 해당 방법은 비교적 사용법이 간단하지만 매우 느리다. (가상 머신을 써본 사람은 알겠지만 진짜 너무 느려서 답답해 미친다..) 어떻게 컨테이너를 격리 시키는걸까? 컨테이너를 과연 어떻게 격리 시키는 걸까? 먼저 Linux에서 사용되는 cgroup과 namespace에 대해 알아봐야 된다. cgroup은 CPU 메모리, Network BandWidth, HD io 등 프로세스 그룹의 시스템 리소스 사용량을 관리하는 기술이다. 즉, 어떤 애플리케이션의 사용량이 너무 많다면 cgroup을 이용해 사용량을 제한할 수 있다. namespace는 하나의 시스템에서 프로세스를 격리시켜 별개의 독립된 공간을 사용하는 것처럼 격리된 환경을 제공하는 경량 프로세스 가상화 기술이다. 이러한 기술들을 이용하여 컨테이너를 격리시킨다. 여기서 궁금증이 하나 생길 수 있다. 우리가 사용하고 있는 Host OS는 보통 Mac OS나 window 일텐데 어떻게 리눅스 기술인 cgroup과 namespace가 사용될 수 있는가? 사실 도커 내부는 다음과 같이 되어있다. 도커가 돌아갈 때 우리의 환경은 Mac OS/windows지만 도커는 결국 리눅스 환경에서 돌아가고 있다. 그래서 리눅스 커널을 기반으로 cgroup과 namespace도 사용할 수 있기 때문에 컨테이너를 분리시킬 수 있게 된다. CLI에 docker version을 한번 쳐보자. 도커 엔진이 linux/arm64 기반으로 돌아가고 있는 걸 확인할 수 있다. 이미지로 컨테이너 만드는 과정 지금까지 도커 정의와 동작 방식에 대해서 알아봤다. 이미지를 이용하여 컨테이너를 생성한다고 했다. 어떻게 이미지로 컨테이너를 만드는지에 대해 알아보자. 도커 명령어 같은 경우는 검색하면 바로바로 나오니깐 자세하게 설명은 하지 않고 나올 때마다 잠깐잠깐씩 설명하겠습니다. 이미지는 프로그램을 실행하는데 필요한 모든 것(설정이나 종속성 등)을 가지고 있다. 여기서 말하는 필요한 모든 것이란 뭘까? 컨테이너가 시작될 때 실행되는 명령어 파일 스냅샷(파일 스냅샷은 디렉토리나 파일을 카피한 것으로 예를들어, 컨테이너에서 스프링 부트 서버를 실행하고 싶다면 jar 파일 스냅샷) 이미지로 컨테이너 만드는 순서 docker run 명령어를 이용해 도커 이미지를 실행하게 되면 먼저 컨테이너가 생성되고 도커 이미지 파일 스냅샷에 있는 파일을 컨테이너 하드 디스크에 넣어준다. 그 후, 이미지에서 가지고 있는 명령어를 이용해서 해당 파일을 실행시키게된다. 도커 이미지 생성 그렇다면 과연 컨테이너에 기반이 되는 이미지는 어떻게 만드는 걸까? Dockerfile 작성 Dockerfile이란 도커 이미지를 만들기 위한 설정 파일로 컨테이너가 어떻게 행동해야 되는지에 대한 설정들을 정의한 파일이다. Dockerfile에 입력된 것들을 도커 서버로 전달하기 위해 도커 클라이언트에서 docker build . 명령어를 입력하게 되면 이미지가 생성된다. 1. Dockerfile을 만들어보자 도커 파일은 도커 이미지를 만들기 위해 작성하는 것이므로 도커 이미지가 필요한 것이 무엇인지를 생각해 봐야 된다. 베이스 이미지를 명시 도커 이미지는 여러 개의 레이어로 되어 있는데 그중 베이스 이미지는 이 이미지의 기반이 되는 부분이다. 예를 들어, 프론트 도커 이미지를 만들기 위해서는 node, 서버 도커 이미지를 만들기 위해서는 jdk를 기반으로 해볼 수 있다. 추가적으로 필요한 파일을 다운 받기 위한 몇 가지 명령어를 명시 컨테이너 시작 시 실행될 명령어를 명시 기본적인 형식은 다음과 같다. # 베이스 이미지를 명시 FROM baseImage # 추가적으로 필요한 파일들을 다운로드 RUN command ... # 컨테이너 시작시 실행 될 명령어를 명시 CMD [ \"executable\" ] 2. 도커파일로 도커 이미지 만들기 위에서 만든 도커파일과 build 명령어( ex)docker build . )를 이용해 이미지를 생성해준다. build 명령어는 해당 디렉토리 내에서 dockerfile이라는 파일을 찾아서 도커 클라이언트에 전달시켜준다. 간단한 도커파일을 한번 빌드해보자. 빌드하는 과정을 보면 이미지를 생성하기 위해 중간중간 임시 컨테이너를 사용하고 삭제하는 걸 확인할 수 있다. FROM alpine CMD [ \"echo\", \"moomin2\" ] Step1에서 첫 번째 임시 컨테이너(ace17…) Step2에서 두 번째 임시 컨테이너(46884..) 를 사용한 후에 임시 컨테이너를 삭제 후 최종 컨테이너(d1f5c4…)를 이용해 이미지를 생성한다. 즉, 베이스 이미지로부터 다른 종속성이나 새로운 커맨드를 추가할 때 임시 컨테이너를 만든 후 그 컨테이너를 토대로 새로운 이미지를 만들고 그 임시 컨테이너는 삭제된다. 위에서 배운 걸로 간단히 node.js 애플리케이션 띄워보자 nodejs 앱은 이미 구현되어 있다고 가정하고 Dockerfile을 먼저 작성해 보자. FROM node:10 # 해당 애플리케이션과 관련된 별도의 폴더를 사용하기 위해 전환 WORKDIR /usr/src/app # package.json, 다른 파일들(server.js) 같은 파일들울 컨테이너안 /usr/src/app에 복사 COPY ./ ./ RUN npm install CMD [\"node\", \"server.js\"] 위처럼 작성 후 docker build -t app ./ 명령어를 이용해 이미지를 생성한다. 그리고 만들어진 이미지를 실행하기 위해서 docker run -p 49160:8080 app 명령어를 실행한다. 해당 옵션어로 명령어를 실행하게되면 다음 구조와 같이 컨테이너가 실행된다. 그 후 http://localhost:49160에서 잘 실행이 되면 끝. 재빌드 효율성을 높이는 방법이나 volume 같은 건 이번 포스트에서 다루지 않겠다. 일단 간단한 예제를 통해 알아봤고 조만간 복잡한 docker 환경(docker compose) + CI, CD 파이프라인을 구축해야 될 일이 있어서 시간이 나면 해당 포스트를 다뤄보려고 한다. 어쨌든, 이 뒤부터는 위의 내용에서 응용 레벨이기 때문에 위의 내용을 잘 이해했으면 충분히 잘할 수 있을 것이다! 참고: docker docs 따라하며 배우는 도커와 CI환경 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "infra docker virtualization study it infra",
    "url": "/study/it%20infra/2024-02-11-docker/"
  },{
    "title": "수백 개의 분산 서비스에서 Observability 시스템 구축하기",
    "text": "현대의 소프트웨어 기반 기술들이 가상화 및 추상화를 기반함으로써 많은 문제들을 해결할 수 있었지만 기술의 기반 환경이 점점 가상화, 추상화되고 있다는 점은 역으로 문제 발생 사이를 추적하기 더 어렵다는 문제점이 발생할 수 있다. 수시로 업데이트되는 MSA 상의 서비스와 의존관계, 동적으로 변경되는 인프라, 단일 요청을 처리함에도 불구하고 여러 개의 예측할 수 없는 네트워크 홉을 통과해야 하는 구조, 높은 Cardinality를 가진 지표 등은 기존의 일반적인 모니터링 기반의 문제 탐색을 어렵게 한다. 기본적인 시스템과 이미 경험한 장애 케이스를 탐지하는 수준의 모니터링을 넘어서 겪어 보지 못한 새로운 현상에 대한 가시성을 제공하고 원인에 대한 질문에 답할 수 있는 시스템을 Observability라고 한다. 좋은 로그가 뭘까? 로그란 무엇일까? 우리가 작성한 로직에 문제가 없다고 하더라도 외부 연계 시스템에서 잘못된 응답을 반환하거나 인프라 레벨의 문제가 발생해 로직이 우리의 의도대로 동작하지 않을 수 있다. 그렇기 때문에 로직을 작성하는 단계부터 로그를 잘 기록하도록 준비해두어 문제가 발생했을 때 우리의 시스템이 어떻게 동작했는지 기록으로 남기는 것은 중요하다. MSA 아키텍처의 도입 등으로 인해 요청이 분산되어 처리하면서 우리의 시스템이 어떻게 동작했는지 확인하는 것은 더욱 어려워졌다. 과연 로그를 잘 남긴다는 게 뭘까? 우선 Spring Web 의존성만 추가한 프로젝트를 기반으로 작성한 API를 호출했을 때 보이는 기본 로그는 다음과 같다. 로그가 기록된 시간, 로그 레벨, 프로세스 id, 요청 처리에 사용된 스레드, 로그를 남긴 코드 위치가 나오는 걸 볼 수 있다. 하지만, 여기서 새로운 API를 하나 더 추가하고 두 개의 API를 섞어 호출한다면 어느 API 호출에 의해 로그가 남았는지 알 수 없다. 이 문제를 해결하기 위해 로그를 남길 때 API 정보를 포함시켜 기록을 남길 수 있다. 또한, 사용자별 API 호출 횟수를 알고 싶으면 사용자 id가 로그에 포함되도록 하면 된다. 즉, 해당 요청을 처리할 때 필요한 맥락 정보를 로그에 추가하면 요청이 처리된 당시 시스템 상황을 이해할 때 도움이 된다. 또한, 에러 원인을 분석하고자 Stacktrace도 로그로 남기고 원인 분석에 사용할 수 있다. 하지만, Stacktrace 로그가 많이 남게 된다면 어느 요청에 의해 남게 된 Stacktrace 인지 알 기 어려울 수 있다. 예외가 발생했을 때를 대비해 요청당 식별자(Trace Id) 를 하나 발급해두고 모든 로그에 요청당 식별자를 남겨두면 상황을 이해하는데 도움이 된다. 이때, 데이터 분석가분께서 유저별로 경험하는 API 별 평균 응답 시간을 알기 위해 데이터를 추출할 수 있는지 물어보신다면? API와 사용자 Id를 로그에 남겨두었으니 쉽게 할 수 있을 거 같지만 로그를 파싱 하려고 저장된 로그를 살펴보니 기록하는 순서에 일관성이 없다. 이를 위해 로그를 정형화해서 저장해두는 것이 필요하다. 로그가 JSON 형식으로 정형화되어 저장되므로 로그에 추가되는 항목이나 기록되는 항목의 순서가 변경되더라도 걱정할 필요가 없다. 또한, JSON 형식은 널리 사용되는 데이터 형식이므로 외부 도구와 연계하여 로그 검색 시스템을 만드는 것도 용이하다. 어떻게 분산 추적을 할 수 있을까? MSA 환경에서는 어떻게 로그를 잘 남길 수 있을까? 사용자가 App을 통해 거래 내역을 조회하는 상황을 가정해 보자. 위에서 말한 것처럼 로그가 JSON 형태로 정형화되어 있고, 같은 맥락의 로그는 동일한 Trace Id로 추적, User 서버는 UserId, Payment 서버는 OrderId를 로그에 남겨두는 등 좋은 로그의 형태처럼 보이지만 MSA 환경에서는 그렇지 않다. 기술적 관점에서 보면 MSA 환경에서의 각 서버 간 호출이 여러 개의 개별 요청으로 해석될 수 있지만, 비즈니스 관점에서 보면 여러 개의 서버 간 호출이 하나의 요청으로 해석되는 것이 좋은 경우도 있다. 예를 들어, “User 서버 담당자가 Payment Server 담당자에게 사용자 Id 37인 고객이 몇 시쯤 거래 내역을 조회했다가 실패했다고 하는데 order Id는 모른다고 합니다. 문제의 원인을 알 수 있을까요?“ 라는 문의가 오면 답변할 수 있을까? Payment 서버 담당자는 사용자 Id를 모르기 때문에 시간 정보만 가지고 문제가 된 요청을 추적할 수밖에 없다. 만일 App, User, Payment 서버 모두가 Trace Id가 “A”라는 Trace Id를 기록해두었으면 어떨까? A라는 Trace Id로 전체 로그를 검색해 보면 Payment 서버 담당자도 사용자 Id를 알 수 있게 되므로 문제를 더 정확히 이해하고 대응하는 것이 가능하다. 이를 위해 서로 다른 서버 간 요청을 보낼 때 Trace Id 정보를 함께 보내주면 된다. 이처럼 비즈니스 관점에서 하나로 해석되면 좋을 요청에 대해 서로 다른 애플리케이션이 같은 Trace Id로 로그를 남기게 된다면 애플리케이션 간에 분산된 호출 흐름을 추적할 수 있게 되는데 이를 분산 추적이라고 한다. 이런 과정을 거쳐 서로 다른 요청이 모두 동일한 “A”라는 Trace Id를 가지게 되고 이러한 로그를 하나의 저장소로 모으는 중앙 집중식 로깅 체계를 만들고 나면, 이전에 답할 수 없었던 “사용자 Id 37인 고객이 몇 시쯤 거래 내역을 조회했다가 실패했다고 하는데 원인을 할 수 있을까요?’와 같은 문의에 답을 할 수 있게 된다. 또한, 하나의 Id로 서비스 간 의존관계 및 각 서비스의 처리 시간을 알 수 있게 되므로 그림과 같이 시각화하여 요청 처리가 실패한 지점이나 성능 병목 구간을 찾는 것도 가능해진다. 토스페이먼츠는 분산 추적을 어떻게 확장하여 사용하고 있을까? 1. GlobalTrace Id 사용 토스 페이먼츠는 Trace Id보다 한 단계 더 상위에 해당하는 GlobalTrace Id를 정의해서 사용한다. 동일한 Trace Id를 서로 다른 서비스가 공유하게 된다면 비즈니스 관점에서의 하나의 요청을 처리하기 위한 전체 맥락을 이해하는 데 도움이 된다. 하지만, 우리는 비즈니스 관점에서 하나의 요청이 아닌 하나의 사용자 시나리오 전체를 이해해야 하는 상황을 자주 맞이한다. 예를 들어, ‘결제 완료 확인 화면’과 관련하여 문제가 발생했는데 ‘결제 정보 확인 화면’에서부터 문제가 발생한 것으로 예상되는 경우 서비스 화면 전환 단계 전체를 엮어줄 수 있는 Trace Id가 없어 로그를 하나의 id로 검색할 수 없게 되어 빠르게 문제를 확인하는 것이 어렵다. 그래서 서비스 화면 전환 단계 전체를 엮어줄 GlobalTrace Id를 정의하고 전파하고 있다. 2. 추적 문맥 전파 항목 추가 GlobalTrace Id, Tracee Id 외에도 API를 호출한 Client 버전 API를 호출한 Service 명 API를 호출한 Service 버전 API 처리와 관련된 고객사 API 처리와 관련된 원천사 등 추적에 도움이 되는 다양한 정보들을 함께 전파하고 있다. 그 결과 “지금 결제 실패가 자주 발생하는 것 같은데 문제가 있을까요?”라는 질문이 들어오면 “네, A 금융사와 관련된 API 처리 실패율이 다수의 서비스에서 높게 확인됩니다. 금융사에 확인 요청은 해두었고, 기술 지원팀이 고객사에 안내를 하고 있습니다”와 같이 현재 시스템 상황을 잘 이해한 상태로 답변을 할 수 있다. 3. 추적 범위 확장 분산 추적의 범위를 MSA를 구성하는 서비스들에만 국한하지 않는다. 실제 서비스 환경은 MSA를 구성하는 서비스 이외에도 다양한 인프라 구성 요소들로 이루어져 있기 때문에 시스템에 대한 전체적인 가시성을 확보하기 위해 CDN, 방화벽, Load Balancer, Istio Gateway, Isto SideCar, DB에 이르기까지 Trace Id만 있다면 전 구간의 로그를 찾아볼 수 있도록 구성해야 한다. DB나 TCP 서버와 같이 HTTP 헤더를 넣을 수 없는 구성 요소와의 통신은 어떻게 추적 문맥 전파를 할 수 있을까? /* 서비스명 | globalTraceId | traceId | spanId | ... */ SELECT * FROM 테이블명 WHERE ... DB의 경우 쿼리의 주석 부분에 추적 문맥을 포함시키면 Trace Id를 전파할 수 있다. TCP 프로토콜 자체는 요청 본문을 변경하지 않고 추가 정보를 보낼 수 있는 방법이 없기 때문에 추적 문맥 전파가 어렵다. 하지만, L4 Load Balancer에서 TCP 요청 본문을 변경하지 않고 클라이언트 정보를 보존하여 전달하는 방법이 있다. 해결 방법을 보기 전 L7 Load Balancer를 한번 살펴보자 위와 같은 구조에서는 L7 Load Balancer 뒤에 있는 서버에 접속하는 클라이언트가 실제 클라이언트가 아닌 L7 Load Balancer이기 때문에 서버는 실제 클라이언트의 IP를 알 수 없는 문제가 생긴다. 이 문제를 해결하기 위해 L7 Load Balancer는 자신에게 접속한 클라이언트 IP를 X-Forwarded-For와 같은 HTTP 헤더에 담아 서버에 전달하여 서버가 실제 클라이언트 IP를 알 수 있게 해준다. 하지만, L4 Load Balancer에게 HTTP 프로토콜은 이해할 수 없는 바이트 덩어리이다. 그래서 X-Forwarded-For와 같은 추가적인 정보를 추가하지 못한다. 그래서 원래 보내고자 했던 TCP 요청 본문을 그대로 유지하고 제일 앞줄에 ‘PROXY’라는 단어와 함께 클라이언트 IP를 포함하여 전달하기로 약속한 Proxy Protocol이라는 규격을 만들어 요청 본문에 변경을 가하지 않고 추가적인 정보를 보낼 수 있다. 이 아이디어를 활용해 위처럼 TCP 본문의 첫 줄에 추적 문맥 정보를 심고 요청을 받는 서버가 수신되는 데이터를 규칙에 맞게 파싱 하게 하면 된다. 4. Trace Id를 클라이언트로부터 생성 프론트엔드 로직은 서버와 통신을 시작하기 전부터 로직이 실행될 수 있고 경우에 따라 서버와의 통신 이전에 사용자의 인터랙션이 발생할 수 있기 때문에 문제가 발생한 당시의 문맥을 이해하기 위해서는 문맥을 이어줄 Trace Id가 미리 발급되어 있어야 한다. 토스 페이먼츠의 프론트엔드 제품군들은 서비스 품질 및 상태 모니터링을 위해 다양한 지표를 서버로 전송한다. Trace Id를 클라이언트에서 미리 생성한 결과 사용자가 경험한 웹의 성능 지표, 크래시 발생 정보, 서버와의 통신 이력을 일관된 방식으로 확인 가능하다. 5. 분석 시스템과의 연계 토스 페이먼츠는 다양한 서비스 분석 시스템과 연계하고 있다. 에러 추적으로는 Sentry, APM(Application Perfomance Management)으로는 PinPoint를 사용하고 있다. Sentry의 경우 tag라는 기능을 통해 GlobalTrace Id, Trace Id 등을 넣어 검색이 가능하다. Pinpoint의 경우 Pinpoint Transaction Id를 MDC(Mapped Diagnostic Context)에 pTx Id라는 키로 노출하며 Pinpoint에서 성능 문제를 검색할 수 있도록 하고 있다. FIN. 현재 토스와 같은 Observability 시스템을 구축하기 위해서는 하나의팀, 분야뿐 아니라 회사 전체적으로 협력이 있었어야 될 것 같은데 얼마나 토스가 잘 협력하고 관리되고 있는지 알 수 있는 컨퍼런스 영상인 것 같다. 프로젝트에서 간단하게 모니터링 시스템을 구축해 보면서 만약 로그가 복잡하게 되면 어떤 게 추가될 수 있고 여러 개의 분산 서버에서는 로그를 어떻게 관리할 수 있는지에 대해 궁금한 점이 많았었는데 많이 해결되었다 🙇🏻‍♂️ 요즘 새로운 프로젝트를 하고 있는데 해당 프로젝트는 장기간 유지 보수 및 운영해 보며 다양한 문제 상황을 경험하고 해결해 보고 싶다. 그 과정에서 오늘 포스트한 내용도 적용해 볼 수 있지 않을까. 참고: 분산 추적 체계 &amp; 로그 중심으로 Observability 확보하기 *오타가 있거나 피드백 주실 부분이 있으면 편하게 말씀해 주세요.",
    "tags": "seminar slash-23 sobeservability Distributed-Tracking log etc",
    "url": "/etc/seminar/2023-12-28-observability/"
  },{
    "title": "도메인 완전성과 순수성(+도메인 모델과 영속 모델 분리)",
    "text": "우선 도메인의 완전성과 순수성에 대해 얘기하기 전에 빈약한 도메인 모델과 풍부한 도메인 모델에 대해 살펴보자. 빈약한, 풍부한 도메인 모델 public class User { private Long id; private String name; private List&lt;Item&gt; items; } public class Item { private Long id; private String name; private int price; } public class UserService { ... public int calculateTotalPrice(long id) { User user = userRepository.findById(id); List&lt;Item&gt; items = user.getItems(); int sum = 0; for (Item item : items) { sum += item.getPrice(); } return sum; } ... } 빈약한 도메인 모델은 상태를 표현하는 필드와 getter, setter 메서드만 존재하고 비즈니스 로직이 서비스 계층에 존재하게 된다. 위와 같은 같은 객체를 제대로 된 객체라고 할 수 있을까? 객체지향적인 개발이라 하면 객체들이 속성을 가지고 있음을 넘어서 서로 상호작용하며 메시지를 주고받을 수 있어야 된다. 해당 도메인 객체에 대한 비즈니스 로직이 있으면 서비스 레이어가 아닌 해당 도메인에 작성되고 서비스 레이어에서는 여러 객체가 조합되거나 외부 의존성과 연동되는 로직이 작성된다. 이렇게 함으로써 좀 더 객체지향적인 개발과 도메인 응집도를 높일 수 있다. public class User { private Long id; private String name; private List&lt;Item&gt; items; public int calculateTotalPrice() { int sum = 0; for (Item item : items) { sum += item.getPrice(); } return sum; } } public class Item { private Long id; private String name; private int price; } 도메인 모델 완전성 vs 도메인 모델 순수성 public class User { ... public Result ChangeEmail(string newEmail, UserRepository repository) { if (Company.IsEmailCorporate(newEmail) == false) return Result.Failure(\"Incorrect email domain\"); User existingUser = repository.GetByEmail(newEmail); if (existingUser != null &amp;&amp; existingUser != this) return Result.Failure(\"Email is already taken\"); Email = newEmail; return Result.Success(); } ... } public class UserController { ... public string ChangeEmail(int userId, string newEmail) { User user = _userRepository.GetById(userId); Result result = user.ChangeEmail(newEmail, _userRepository); if (result.IsFailure) return result.Error; _userRepository.Save(user); return \"OK\"; } ... } 완전한 도메인 모델은 모든 비즈니스 논리가 도메인 클래스에 위치한다. (외부 의존성과 상호작용 논리조차도) 즉, 도메인 논리 단편화가 존재하지 않아 도메인 논리가 도메인 계층 이외에는 존재하지 않는다. 하지만, 이 경우 도메인 모델 순수성을 희생한다. 순수성이라는 것은 도메인 클래스는 원시 유형 또는 다른 도메인 클래스에만 의존해야 한다는 것이다. 위 코드를 순수성을 유지하도록 변경하면 다음과 같이 변경할 수 있을 것이다. public class UserController { ... public string ChangeEmail(int userId, string newEmail) { /* validation */ User existingUser = _userRepository.GetByEmail(newEmail); if (existingUser != null &amp;&amp; existingUser.Id != userId) return \"Email is already taken\"; User user = _userRepository.GetById(userId); Result result = user.ChangeEmail(newEmail); if (result.IsFailure) return result.Error; _userRepository.Save(user); return \"OK\"; } ... } 그러나 이 경우는 도메인 논리 단편화가 발생한다. 도메인 계층에 모든 비즈니스 로직이 존재하지 않는다는 것이다. 이처럼 완성도와 순수성 두 가지를 동시에 가질 수 없다. 하지만, 위 두 가지를 가질 수 있는 방법이 있다. 바로 성능을 포기하는 것이다. public class User { ... public Result ChangeEmail(string newEmail, User[] allUsers) { if (Company.IsEmailCorporate(newEmail) == false) return Result.Failure(\"Incorrect email domain\"); bool emailIsTaken = allUsers.Any(x =&gt; x.Email == newEmail &amp;&amp; x != this); if (emailIsTaken) return Result.Failure(\"Email is already taken\"); Email = newEmail; return Result.Success(); } ... } public class UserController { ... public string ChangeEmail(int userId, string newEmail) { User[] allUsers = _userRepository.GetAll(); User user = allUsers.Single(x =&gt; x.Id == userId); Result result = user.ChangeEmail(newEmail, allUsers); if (result.IsFailure) return result.Error; _userRepository.Save(user); return \"OK\"; } ... } 파라미터에 모든 User를 넣어줘 순수성을 보장해 주었다. 위 코드는 순수성과 완전성을 둘 다 가진다고 할 수 있지만 성능 관점에서는 실용적이라 할 수 없다. 결국 완전성, 순수성, 성능 세 마리 토끼를 모두 잡을 수는 없고 어느 하나를 포기하는 선택을 해야 한다. 완전성, 순수성(성능 희생) 완존성, 성능(순수성 희생) 성능, 순수성(완전성 희생) 성능을 희생하고 완전성, 순수성을 보장해 주기 위해서는 외부에서 읽은 모든 객체를 파라미터로 넣어주면 된다. 하지만, 이 방법 같은 경우 비교하려는 모든 데이터를 애플리케이션에 로딩한다는 건데 실용성이 너무 없다. 그렇기 때문에 순수성을 희생하는 두 번째 옵션과 완전성을 희생하는 세 번째 옵션 중 택할 수 있다. 외부 종속성을 도메인에 주입하여 순수성을 희생시키는 비용과 도메인 계층과 컨트롤러 사이에 의사결정 프로세스를 분할하여 완전성을 희생시키는 비용을 생각해 보면 순수성보다는 완전성을 희생하는 것이 좋다. 논리 단편화로 응집도가 조금 떨어지는 것보다 도메인에 외부 종속성이 추가되는 것이 복잡성이 훨씬 커지기 때문이다. 도메인에 외부 종속성이 있을 때 단위 테스트는 과연 어떻게 해야 될까 생각해 보자. 도메인 모델과 영속 모델 분리 도메인 순수성이 중요하다는 건 알겠다. 근데 이런 생각이 들었다. JPA를 사용하게 되면 일반적으로 JPA Entity을 통합적으로 사용하게 되는데 이 경우 도메인이 순수하다고 할 수 있을까? (결국 JPA를 사용 안 하게 되면 달았던 애노테이션 부분을 다 지워야 될 테니깐) 그래서 JPA를 사용하면서 순수성을 보장하려면 도메인 모델과 영속 모델이 분리되어야 될 것 같은데 이게 이전에 읽었었던 만들면서 배우는 클린 아키텍쳐의 맥락과 일맥상통할 것 같다. (예제 코드 참고) 하지만, 이렇게 사용하게 될 경우 도메인, 영속 모델을 같이 관리해야 하는 오버헤드도 크고 연관관계가 복잡한 경우 영속화하는 과정도 복잡하다. 그렇기 때문에 트레이드오프를 잘 고려하고 사용하자. 개인적으로는 해당 방법으로 한번 프로젝트를 진행해 보고 싶긴 한데 잘 엄두가 안 난다. 나중에 영속 매커니즘이 유연해야 하는 시스템에서 사용해 볼 수 있을 것 같은데 그게 언제지…? 앞으로 계속 고민해 볼 숙제가 생겼다. 개발이란 그런 것 같다. 새로운 것을 공부할 때 이전 기억과 퍼즐이 맞춰져서 매우 짜릿한 동시에 또 새로운 숙제거리가 주어진다 ㅋㅋㅋ 그래도 이게 개발의 매력이 아닐까 참고: Domain model purity vs. domain model completeness (DDD Trilemma) *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java domain integrity purity etc",
    "url": "/etc/java/2023-12-16-Domain/"
  },{
    "title": "여러 서비스에서 트랜잭션을 어떻게 보장해줄 수 있을까 (2)",
    "text": "이전 글에서 외부 서비스로부터 영향 범위를 최소화하기 위한 방법들에 대해 알아봤다. 이번에는 여러 서비스에서 트랜잭션을 어떻게 보장해 줄 수 있는지에 대해 알아보자. 여러 서비스에서의 트랜잭션 구매자가 어떤 상품을 주문을 했는데 주문은 성공적으로 주문되었지만 상품은 제대로 차감이 되지 않았다면? 상품 서비스에서는 예외가 터져서 처리가 될 것인데 현재 서비스가 분리되어 있기 때문에 주문 쪽은 상품 서비스가 성공을 했는지 실패를 했는지 모른다. 그래서 실패를 한 경우 주문은 성공하고 상품은 차감되지 않아 데이터의 정합성이 맞지 않는 경우가 발생할 수 있다. 해결 방법으로 Saga 패턴 같은 글로벌 트랜잭션을 다루는 방법들을 사용할 수 있다. 하지만, 이번 글에서는 어떤 네트워크 예외적 상황이 있을 수 있고 이를 코드 레벨에서 어떻게 다룰 수 있을지 살펴보자. Saga 패턴: 마이크로 서비스 간 이벤트를 주고받아 특정 마이크로 서비스에서 작업이 실패하면, 이전까지의 작업이 완료된 마이크로 서비스에 보상 이벤트를 발행함으로써 원자성을 보장하는 패턴 다양한 네트워크 예외와 예외 처리 네트워크 통신을 하다 보면 다음과 같은 예외 상황이 있을 수 있다. 같은 요청이 짧은 시간에 두 번 이상 발생 네트워크 순서가 뒤집힌 경우(취소 요청 후 결제 요청) 각종 타임아웃 인프라 문제로 실패 위와 같은 이유들로 다음과 같은 상황이 발생할 수 있다. 여러 번 결제 요청 발생 결제 취소 요청이 성공 요청보다 먼저 오는 경우 결제가 실패했는지 성공했는지 알 수 없는 경우 결제가 실패됐는데 기록이 없는 경우 API를 요청하면 성공, 실패, 알 수 없음 같은 3가지 결과를 받을 수 있고 각 상태에 따른 적절한 처리가 필요하다. 성공과 실패는 비교적 명확하다. 성공은 로직 그대로 수행하면 되고 실패하면 그전 상황들을 롤백 하면 된다. 하지만, 여기서 어려운 부분은 바로 알 수 없음인데 성공을 했는지 실패를 했는지 판단하기 어려운 경우이다. 주문을 통해 결제를 하는 서비스가 있다고 생각해 보자. 만약, 주문 서비스에서 결제 요청을 하고 성공적으로 결제가 되었는데 주문 서비스로 통신이 오던 도중 타임아웃이 나면? 혹은 결제에 실패하였는데 타임아웃이 나면? 만약 단순하게 다 실패하는 쪽으로 처리 하게 되면 큰 문제가 발생할 수 있다. 결제 서비스에서 요청을 받아 결제에 성공하고 잔액이 차감되었지만 주문 서비스로 가던 도중 타임아웃이 발생했고 이를 실패하는 쪽으로 처리하기로 했으므로 주문을 취소하게 된다. 이렇게 되면 돈은 계좌에서 차감되었지만 주문은 들어가지 않은 상황이 발생한다. 알 수 없음 같은 상황들은 다음과 같은 후처리를 통해 보정해볼 수 있다. 즉시 재요청 시도 일정 시간 뒤 재시도 결제 취소 요청 (트랜잭션 무효화 요청) 무조건 성공 후 뒤처리 근데 이런 후처리 또한 API를 요청하는 것이기 때문에 똑같이 알 수 없음으로 결과가 나올 수도 있다. 그니깐 재시도를 보냈는데 또 알 수 없음으로 와서 재시도의 재시도의 재시도..? 같은 무한 루프가 반복될 수 있다. 카카오페이에서는 다음과 같이 처리하고 있다고 한다. 처음 요청의 예외상황까지는 고객 응답이 나가기 전에 후처리를 진행해보지만, 이것마저 실패가 되면 '알 수 없는 상태'로 저장 후 사용자에게 재시도 안내와 함께 응답을하게 됩니다. 그리고 고객에 의해 재시도하게 되면, 해당 결제건이 알 수 없음으로 저장된 트랜잭션인지 확인하고 후처리를 다시 진행하게 됩니다. 혹여나 이마저도 실패하는 경우가 있다면 다른 장치들로 데이터를 보정하고 있습니다. 한 번까지 후처리를 하고 그 뒤로는 트랜잭션을 종료하는 것 같다. 여기서 궁금한 점이 생겼었다. 만약 결제가 완료된 상태고 한 번 더 알 수 없음으로 오면 트랜잭션이 종료될 텐데 이대로 종료를 해도 되는 건가? 재시도를 보낸다고 하지만 고객이 재시도를 하지 않으면 결제만 빠져나가는 게 아닌가 생각이 들었다. 또한, 재시도를 했을 때 또 실패하면 어떻게 보정해 주는지도 궁금했는데 그 내용은 나와 있지 않았고 검색을 해도 찾을 수 없었다 🥹 그래서 한참 끙끙 앓다가 눈우(feat.토스개발자님..)에게 HELP를 쳤더니 한방에 해결해 주었다. 위와 같이 의문을 던졌더니 추후에 배치로 맞추는 것 같다고 했다. 그때 딱 머릿속에 지나간 영상이 있었는데 해당 영상에서 주문 및 체결 정합성이 틀어질 경우 대사 배치에서 잡히게 되고 이는 별도 오퍼레이션에 의해...부분이 딱 생각나서 소름 돋았다. 정합성이 틀어지면 추후에 배치에서 잡히게 되고 후보정이 들어간다. 근데 여기서 또 궁금점이 하나 더 생긴 게 만약 추후에 배치로 맞춰서 보상해 준다고 하면 그때 당시에는 고객 계좌에 바로 돈이 들어가지 않아 고객이 당황하지 않을까? 했었는데 가지고 있는 여유금으로 해결할 수 있다고 한다. 일단 여유금으로 해결을 하고 후에 보정을 하는 그런 방식인 것 같다. 그래서 거래 금액이 커서 여유금이 많이 필요하면 배치 시간 간격을 줄인다고 한다. 멱등성 API 위에서 알 수 없는 상황이면 후처리 API를 요청한다고 하였다. 만약에, 주문 서비스에서 결제 서비스에 결제 요청을 하고 성공적으로 결제가 되었는데 타임아웃으로 인해 알 수 없음으로 왔다고 하자. 이때 후처리로 재시도를 하게 된다면 어떻게 될까? 첫 번째 주문일 때 결제가 돼서 금액이 차감되고 한 번 더 재시도를 해서 한 번 더 결제가 되어 금액이 또 차감되게 된다. 즉, 1번의 주문에 2번의 결제가 되어 정합성이 어긋나게 될 수 있는데 이를 멱등성 API로 해결해 볼 수 있다. 멱등성: 동일한 요청을 한 번 보내는 것과 여러 번 연속으로 보내는 것이 같은 값을 유지하는 성질 다음과 같이 API 요청 값에 유니크한 멱등키(idempotency-key)를 추가해서 보내 동일한 요청임을 알려주면 동일한 응답을 받을 수 있다. curl https://checkout-test.adyen.com/v70/payments \\ -H 'x-api-key: YOUR_API_KEY' \\ -H 'idempotency-key: YOUR_IDEMPOTENCY_KEY' \\ -H 'content-type: application/json' \\ -d '{ \"merchantAccount\": \"YOUR_MERCHANT_ACCOUNT\", \"reference\": \"My first Adyen test payment\", \"amount\": { \"value\": 1000, \"currency\": \"EUR\" }, \"paymentMethod\": { \"type\": \"scheme\", \"encryptedCardNumber\": \"test_4111111111111111\", \"encryptedExpiryMonth\": \"test_03\", \"encryptedExpiryYear\": \"test_2030\", \"encryptedSecurityCode\": \"test_737\" } }' 즉, 하나의 주문키에는 하나의 주문만 생성하도록 약속한 것이다. 이렇게 하게 되면 주문 쪽에서 타임아웃으로 인한 알 수 없음 응답을 받아도 굳이 결제 무효화 같은 불필요한 작업을 하지 않아도 그냥 재요청을 통해 성공 응답을 받을 수 있다. (만약 멱등성이 없는 상태에서 알 수 없음을 받으면 있는지 없는지 모르니 확인 요청을 하거나 결제 무효화 같은 추가적인 요청 필요) +타임아웃 특성상 짧은 주기로 재요청을 하게 되면 네트워크 지연상황을 더욱 악화 시킬 수 있다. 네트워크 지연으로 인해 더 빈번한 타임아웃이 발생하므로 재시도를 할 때 지수 백오프 전략을 사용하면 더 좋다(1, 2, 4, 8…) 지수 백오프: 허용 가능한 속도를 점진적으로 찾기 위해 일부 프로세스의 속도를 곱셈적으로 감소시키는 알고리즘 FIN. 두 개의 포스팅으로 외부 서비스로부터 영향 범위를 최소화하기위한 방법과 여러 서비스에서 어떻게 트랜잭션을 보장할 수 있을지에 대해 알아보았다. 현업에서는 정말 다양한 상황이 발생하는 것 같고 각자 다양한 방법으로 풀어 나간다는 게 너무 재밌는 거 같다. 해당 주제에 대해 공부하면서 앞으로 성장할 수 있는 키워드를 많이 얻은 것 같고 빨리 적용할 수 있는 날이 왔으면 좋겠다 🙌 참고: MSA 환경에서 네트워크 예외를 잘 다루는 방법 애플 한 주가 고객에게 전달 되기까지 adyen Docs Exponential backoff 갓누누 (_ㅇ_) *오타가 있거나 피드백 주실 부분이 있으면 편하게 말씀해 주세요.",
    "tags": "seminar minimize-dependency external-service etc",
    "url": "/etc/seminar/2023-12-02-better-external-api(2)/"
  },{
    "title": "외부 서비스로부터 영향 범위 최소화하기 (1)",
    "text": "외부 서비스는 우리가 통제할 수 없는 영역이기 때문에 많은 고민이 필요하다. 최근에 실무 쪽에 관심이 많아지면서 현업에서는 어떻게 외부 서비스와의 의존성을 관리하고 영향을 최소화하고 있는지 궁금했다. 두 개의 포스팅으로 나눠서 이번 글에는 외부 서비스와의 영향을 최소화하는 방법에 알아보고 다음 글에서는 여러 서비스에서 트랜잭션을 어떻게 보장해줄 수 있는지에 대해 알아보자. 외부 영향 최소화하기 1. 의존성 제거 처음에 외부 시스템을 연동할 때는 해당 연동이 꼭 필요한지 고민 외부 시스템의 안정성을 신뢰할 수 있을지, 연동을 통해 얻고자 하는 목적이 뚜렷한지, 리스크를 감수할 만큼의 가치가 있는지 이미 연동해서 사용 중인 시스템이더라도 지속적으로 적합성을 살펴보는 관심 필요 서비스가 커지고 구조가 변경되면 외부 시스템을 통한 이득은 변하지 않지만 리스크가 더 커지는 경우가 있고, 필요한 곳이 더 늘어나면서 연동 대상이 변경되거나 내부에서 직접 구현하는 경우도 있음 이런 변화를 지켜보다가 적절한 시점에 의존성을 제거해 주는 것이 좋음 2. 벤더 이중화 동일한 기능을 제공하는 여러 벤더 사가 존재하는 경우 벤더 이중화를 통해서 장애를 회피해 볼 수 있음 이중화를 한 경우라도, 일정 비율을 두고 양쪽 벤더를 모두 사용할 수 있도록 연동하는 것을 추천 한 쪽만 사용하다 전환했을 때 반대편이 정상적으로 동작하지 않아 장애가 해소되지 않을 수도 있기 때문에 양쪽과 상시로 트래픽을 주고 받어 연동 상태를 늘 체크하는 것이 좋음 3. 장애 격리 여러 이유(비용이 비싸거나, 대체 벤더가 없거나)로 이중화 불가능한 외부 시스템도 당연히 존재 이런 경우 외부 시스템 장애가 연동과 관련 없는 부분으로 전파되지 않도록 장애를 격리하는 것이 중요 특정 기능이 일부 동작하지 않거나 기능이 저하되더라도 사용자가 최소 기능을 사용할 수 있도록 제공하는 것이 핵심 4. 미작동 감내 위에서 설명한 1, 2, 3 방법으로도 장애 회피가 불가능한 경우가 바로 AWS, GCP 같은 클라우드 서비스 이런 경우 외부 시스템의 장애를 감내하고 장애 상황을 빠르게 인지할 수 있도록 모니터링을 강화하고 해당 서비스 담당 부서에 빠르게 확인할 수 있는 핫라인 조치 필요 장애 격리를 위한 3가지 방법 외부 서비스와 연동을 하다가 외부 서비스에서 처리 시간이 길어지면 전체 실행 시간에 영향이 가고 그에 따라 성능 저하가 있을 수 있다. 예를 들어, 서버 쓰레드 풀이 5개인 서비스가 있다고 가정해 보자. 외부 연동이 정상적으로 처리되어서 0.1초 + 나머지 시간 0.1초 =&gt; 평균 응답 시간 0.2초 1개 쓰레드가 초당 5개 요청을 처리 5개 쓰레드가 초당 25개 요청 처리로 25 TPS 외부 연동이 지연되어 0.9로 증가 + 나머지 시간 0.1초 =&gt; 평균 응답 시간 1초 1개 쓰레드가 초당 1개 요청 처리 5개 쓰레드가 초당 5개 요청 처리로 5 TPS 결과적으로 1/5의 성능이 나오게 되어 25개의 요청이 들어오게 되면 원래 1초 만에 응답을 받던 클라이언트가 5초 만에 응답받게 된다. 타임 아웃 RestTemplate의 타임아웃 기본 값에는 제한이 없다. 이 경우 동시에 대기하는 외부 연동 개수가 많아질수록 내 서비스는 점점 느려진다는 걸 뜻한다. 최악의 경우 모든 쓰레드가 대기 상태에 빠져 다른 클라이언트 요청에 응답할 쓰레드가 남아있지 않게 되기 때문에 반드시 타임아웃 설정이 필요하다. 일반적으로 Connection Timeout과 Read Timeout을 1초에서 5초 이내로 설정해 줄 수 있는데 상황에 따라 달라질 수 있기 때문에 근거 있는 기준이 필요하다. (ex. 한 번의 패킷 유실 정도는 재전송을 통해 해결할 수 있는 수준의 타임아웃) 더 자세한 건 다음 블로그를 참고하면 좋을 것 같다. 벌크 헤드 외부 연동 서비스가 많은 경우 특정 서비스에 장애가 발생하면 전체에도 영향이 가게 된다. 위와 같이 커넥션풀을 공유하면서 A, B, C 서비스에 대해 연동을 했다고 가정해 보자. A 서비스가 장애가 생겨서 응답 시간에 지연이 발생하면 풀에 남은 사용 가능한 커넥션이 줄면서 풀에서 커넥션을 구하는 대기시간이 증가하고 B, C 서비스에 대한 연동도 같이 대기하게 된다. 벌크헤드는 격벽을 치는 것을 뜻하며 배에서 4개의 격벽을 치는 것으로부터 유래되었다. 각자의 구역에서 무슨 일이 있더라도 다른 곳에서는 영향이 덜 하도록. 이를 서비스에도 적용하여 기능/서비스/클라이언트마다 자원을 분리해서 사용할 수 있다. 즉, 다음과 같이 외부 서비스마다 별도 커넥션풀을 사용하는 것이다. 그2023-11-30-better-external-api.md렇게 하면 A 서비스와 연동이 느려져도 B, C 서비스 연동에서의 영향은 감소시킬 수 있다. 서킷 브레이커 외부 시스템 장애가 지속되면 외부 서비스가 비정상임에도 내 서비스에서 계속해서 요청을 보낼 것이다. 그러면 응답 시간도 계속 느려지고 처리량도 감소한다. 이때 서킷 브레이커를 적용하면 내 서비스의 응답 시간과 처리량을 일정하게 유지할 수 있다. 서킷 브레이커는 오류 지속 시 일정 시간 동안 기능 실행을 차단한다. 즉, 기능을 실행하지 않고 바로 에러를 응답한다. 이렇게 빠른 실패(fail fast)를 함으로 써 외부 서비스의 장애로 인한 영향을 차단하고 응답 시간이 증가하거나 처리량이 감소하는 증상을 어느 정도 완화할 수 있다. 서킷 브레이커의 기본적인 동작 방식을 보자. 처음 상태는 서킷 브레이커가 닫혀있다.(요청을 통과시킨다는 것) 그러다 임계치를 초과한 실패가 발생하면 서킷 브레이커가 열리게 된다. 열리면 외부 api를 호출하지 않고 바로 에러를 리턴 한다. 일정 시간 동안 열린 상태를 유지하다가 시간이 지나면 반정도 연다.(half open) 반 정도 열게되면 다시 외부 api를 호출하고 제대로 작동하는지 확인해본다. 확인해서 다시 임계치를 초과한 실패가 발생하면 열리고 만약 임계치 이하로 실패가 발생하게되면 닫히게 된다. 이번 글은 여기서 끊고 다음 포스팅에서 여러 서비스에서 트랜잭션을 어떻게 보장해줄 수 있을지 알아보도록 하겠습니다. 참고: 외부 시스템 장애에 대처하는 우리의 자세 외부 API 장애에 영향 덜 받는 3가지 방법 *오타가 있거나 피드백 주실 부분이 있으면 편하게 말씀해 주세요.",
    "tags": "seminar minimize-dependency external-service etc",
    "url": "/etc/seminar/2023-11-30-better-external-api/"
  },{
    "title": "토스뱅크에서 데이터는 어떤 방식으로 설계하고 있을까?",
    "text": "테이블의 통합과 분리 테이블의 통합과 분리는 어떤 것이 맞는지 틀린 지 판단하기 어렵다. 잘못된 판단은 프로그램 본 수량, 소스량, 개발 시간 등을 증가시킨다. 업무에 따라서 통합 혹은 분리를 판단할 필요가 있다. 몇 가지 예시들을 보자 대출/카드 대출기본, 카드기본은 심사내역 분리 심사는 평가항목이 대출/카드가 상이한 항목이 많음 테이블을 심사내역 하나로 통합했을 때 불리한 점이 많음 연체 이후는 통합 대출이나 카드나 연체금액, 연체일 등 공통적인 항목이 많다. 또한 이후에 발생하는 법적 절차 테이블들은 고객별 데이터 → 통합이 더 효율적 보증 개별적으로 테이블 구성하면 3*6*10 하면 200여 개 까지 테이블 증가 어느 정도 통합 필요 대출/예금 과연 분리가 좋을까 통합이 좋을까? 테이블 통합과 분리: 이론 테이블 통합과 분리는 다음과 같이 3가지 방법이 있다. OneToOne 타입은 슈퍼 테이블/서브 테이블로 구성하는 방식 고객 테이블을 부모 테이블, 법인 테이블/개인 테이블을 자식 테이블로 구성 플러스 타입은 각각 개별 테이블로 구성하는 방법으로 법인 고객 테이블/개인 고객 테이블로 구성 싱글타입은 통합 테이블로 고객 테이블 한 개 구성 앞에 설명한 대출을 예시로 한번 분리해보자 대출을 개별 테이블로 분리한 경우 각 테이블에서 컬럼을 추가하면 되니까 확장성이 괜찮은 편이고 업무 디펜던시가 제거된다는 장점이 있다. 그러나 단점으로는 거래내역 테이블을 개별로 만들어야 하고 SQL 조인 시에도 각각 조인 해야하기 때문에 관리 용이성이 좋지 않음 대출을 슈퍼 + 서브 테이블로 분리 대출기본에 공통 속성 여신대출기본 수신대출기본에 개별 속성 테이블 조인 시에 대출기본 하나의 테이블과 조인을 해도 공통 속성인 계좌 상태 코드와 실행 일자를 조회할 수 있음 거래내역 테이블도 대출거래내역 한 개로 구성할 수 있다. 여신대출기본, 수신대출기본 각각 개별 속성인 실행금액과 한도 금액을 조회할 때는 여전히 여신대출기본, 수신대출기본 각각의 개별 테이블과 조인해야 하는 단점이 있음 대출을 통합 테이블로 구성 여수신대출기본 하나의 테이블과 조인을 해도 원장의 모든 속성 조회 가능 + 거래내역 테이블도 하나의 테이블로 구성 → 조인 성능이 우수하고 관리 용이성도 좋음 but 여러 프로그램에서 하나의 테이블만 바라보게 되고 테이블 ALTER가 빈번(프로그램 수정 자주 발생) 대량 데이터 발생하면 성능 저하 → 인덱스 파티션 전략 필요 이번엔 보증으로 예시를 들어보자 개별 테이블로 분리 운영하다 보면 테이블이 수백 개로 늘어날 수 있기 때문에 어느 정도 통합을 고민해야 함 중금리 대출은 심사 시 연간 소득 금액 필요 마이너스 대출 필요 x 마이너스대출은 연장이 되고 중금리는 연장이 안됨 중금리 대출은 신용대출이고 주택금융 대출은 부동산 정보 필요 기관별/상품별/업무별로 전문과 필요 항목이 상이하기 때문에 통합이 쉽지 않음. 슈퍼 + 서브 테이블로 분리 공통 속성인 보증서 번호나 보험료율을 조회할 때는 통합보증기본 테이블 하나와 조인 but 개별 속성을 조회할 때는 여러 번 조인 발생 통합 테이블 구성 아까 말했듯이 상이한 속성들이 너무 많아서 현실적인 설계는 아님(통합이 불가능한 건 아님) 해법은? 최대한 공통적인 것들은 모으자 데이터량/Transaction의 유형 고려 데이터량 없으면 성능 이슈가 없기 때문에 되도록 통합하는 것이 좋음 데이터량 많으면 슈퍼/서브 테이블 고려 사용자 경험을 해치지 않는다는 가정하에 트랜잭션을 분리해서 공통적인 속성을 먼저 입력하고 개별적인 속성은 팝업으로 입력하도록 담당자에게 업무 변경을 유도하는 것도 좋은 방법 통합사례들을 한번 알아보자 상품, 금리, 수수료는 명확하기 때문에 은행권에서 테이블 통합이 잘되고 있음 고객 정보는 고객에 통합되어 관리되고 있어 개인정보가 효율적으로 관리 순환참조의 활용 자기 자신 테이블의 PK를 일반 속성으로 넣는 방식으로 설계 CONNECT BY START WITH으로 선행 상태 추적 가능(신규 -&gt; 금리인하 -&gt; 감면 -&gt; 증액 -&gt; 연장) 비대면 대량거래 설계 주요 키인 계좌번호, 대출신청번호, 금리산출번호 등의 컬럼 자릿수 설계는 어떻게 해야될까? 계좌번호는 기본적으로 고객이 기억해야 하는 번호이기 때문에 자릿수가 되도록 짧아야 한다. 비대면 은행 특성상 고객이 계좌를 빈번히 발생시킬 수 있기 때문에 계좌번호 자체가 모두 FULL이 찰 수 있다. 그렇게 되면 안 쓰는 계좌를 DELETE 해야 되는데 은행권에서 데이터를 날리는 것은 명확히 준칙 하에 이루어져야 하므로 어렵다. 다른 방법으로 컬럼 자릿수를 늘려야 되는데 자릿수를 늘리려면 관련 계좌 테이블과 자식 테이블 수십 개를 ALTER 해야 하고 관련 프로그램 수백 개를 수정해야 함 그러므로 처음 데이터 설계 시 많은 고민해야 됨. 현재 토스 계좌번호 체계는 앞 3자리 계정과목 코드와 마지막 자리 체크 디지트를 빼고 8자리를 활용한다.(ex.800-00000466-2) 이렇게 하면 총 채번 가능 건수는 일억 건이 가능 만약 하루에 대출이 만 건 정도 발생한다고 가정하면 1년 발생은 365만 건이 되고 사용 가능 년수는 27년이 됨. (1억 % 3,650,000) 27년 후인 2048년에 FULL이 차면 그때 계정과목코드를 바꾸면 된다. 성능 최적화 설계: 채번(id) 채번 엔터티 장점 순차적으로 엄격하게 채번. 체계적인 채번 가능 단점 객체(엔터티) 증가 Lock 사용으로 느린 채번 시퀀스 장점 nextval로 사용하기 편함 가장 빠른 성능 최소한의 Lock 장점 체계 부여가 불편 빈 번호 발생 가능 객체(시퀀스) 증가 Max+1 장점 별도의 객체 필요x 순차적으로 엄격하게 채번. 체계적인 채번 가능 단점 Lock 사용으로 느린 채번 예외 상황 발생가능 최대값 관리 부담 존재 채번엔터티 같은 경우 락은 채번 테이블에서 발생하고 MAX+1은 INSERT 할 테이블에서 락이 발생. 토스 뱅크는 결번 발생을 허용하지 않는 업무를 제외하고는 성능 측면에서 뛰어난 오라클 시퀀스를 대부분 사용하는 방향으로 결정 후기 역시 실무는 토이 프로젝트들과는 달리 엄청나게 복잡한 것 같다. 도메인도 엄청나게 많고 그 안에서 분리된 것도 많다. 용어도 비슷한 게 많은 거 같고. 그리고 아무래도 실무다 보니깐 하나의 도메인이 있으면 추적하기 위한 이력 테이블도 존재해야 되니 더욱 고려할게 많은 것 같다. 계좌번호 자릿수 설계를 하는 걸 보고 아 설계는 저런 식으로 하는 거구나 하고 많이 배운 것 같다. 항상 답이 정해져 있는 게 아니라 분석을 통해 이렇게 나올 거 같고(하루 만 건) 그걸 기반으로 이렇게까지 할 수 있을 거(1억 % 365만건 = 27년) 같아서 이렇게 설계를 한다. 최근에 이론적인 것들을 공부하다 이렇게 현업에서 어떻게 하고 있는지 보니깐 너무 재밌는 것 같다. 매일 조금씩 공부해놓으면 나중에 현업에서 많은 도움이 될 것 같다. 참고: 토스뱅크 데이터 설계 사상 *오타가 있거나 피드백 주실 부분이 있으면 편하게 말씀해 주세요.",
    "tags": "seminar toss-bank slash-21 data etc",
    "url": "/etc/seminar/2023-11-29-tossbank-data/"
  },{
    "title": "과연 MySQL의 REPEATBLE READ에서는 PHANTOM READ 현상이 일어나지 않을까?",
    "text": "MySQL의 REPEATBLE READ 격리 수준에서는 PHANTOM READ 현상이 발생하지 않는다고 많이 본거 같은데 과연 진짜일까? REPEATABLE READ이란? REPEATABLE READ는 동일 트랜잭션 내에서는 동일한 결과를 보장할 수 있는 격리 수준으로 MySQL의 InnoDB 스토리지 엔진에서 기본으로 사용되는 격리 수준이다. 어떻게 동일한 결과를 보장해 줄 수 있을까? MVCC(Multi Version Concurrency Control)라는 매커니즘을 이용해 언두 영역에 이전 데이터를 백업해두고 실제 레코드 값을 변경한다. MVCC: 데이터베이스에서 동시성을 제어하기 위해 사용하는 방법으로 새로운 값을 업데이트하면 이전 값은 언두(UNDO) 영역에 관리함으로써 하나의 레코드에 대해 여러 개의 버전이 동시에 관리된다. MVCC의 가장 큰 목적은 잠금을 사용하지 않는 일관된 읽기를 제공하는 데 있음. REPEATABLE READ 동작 과정 사용자 B가 처음 SELECT(emp_no = 500000) 했을 때 결과로 Lara를 반환받음 사용자 A가 Lara를 Toto로 UPDATE 하면 언두 로그에 이전 데이터 복사 그 후에 사용자 B가 다시 SELECT로 읽었을 때 언두 로그를 이용해 같은 값인 Lara 반환 사용자 B가 두 번째 읽었을 때 해당 레코드의 TRX-ID(12)가 자신의 TRX-ID(10)보다 늦게 시작되었기 때문에 언두 로그를 통해 읽었다. 여기서 궁금증이 생겼다. 다음과 같이 자신의 트랜잭션 ID보다 작은 경우에는 어떻게 될까? 확인해 본 결과 같은 값인 moomin2를 들고 온다. 자신보다 큰 트랜잭션 ID인 경우에 언두 로그에 들어가서 가져오는 건가 했었는데 이런 경우도 있기 때문에 select 할 때 해당 레코드가 변경되었는지 혹은 해당 언두로그들이 있는지 확인 후 가져오는 매커니즘이 있는 것 같다. PHANTOM READ(유령 읽기) REPEATABLE READ 격리 수준에서는 DIRTY READ, NON REPEATBLE 문제는 해결할 수 있으나 PHANTOM READ 문제는 여전히 발생한다. REPEATABLE READ에서 새로운 레코드의 삽입까지는 막지 않기 때문에 SELECT로 조회했을 때 다른 트랜잭션에 의해 추가된 데이터를 얻을 수 있는데 이를 PHANTOM READ라 한다. 위에서 설명한 MVCC 덕분에 일반적인 조회에서는 PHANTOM READ가 발생하지 않는다. 자신보다 나중에 실행한 트랜잭션이 추가하는 레코드는 무시하면 된다. (자신보다 먼저 시작된 트랜잭션이 추가한 레코드도 무시하는지 실행해 보았는데 무시하는 걸로 보아 스냅샷이나 원래 없던 데이터라고 언두로그에 반영을 하는 것 같다. for update로 잠금을 걸고 조회 시 보임) 하지만, 잠금을 하고 읽는 경우 달라진다. 언두 레코드에는 잠금을 걸 수 없기 때문에 잠금을 하고 조회하는 레코드의 경우 언두 영역의 변경 전 데이터를 가져오는 것이 아니라 현재 레코드의 값을 가져오게 된다. 이로 인해 데이터의 부정합이 일어나 PHANTOM READ가 발생한다. 다음 4가지의 경우에 갭락이 있는 MySQL과 일반적인 RDBMS에서 어떻게 작동하는지 알아보자. SELECT FOR UPDATE -&gt; INSERT -&gt; SELECT SELECT FOR UPDATE -&gt; INSERT -&gt; SELECT FOR UPDATE SELECT -&gt; INSERT -&gt; SELECT SELECT -&gt; INSERT -&gt; SELECT FOR UPDATE 갭락: 레코드가 아닌 레코드와 레코드 사이의 간격을 잠금으로써 레코드의 생성, 수정 및 삭제를 제어 SELECT FOR UPDATE -&gt; INSERT -&gt; SELECT 일반적인 RDBMS의 경우 일반적인 RDBMS의 경우 갭락은 없기 때문에 2번 레코드에만 잠금이 걸림. moomin3 정상적으로 INSERT MVCC 덕분에 User B가 두 번째 조회 시 moomin2만 나옴 -&gt; MVCC 덕분에 PHANTOM READ 발생 X MySQL의 경우 MySQL의 경우 2번 레코드에 레코드 락과 id가 2보다 큰 범위에는 갭락으로 넥스트 키 락이 걸리게 된다. moomin3을 INSERT 하려고 하지만 갭락으로 인해 대기 User B가 두 번째 조회했을 때 moomin2만 반환하게 됨 User B의 트랜잭션이 커밋 되고 나면 갭락이 해제되면서 User A의 트랜잭션이 진행 -&gt; 갭락덕분에 PHANTOM READ 발생 X SELECT FOR UPDATE -&gt; INSERT -&gt; SELECT FOR UPDATE 일반적인 RDBMS의 경우 일반적인 RDBMS의 경우 갭락은 없기 때문에 2번 레코드에만 잠금이 걸림. moomin3 정상적으로 INSERT 잠금 있는 읽기는 데이터 조회가 언두 로그가 아닌 테이블에서 수행되기 때문에 두 번째 조회 시 moomin2와 moomin3 2건이 나오게 됨. -&gt; 잠금 읽기 때문에 PHANTOM READ 발생 MySQL의 경우 MySQL의 경우 2번 레코드에 레코드 락과 id가 2보다 큰 범위에는 갭락으로 넥스트 키 락이 걸리게 된다. moomin3을 INSERT 하려고 하지만 갭락으로 인해 대기 언두 로그 대신 테이블을 읽지만 어차피 아직 데이터가 Insert가 되지 않아 User B가 두 번째 조회했을 때 moomin2만 반환하게 됨 User B의 트랜잭션이 커밋 되고 나면 갭락이 해제되면서 User A의 트랜잭션이 진행 -&gt; 갭락덕분에 PHANTOM READ 발생 X SELECT -&gt; INSERT -&gt; SELECT 이 경우에는 일반적인 RDBMS나 MySQL 둘 다 같음. User B는 아무 잠금 없이 조회. 잠금이 없으므로 moomin3 정상적으로 INSERT MVCC 덕분에 User B가 두 번째 조회 시 moomin2만 나옴 -&gt; MVCC덕분에 PHANTOM READ 발생 X SELECT -&gt; INSERT -&gt; SELECT FOR UPDATE MySQL의 경우 갭락 덕분에 일반적으로 PHANTOM READ가 발생하진 않는다. 하지만, 유일하게 이 경우에만 발생하게 된다. 마찬가지로, 일반적인 RDBMS 경우도 여기서 또 발생하게 됨. User B는 아무 잠금 없이 조회. 잠금이 없으므로 moomin3 정상적으로 INSERT 사용자 B가 두 번째 조회 시 SELECT FOR UPDATE로 조회했기 때문에 2건의 결과 획득 -&gt; 잠금 읽기 때문에 PHANTOM READ 발생 마지막으로 표로 요약하자면 다음과 같다. 참고: Real MySQL 8.0 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse transaction repeatable-read PHANTOM_READ",
    "url": "/woowacourse/2023-11-28-Repeatable-Read/"
  },{
    "title": "Index 도사가 되어보자",
    "text": "왜 인덱스가 필요할까? 데이터베이스의 성능 튜닝은 어떻게 디스크 I/O를 줄이냐가 중요하다. 데이터베이스 서버에서 순차 I/O 작업은 그다지 비중이 크지 않고 랜덤 I/O를 통해 작은 데이터를 읽고 쓰는 작업이 대부분이다. 랜덤 I/O와 순차 I/O에는 어떤 차이가 있을까? 순차 I/O는 3개의 페이지를 디스크에 기록하기 위해 1번 시스템 콜을 요청했지만, 랜덤 I/O는 3개의 페이지를 디스크에 기록하기 위해 3번 시스템 콜을 요청했다. 즉, 디스크의 헤드를 각각 1번, 3번 움직였다. 디스크에 데이터를 쓰고 읽는데 걸리는 시간은 디스크 헤더를 움직여서 읽고 쓸 위치로 옮기는 단계에서 결정된다. 위의 그림의 경우 순차 I/O는 랜덤 I/O보다 3배 정도 빠르다. 결국 디스크의 성능은 디스크 헤더의 위치 이동 없이 얼마나 많은 데이터를 한 번에 기록하느냐에 의해 결정된다. 그래서 여러 번 쓰기 또는 읽기를 요청하는 랜덤 I/O 작업이 작업부하가 훨씬 더 크다. 일반적으로 쿼리를 튜닝하는 것은 랜덤 I/O 자체를 줄여주는 것이 목적이다. 인덱스란? 인덱스란 보통 책의 맨 끝에 있는 찾아보기(또는 “색인”)이라 할 수 있다. 찾아보기도 내용이 많아지면 우리가 원하는 검색어를 찾아내는 데 시간이 걸릴 것이다. 그래서 최대한 빠르게 찾아갈 수 있게 정렬이 돼있으며 인덱스도 마찬가지로 칼럼의 값을 주어진 순서로 미리 정렬해서 보관한다. 모든 데이터를 검색해서 원하는 결과를 가져오려면 시간이 오래 걸리기 때문에 인덱스를 이용해 빠르게 찾아볼 수 있다. DBMS에서 인덱스는 데이터의 저장(INSERT, UPDATE, DELETE) 성능을 희생하고 그 대신 데이터의 읽기 속도를 높이는 기능이다. 테이블의 인덱스를 하나 더 추가할지 말지는 데이터의 저장 속도를 어디까지 희생할 수 있는지, 읽기 속도를 얼마나 더 빠르게 만들어야 하느냐에 따라 결정해야 한다. 인덱스를 역할별로 구분해 본다면 프라이머리 키와 보조 키(세컨더리 인덱스)로 구분할 수 있다. 프라이머리 키: 그 레코드를 대표하는 칼럼의 값으로 만들어진 인덱스를 의미. 프라이머리 키는 NULL 값을 허용하지 않으며 중복을 허용하지 않는 것이 특징 보조 키(세컨더리 인덱스): 프라이머리 키를 제외한 나머지 모든 인덱스는 세컨더리 인덱스로 분류한다. 데이터 저장 알고리즘은 많은 분류가 있겠지만 대표적으로 B-tree 인덱스와 Hash 인덱스로 구분할 수 있다. B-Tree 인덱스는 가장 일반적으로 사용되는 인덱스 알고리즘으로 칼럼의 값을 변형하지 않고 원래의 값을 이용해 인덱싱하는 알고리즘이다. Hash 인덱스 알고리즘은 칼럼의 값으로 해시값을 계산해서 인덱싱하는 알고리즘으로 매우 빠른 검색을 지원한다. 하지만, 값을 변형해서 인덱싱하므로 일부만 검색하거나 범위를 검색할 때는 사용할 수 없다. B-Tree 구조 B-Tree의 트리구조는 최상위에 하나의 루트 노드가 존재하고 그 하위에 자식 노드가 붙어 있는 형태이다. 트리 구조의 가장 하위에 있는 노드를 리프 노드라 하고 중간 노드를 브랜치 노드라 한다. 리프 노드는 항상 실제 데이터 레코드를 찾아가기 위한 주솟값을 가지고 있다. 인덱스 키값은 모두 정렬돼 있지만 데이터 파일의 레코드는 정렬돼 있지 않고 임의의 순서로 저장돼 있다. 인덱스와 데이터 파일의 관계는 스토리지 엔진에 따라 달라진다. 두 스토리지 엔진(MyISAM, InnoDB)의 가장 큰 차이점은 세컨더리 인덱스를 통해 데이터 파일의 레코드를 찾아가는 방법에 있다. MyISAM 테이블은 세컨더리 인덱스가 물리적인 주소를 가지는 반면 InnoDB 테이블은 프라이머리 키를 주소처럼 사용하기 때문에 논리적인 주소를 가진다. 그래서 InnoDB 테이블에서 인덱스를 통해 레코드를 읽을 때는 데이터 파일을 바로 찾아가는 게 아니고 프라이머리 키 인덱스를 한 번 더 검색한 후, 프라이머리 키 인덱스의 리프 페이지에 저장돼 있는 레코드를 읽는다. 즉, InnoDB 스토리지 엔진에서는 모든 세컨더리 인덱스 검색에서 데이터 레코드를 읽기 위해서는 반드시 프라이머리 키를 저장하고 있는 B-Tree를 다시 한번 검색한다. 이 작업으로 인해 MyISAM보다 성능이 안 좋아 보일 수 있지만 각각의 스토리지 엔진은 장단점이 있다. 그 내용은 마지막에 살펴보자. B-Tree 인덱스 키 추가 및 삭제 인덱스 키 추가 새로운 키값이 B-Tree에 저장될 때 새로운 키값이 즉시 인덱스에 저장될 수도 있고 그렇지 않을 수도 있다. 저장될 키값을 이용해 B-Tree 상의 적절한 위치를 검색 후 리프 노드에 저장한다. 만약 리프 노드가 꽉 차서 더 저장할 수 없을 때는 리프 노드가 분리되어야 하는데 이때 상위 브랜치 노드까지 처리해 줘야 되기 때문에 비용이 많이 든다. MyISAM이나 MEMORY 스토리지 엔진을 사용하는 테이블에서는 INSERT 문장이 실행되면 즉시 새로운 값을 B Tree 인덱스에 변경한다. 하지만, InnoDB 스토리지 엔진은 필요하면 인덱스 키 추가 작업을 지연시켜 나중에 처리할 수 있다. 하지만, 프라이머리 키나 유니크 인덱스의 경우 중복 체크가 필요하기 때문에 즉시 B-tree에 추가하거나 삭제한다. 인덱스 키 삭제 삭제 같은 경우 그냥 삭제 인덱스 키 변경 변경 같은 경우 단순히 키값만 변경하는 것은 불가능하다. 먼저 키값을 삭제한 후 다시 새로운 키값을 추가하는 형태로 처리된다. B-Tree 인덱스 사용에 영향을 미치는 요소 인덱스를 구성하는 칼럼의 크기와 레코드의 건수 유니크한 인덱스 키값의 개수 인덱스 키값의 크기 InnoDB 스토리지 엔진은 디스크에 데이터를 저장하는 가장 기본 단위를 페이지 또는 블록이라 하며 디스크의 모든 읽기 및 쓰기 작업의 최소 작업 단위이다. 인덱스도 결국은 페이지 단위로 관리되며 리프 노드를 구분한 기준이 바로 페이지 단위다. 일반적으로 DBMS의 B-Tree의 자식 노드의 개수는 가변적인 구조로 인덱스 페이지 크기와 키값의 크기에 따라 결정된다. 예를 들어, 인덱스 키가 16바이트이고 자식 노드 주소 영역이 12바이트로 구성된다고 가정하면 하나의 인덱스 페이지(16KB)에 16*1024/(16+12)으로 585개 저장할 수 있다. 인덱스 키값이 32바이트로 커지면 몇 개를 저장할 수 있을까? 16*1024/(32+12) = 372개를 저장할 수 있다. 레코드 500개를 읽어야 하는 select 쿼리가 있다고 하면 전자의 경우는 페이지 한 번으로 해결할 수 있지만, 후자의 경우 최소 2번은 읽어야 된다. 키값의 크기는 B 트리의 깊이와도 연관 있다. 키값의 크기가 커질수록 한 인덱스 페이지에 담을 수 있는 인덱스 키값의 개수가 적어져 깊이가 깊어지고 디스크 읽기가 더 많이 필요하다. 그러므로 키값의 크기는 가능하면 작게 만드는 것이 좋다. 선택도(기수성) 선택도는 모든 인덱스 키값 가운데 유니크한 값의 수를 의미한다. 인덱스 키값 가운데 중복된 값이 많아지면(유니크 값 줄어들면) 기수성은 낮아지고 동시에 선택도 또한 떨어진다. 인덱스는 선택도가 높을수록(유니크가 많아질수록) 검색 대상이 줄어들기 때문에 그만큼 빠르게 처리된다. 예를 들어, 다음과 같은 상황이 있을 때 전체 레코드 데이터 건수는 1만 건 케이스 A - country 칼럼의 유니크한 값의 개수가 10개 케이스 B - country 칼럼의 유니크한 값의 개수가 1000개 SELECT * FROM tb_test WHERE country = 'KOREA' AND city = 'SEOUL'; 위의 쿼리를 실행하면 A 케이스의 경우 평균 1000건(10000/10), B 케이스의 경우 평균 10(10000/1000) 건이 조회될 수 있다는 것을 예측할 수 있다. A, B 케이스 모두 실제 모든 조건을 만족하는 레코드가 단 1건만 있었다고 하면 A 케이스의 경우 999건의 레코드를 더 읽었고 B 케이스의 경우 9건만 더 읽었기 때문에 A 케이스의 경우(중복도가 많은 경우) 비효율적이라 할 수 있다. 인덱스를 탄다고 무조건 좋을까? 인덱스를 통해 테이블의 레코드를 읽는 것은 인덱스를 거치지 않고 바로 테이블의 레코드를 읽는 것보다 높은 비용이 드는 작업이다. 인덱스를 통해 읽어야 할 레코드의 건수가 전체 테이블 레코드의 20~25%를 넘어서면 인덱스를 이용하지 않고 테이블을 모두 직접 읽어서 필요한 레코드만 가려내는 방식으로 처리하는 것이 효율적이다. MySQL이 인덱스를 이용하는 대표적인 방법 세 가지 인덱스 레인지 스캔 검색해야 할 인덱스의 범위가 결정됐을 때 사용하는 방식으로 루트 노드에서부터 비교를 시작해 브랜치 노드를 거치고 최종적으로 리프 노드까지 들어가면 그때부터 리프 노드의 레코드만 순서대로 읽으면 된다. 인덱스 탐색 - 인덱스에서 조건을 만족하는 값이 저장된 위치를 찾는다. 인덱스 스캔 - 1번에서 탐색된 위치부터 필요한 만큼 인덱스를 차례대로 쭉 읽는다. 2번에서 읽어들인 인덱스 키와 레코드 주소를 이용해 레코드가 저장된 페이지를 가져오고, 최종 레코드 읽음 데이터가 인덱스 안에 다 있으면 커버링 인덱스로 3번 과정이 일어나지 않음 인덱스 풀 스캔 인덱스를 사용하지만 인덱스 레인지 스캔과는 달리 인덱스의 처음부터 끝까지 모두 읽는 방식이다. 대표적으로 쿼리의 조건절에 사용된 칼럼이 인덱스의 첫 번째 칼럼이 아닌 경우 인덱스 풀 스캔 방식이 사용된다. 예를 들어, 인덱스는 (A, B, C) 칼럼의 순서로 만들어져 있지만 쿼리의 조건절은 B 칼럼이나 C칼럼의 검색하는 경우이다. 루스 인덱스 스캔 인덱스 레인지 스캔과 비슷하게 작동하지만 중간에 필요치 않은 인덱스 키값은 무시하고 다음으로 넘어가는 방식이다. GROUP BY 작업을 처리하기 위해 인덱스를 사용하는 경우에만 적용할 수 있다. 인덱스 스킵 스캔 MYSQL 8.0부터 옵티마이저가 앞 칼럼을 뛰어넘어서 뒤 칼럼만으로도 인덱스 검색이 가능하게 해주는 최적화 기능이다. 데이터베이스에서 인덱스의 핵심은 값이 정렬돼 있다는 것이고 이로 인해 인덱스를 구성하는 칼럼의 순서가 매우 중요하다. 예를 들어, 다음과 같은 인덱스를 생성되어 있다. ALTER table employees ADD INDEX ix_gender_birthdate (gender, birth_date); 위 인덱스를 사용하려면 WHERE 조건절에 gender 칼럼에 대한 비교 조건이 있어야 한다. -- 인덱스 사용X SELECT * FROM employees WHERE birth_date &gt;= '1965-02-01'; -- 인덱스 사용O SELECT * FROM employees WHERE gender='M' AND birth_date &gt;= '1965-02-01'; 첫 번째 쿼리의 경우 인덱스를 사용할 수 없어서 인덱스를 새로 생성했어야 했다. 하지만 MYSQL8.0 부터는 인덱스 스킵 스캔으로 처리가 가능하게 되었다. 하지만, 새로 도입된 기능이라 아직 다음과 같은 단점이 있다. WHERE 조건절에 조건이 없는 인덱스의 선행 칼럼의 유니크한 값의 개수가 적어야 함 쿼리가 인덱스에 존재하는 칼럼만으로 처리 가능해야 함(커버링 인덱스) 다중 칼럼(Multi-column) 인덱스 두 개 이상의 칼럼으로 구성된 인덱스로 실제 서비스에서는 다중 칼럼 인덱스가 더 많이 사용된다. 뒤에 칼럼은 앞의 칼럼에 의존해서 정렬되어 있기 때문에 각 칼럼의 위치가 상당히 중요하다. 클러스터링 인덱스 프라이머리 키값이 비슷한 레코드끼리 묶어서 저장하는 것을 클러스터링 인덱스라 하며 InnoDB 스토리지 엔진에서만 지원한다. 클러스터링 인덱스는 프라이머리 키값에 의해 레코드의 저장 위치가 결정된다. 즉, 키값이 변경된다면 그 레코드의 물리적인 저장 위치가 바뀌어야 한다는 것을 의미하므로 신중히 프라이머리 키를 결정해야 한다. 클러스터링 인덱스로 저장되는 테이블은 프라이머리 키 기반의 검색이 매우 빠르며, 대신 레코드의 저장이나 프라이머리 키의 변경이 상대적으로 느리다. 클러스터링 인덱스 구조를 보면 테이블의 구조 자체는 B-Tree와 비슷하다. 하지만, B-Tree의 리프 노드와는 달리 클러스터링 인덱스의 리프 노드에는 레코드의 모든 칼럼이 같이 저장되어 있다. 프라이머리 키가 없는 InnoDB 테이블은 어떻게 클러스터링 테이블로 구성될까? 프라이머리 키가 없는 경우에는 InnoDB 스토리지 엔진이 다음 우선순위대로 프라이머리 키를 선택한다. 프라이머리 키가 있으면 기본적으로 프라이머리 키를 클러스터링 키로 선택 NOT NULL 옵션의 유니크 인덱스 중에서 첫 번째 인덱스를 클러스터링 키로 선택 자동으로 유니크한 값을 가지도록 증가되는 칼럼을 내부적으로 추가한 후, 클러스터링 키로 선택 InnoDB 테이블에서 클러스터링 인덱스는 테이블당 단 하나만 가질 수 있는 엄청난 혜택이므로 가능하다면 프라이머리 키를 명시적으로 생성하자. 세컨더리 인덱스에 미치는 영향 MyISAM이나 MEMORY 테이블 같은 클러스터링 되지 않은 테이블은 INSERT 될 때 처음 저장된 공간에서 절대 이동하지 않는다. 데이터 레코드가 저장된 주소는 내부적인 레코드 아이디(ROWID) 역할을 한다. 프라이머리 키나 세컨더리 인덱스의 각 키는 그 주소를 이용해 실제 데이터 레코드를 찾아온다. 그래서 MyISam 테이블이나 MEMORY 테이블에서는 프라이머리 키와 세컨더리 인덱스는 구조적으로 아무런 차이가 없다. ROWID: 테이블 각 레코드(행)이 가지고 있는 고유의 주소 그렇다면 InnoDB 테이블에서 세컨더리 인덱스가 실제 레코드가 저장된 주소를 가지고 있다면 어떻게 될까? 클러스터링 키값이 변경될 때마다 데이터 레코드의 주소가 변경되고 그때마다 해당 테이블의 모든 인덱스에 저장된 주솟값을 변경해야 된다. 검색하는 과정에서 MyISAM과 InnoDB와 어떤 차이가 있는지 한번 살펴보자 MyISAM: 인덱스를 검색해서 레코드의 주소를 확인한 후, 레코드의 주소를 이용해 최종 레코드를 가져옴 InnoDB: 인덱스를 검색해 레코드의 프라이머리 키값을 확인한 후, 프라이머리 키 인덱스를 검색해서 최종 레코드를 가져옴 클러스터링 인덱스의 장점과 단점 마지막으로 MyISAM과 같은 클러스터링 되지 않은 일반 프라이머리 키와 클러스터링 인덱스를 비교했을 때 장단점을 보자. 장점 프라이머리 키(클러스터링 키)로 검색할 때 처리 성능이 매우 빠르다. 테이블의 모든 세컨더리 인덱스가 프라이머리 키를 가지고 있기 때문에 인덱스만으로 처리될 수 있는 경우가 많다(커버링 인덱스) 단점 테이블의 모든 세컨더리 인덱스가 클러스터링 키를 갖기 때문에 클러스터링 키값의 크기가 클 경우 전체적으로 인덱스의 크기가 커진다. 세컨더리 인덱스를 통해 검색할 때 프라이머리 키로 다시 한번 검색해야 하므로 처리 성능이 느리다. INSERT 할 때 프라이머리 키에 의해 레코드의 저장 위치가 결정되기 때문에 처리 성능이 느리다. 프라이머리 키를 변경할 때 레코드를 DELETE 하고 INSERT 하는 작업이 필요하기 때문에 처리 성능이 느리다. 참고: Real MySQL 8.0 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse index b-tree clustering",
    "url": "/woowacourse/2023-11-02-Index/"
  },{
    "title": "우당탕탕 쿼리 성능 개선하기",
    "text": "현재 우리 api 중 가장 많이 호출되는 메인 api는 반려동물 식품 조회이다. 데이터가 적을 때는 상관없었지만 100만 데이터 이상 일때 심각한 성능 이슈를 보이는 걸 확인하였다. 무려 한번 조회할 때마다 10초(아무 필터가 없을 때는 1분 이상도 걸림) 정도 걸리고 있는데 이 정도면 절대 서비스를 할 수 없는 수준이다. 왜 이렇게 오래 걸리는 걸까? 페치 조인과 페이지네이션 select distinct p1_0.id, p1_0.brand_id, b1_0.id, b1_0.created_at, b1_0.founded_year, b1_0.has_research_center, b1_0.has_resident_vet, b1_0.image_url, b1_0.modified_at, b1_0.name, b1_0.nation, p1_0.created_at, p1_0.eu_standard, p1_0.us_standard, p1_0.image_url, p1_0.modified_at, p1_0.name, p2_0.pet_food_id, p2_0.id, p2_0.primary_ingredient_id, p1_0.purchase_link from pet_food p1_0 join brand b1_0 on b1_0.id=p1_0.brand_id join pet_food_primary_ingredient p2_0 on p1_0.id=p2_0.pet_food_id join pet_food_functionality p3_0 on p1_0.id=p3_0.pet_food_id where b1_0.name in (?,?) and p1_0.us_standard=? and exists(select 1 from pet_food_primary_ingredient p5_0 join primary_ingredient p6_0 on p6_0.id=p5_0.primary_ingredient_id where p6_0.name in (?,?) and p1_0.id=p5_0.pet_food_id) and exists(select 1 from pet_food_functionality p7_0 join functionality f1_0 on f1_0.id=p7_0.functionality_id where f1_0.name in (?,?) and p1_0.id=p7_0.pet_food_id) 쿼리를 확인해 봤는데 분명 페이징 20을 걸었는데 limit이 안 날아가고 있는 걸 확인했다. 즉, 모든 데이터를 애플리케이션단으로 읽어들이고 있다는 얘기였다. 하지만, 클라이언트에서 받은 데이터를 확인해 보면 20개만 잘 날라오고 있다. 해당 쿼리 위에 다음과 경고 로그가 떠있다. WARN org.hibernate.orm.query - HHH90003004: firstResult/maxResults specified with collection fetch; applying in memory 컬렉션 가져오기로 지정된 첫 번째 결과/최대 결과가 메모리에서 적용되고 있다는 경고였다. 이 로그를 보자마자 예전에 공부했었던 페치조인과 페이징을 동시에 사용하면 생기는 문제점이 떠올랐고 바로 우리 코드를 살펴봤다. queryFactory .selectDistinct(petFood) .from(petFood) .join(petFood.brand, brand) .fetchJoin() .join(petFood.petFoodPrimaryIngredients, petFoodPrimaryIngredient) .fetchJoin() .join(petFood.petFoodFunctionalities, petFoodFunctionality) .where( isLessThan(lastPetFoodId), isContainBrand(brandsName), isMeetStandardCondition(standards), isContainPrimaryIngredients(primaryIngredientList), isContainFunctionalities(functionalityList) ) .orderBy(petFood.id.desc()) .limit(size) .fetch(); 현재 우리는 QueryDsl을 사용 중이다. fetchJoin과 limit이 같이 사용되고 있어서 저런 경고 로그가 찍히고 있다. fetch() 메서드를 한번 들어가 보자 limit으로 maxResult를 채워주고 있다.(offset -&gt; firstResult) 그리고 limit과 collectionFetchf를 포함하고 있으면 QueryLogging.QUERY_MESSAGE_LOGGER.firstOrMaxResultsSpecifiedWithCollectionFetch()가 실행되어 해당 로그가 찍히게 된다. 결국에는 해당 경고 로그를 찍어주고 애플리케이션으로 모든 데이터를 가져와서 페이징 처리를 한다는 것인데 왜 경고를 찍어주는 것일까? 우선 일대다 같은 경우 페치 조인을 하면 데이터가 뻥튀기가 되어서 데이터 수가 변하기 때문에 페이징을 할 수가 없다. 그래서, 모든 데이터를 애플리케이션 단에 다 가져와서 페이징 처리를 하게 된다. 이때 100만 개 이상의 데이터가 있다고 하면 성능 이슈가 발생할 뿐만 아니라 Out of Memory가 발생할 수도 있다. 또한, 잘 생각해 보면 페치조인으로 가져오는 N 관계의 테이블을 사용하지도 않고 where 절에서 조건으로만 사용한다. 그렇기 때문에 해당 join(join, fetchJoin)들을 삭제해 주었고 hibernate 6.0부터 distinct를 자동으로 적용해 주고 있기 때문에 distinct도 제거했다. 만약 여기서 지연 로딩한 부분을 나중에 사용하게 된다고 하면 N+1이 터질 것이다. 그런 경우는 BatchSize를 조절해서 최적화해주면 된다. queryFactory .select(petFood) .from(petFood) .join(petFood.brand, brand) .fetchJoin() .where( isLessThan(lastPetFoodId), isContainBrand(brandsName), isMeetStandardCondition(standards), isContainPrimaryIngredients(primaryIngredientList), isContainFunctionalities(functionalityList) ) .orderBy(petFood.id.desc()) .limit(size) .fetch(); select p1_0.id, p1_0.brand_id, b1_0.id, b1_0.created_at, b1_0.founded_year, b1_0.has_research_center, b1_0.has_resident_vet, b1_0.image_url, b1_0.modified_at, b1_0.name, b1_0.nation, p1_0.created_at, p1_0.eu_standard, p1_0.us_standard, p1_0.image_url, p1_0.modified_at, p1_0.name, p1_0.purchase_link from pet_food p1_0 join brand b1_0 on b1_0.id=p1_0.brand_id where b1_0.name in (?,?) and p1_0.us_standard=? and exists(select 1 from pet_food_primary_ingredient p2_0 join primary_ingredient p3_0 on p3_0.id=p2_0.primary_ingredient_id where p3_0.name in (?,?) and p1_0.id=p2_0.pet_food_id) and exists(select 1 from pet_food_functionality p4_0 join functionality f1_0 on f1_0.id=p4_0.functionality_id where f1_0.name in (?,?) and p1_0.id=p4_0.pet_food_id) order by p1_0.id desc limit ? 소량의 데이터일 때는 이런 심각한 상황인 줄 몰랐고 시간도 얼마 안 걸렸기 때문에 눈치를 못 챘었다. 저 warn 부분도 로그들 사이에 작게 있어서 보이지도 않았다. 해당 부분을 수정했을 때 6초까지 줄어들었다. 그만큼 대량의 데이터를 애플리케이션 단으로 가져오는데 비용이 많이 든다. 제대로 체감했으니 앞으로 이런 실수는 하지 않을 것 같다! DTO로 필요한 것만 하지만, 6초도 굉장히 오래 걸린다고 생각한다. 더 줄여보자. 현재 우리 쿼리의 projection 부분을 보면 매우 많다. p1_0.id, p1_0.brand_id, b1_0.id, b1_0.created_at, b1_0.founded_year, b1_0.has_research_center, b1_0.has_resident_vet, b1_0.image_url, b1_0.modified_at, b1_0.name, b1_0.nation, p1_0.created_at, p1_0.eu_standard, p1_0.us_standard, p1_0.image_url, p1_0.modified_at, p1_0.name, p1_0.purchase_link 하지만, 우리 프론트 화면에 필요한 부분은 petFood.id, petFood.name, brand.name, petFood.imageUrl 이 4개밖에 필요 없다. 불필요한 정보는 제거하고 필요한 것만 받아서 DTO로 받아보자. queryFactory .select(new QGetPetFoodQueryResponse( petFood.id, petFood.name, brand.name, petFood.imageUrl) ) .from(petFood) .join(petFood.brand, brand) .where( isLessThan(lastPetFoodId), isContainBrand(brandsName), isMeetStandardCondition(standards), isContainPrimaryIngredient(primaryIngredientList), isContainFunctionalities(functionalityList) ) .orderBy(petFood.id.desc()) .limit(size) .fetch(); QueryDsl 빈 생성 방법 중 QueryProjection을 이용해 4가지 필드로만 이루어진 DTO를 반환해 주었고 그렇게 6초에서 3.71초까지 줄게 되었다. 인자를 이름에서 id로 받아 조인 테이블 줄이기 현재 쿼리의 where을 보면 서브 쿼리들이 나가고 있는데 그 서브 쿼리에서도 join이 일어나고 있다. join이 일어나는 이유는 해당 필터는 id가 아니고 이름(String)으로 받고 있기 때문이다. 해당 테이블의 관계는 다대다(펫 푸드 - 중간 테이블 - 해당 테이블)인데 중간 테이블에는 이름이 없기 때문에 해당 테이블까지 조인이 발생하여 비용이 추가된다. select p1_0.id, p1_0.name, b1_0.name, p1_0.image_url from pet_food p1_0 join brand b1_0 on b1_0.id=p1_0.brand_id where b1_0.name in (?,?) and p1_0.us_standard=? and exists(select 1 from pet_food_primary_ingredient p2_0 join primary_ingredient p3_0 on p3_0.id=p2_0.primary_ingredient_id where p3_0.name in (?,?) and p1_0.id=p2_0.pet_food_id) and exists(select 1 from pet_food_functionality p4_0 join functionality f1_0 on f1_0.id=p4_0.functionality_id where f1_0.name in (?,?) and p1_0.id=p4_0.pet_food_id) order by p1_0.id desc limit ? 이름으로 받고 있는 필터(주원료, 기능성)들을 id로 받도록 수정해보자. //before private BooleanExpression isContainFunctionalities(List&lt;String&gt; functionalityList) { if (functionalityList.isEmpty()) { return null; } return petFood.petFoodFunctionalities.any() .functionality.name.in(functionalityList); } //after private BooleanExpression isContainFunctionalities(List&lt;Long&gt; functionalityList) { if (functionalityList.isEmpty()) { return null; } return petFood.petFoodFunctionalities.any() .functionality.id.in(functionalityList); } 인자를 이름에서 id로 바꿔서 불필요한 조인을 줄였고 3.71초에서 2.56초로 감소한 걸 확인할 수 있다. 복합 인덱스로 커버링 인덱스 이제 쿼리는 더 이상 뭘 건드려야 될지 안 보여서 EXPLAIN을 이용해 쿼리 실행 계획을 살펴봤다. 기존의 테이블 말고도 2개의 MATERIALIZED가 생성되었다. MATERIALIZED는 MySQL 5.6 버전에 추가된 셀렉트 타입으로 서브 쿼리를 임시테이블로 만들어 조인을 하는 형태로 최적화를 해준다고 한다. 최적화를 해주긴 하지만 임시 테이블을 만든다는 거 자체가 비용이 들 것이다. 또한, Using index도 아니고 Using index condition은 뭘까? 인덱스를 사용하여 테이블에서 행을 검색하는 경우를 위한 최적화 기능인데 자세한 내용은 다음 포스트를 참고하자. 이를 좀 더 개선하여 커버링 인덱스로 만들어보자. Using Index로 만들기 위해 현재 서브 쿼리에 있는 fk들을 복합 인덱스로 엮어 추가해 줄 수 있다. Using Index: 데이터 파일을 전혀 읽지 않고 인덱스만 읽어서 쿼리를 처리하여 매우 빠름 현재 두 개의 서브 쿼리는 다음과 같다. select 1 from pet_food_primary_ingredient p2_0 where p2_0.primary_ingredient_id in (?,?) and p1_0.id=p2_0.pet_food_id select 1 from pet_food_functionality p4_0 where p4_0.functionality_id in (1,2) and p1_0.id=p4_0.pet_food_id CREATE INDEX를 이용해 각 관계에 인덱스를 걸어주면 된다. CREATE INDEX idx_pet_food_primary_ingredient on pet_food_primary_ingredient(pet_food_id, primary_ingredient_id); CREATE INDEX idx_pet_food_functionality on pet_food_functionality(pet_food_id, functionality_id); 이렇게 하여 임시 테이블도 삭제되었고 커버링 인덱스도 걸렸다. 그럼, 최종적으로 시간이 얼마나 걸리는지 보자! 최종적으로 10초 -&gt; 0.662초로 개선되었다. 쿼리를 개선하는 과정이 정말 신선했고 재밌었지만 시간이 정말 오래 걸렸고 그만큼 너무 힘들었다…ㅠㅠ 개선해 보면서 정말 어려운 용어도 많았고 스킵 한 내용도 많은 거 같은데 Real My SQL을 읽어보면서 다시 한번 정리하는 시간을 가져야겠다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse query fetch-join index",
    "url": "/woowacourse/2023-10-12-better-query/"
  },{
    "title": "동시성 문제 해결을 위한 고민",
    "text": "현재 우리 집사의고민 반려동물 식품 리스트 view이다. 만약에 앞으로 추가적인 고도화를 한다면 해당 view에 좋아요 수, 평점, 댓글 수 등을 보여줄 수 있을 것이다. 해당 view는 사용자가 가장 많이 호출하는 메인 페이지의 API이기도 하다. 그래서 나중에 트래픽이 많아지면 해당 로직을 구현할 때 성능 개선을 위해 통계 컬럼을 식품 엔티티에 충분히 추가해줄 수도 있을 것 같다. 하지만, 이렇게 하게 되면 데이터의 정합성이 안 맞을 수 있는 가능성이 생길 수 있다. 예를 들어 두 명의 사용자가 동시에 해당 식품에 좋아요를 했다고 가정해 보면 다음과 같은 상황이 일어날 수 있다. (참고로 JPA를 사용하고 있다.) 두 사용자가 한 식품에 대해 좋아요를 눌러서 해당 식품의 좋아요 수는 7개가 되어야 하지만 조회한 시점에 카운트로 5를 읽고 서로 1씩 증가를 해버렸기 때문에 결과적으로 6이 되어버렸다. 이를 갱실 분실 문제라고 한다. 어떻게 해결해 볼 수 있을까? 우선 대표적으로 낙관적 락이나 비관적 락이 생각나긴 하는데 근본적인 문제가 무엇인지부터 살펴보고 다른 방법이 있나 고민해 보자. 직접 Update update 하는 과정을 잘 분석해 보면 JPA의 변경 감지를 사용하고 있기 때문에 동시성 문제가 일어나는 걸 알 수 있다. 변경 감지의 경우 바로바로 데이터베이스 값을 수정하는 것이 아니라 커밋 하는 시점에 flush가 일어나 쿼리가 날아가게 된다. 그래서 한 트랜잭션에서 select와 update 하는 사이에 누군가가 동시에 select 하고 update 하게 되면 갱실 분실이 일어난다. 그렇기 때문에, 변경 감지를 포기하고 직접 update 문을 날려주어서 정합성을 맞춰줄 수도 있다. update 문의 경우 x lock이 걸리기 때문에 해당 부분에서 다음과 같은 과정이 일어나 동시성이 발생하지 않는다. 즉, 다음과 같이 jpql을 이용해 해당하는 카운트의 개수를 증가시켜주는 리포지토리 메서드를 생성 후 서비스에서 직접 호출해 주면 된다. 이 방법의 경우 직접 쿼리를 이용해 데이터의 값을 변경 시켜주기 때문에 객체지향적이지는 않지만 성능 저하를 최소화하며 정합성을 지킬 수 있기 때문에 trade off로 충분히 감안해서 사용해 볼 수 있을 것 같다. @Query(value = \"update PetFood p set p.likeCount = p.likeCount + 1 where p.id = :petFoodId\") void increaseLikeCount(Long petFoodId); 근데 위의 코드로 실행하게 되면 다음과 같은 에러가 뜨게 된다. org.springframework.dao.InvalidDataAccessApiUsageException: org.hibernate.hql.internal.QueryExecutionRequestException: Not supported for DML operations … 공식 문서를 보면 @Query를 이용해서 INSERT, UPDATE, DELETE 같은 쿼리를 사용할 때는 해당 애노테이션을 사용하라고 되어있다. 해당 문서의 추가 옵션을 보면 clearAutomatically와 flushAutomatically가 있는데 애네는 뭘까? 우선 각 정의는 다음과 같다. clearAutomatically: 수정 쿼리를 실행한 후 기본 지속성 컨텍스트를 지워야 하는지 여부 flushAutomatically: 수정 쿼리를 실행하기 전에 기본 지속성 컨텍스트를 플러시 해야 하는지 여부 음.. 그냥 @Modifying만 사용하면 안 되는 걸까? @Modifying @Query(value = \"update PetFood p set p.likeCount = p.likeCount + 1 where p.id = :petFoodId\") void increaseLikeCount(Long petFoodId); 위처럼 하고 실행하면 잘 실행되긴 한다. 하지만, 실행하고 난 뒤 데이터 베이스의 값과 영속성 컨텍스트의 값이 일치하지 않는 걸 목격할 수 있다. 왜냐하면 @Query로 정의된 JPQL은 영속성 컨텍스트를 거치는 게 아니라 바로 Database로 쿼리를 날리기 때문에 데이터베이스 값은 증가되어 있지만, 영속성 컨텍스트의 값은 그대로이다. 해당 id로 데이터베이스에서 다시 값을 가져오더라도 이미 영속성 컨텍스트에 같은 id로 저장되어 있는 값이 있기 때문에 무시한다.(영속성 컨텍스트가 우선순위이기 때문에) @Modifying(clearAutomatically = true) @Query(value = \"update PetFood p set p.likeCount = p.likeCount + 1 where p.id = :petFoodId\") void increaseLikeCount(Long petFoodId); 그래서 위처럼 clearAutomatically를 사용해서 @Query로 정의된 JPQL이 실행된 후에 자동으로 영속성 컨텍스트를 비어줄 수 있다. flushAutomatically는 플러시와 관련이 있는 거 같은데 언제 사용하는 걸까? JPA에서 플러시가 일어날 수 있는 상황은 다음과 같다. flush() 직접 호출 트랜잭션 커밋 시 JPQL 쿼리 실행 시 분명 JPQL 쿼리가 실행되면 플러시가 자동으로 호출된다고 들었다. 근데 왜 필요한지 궁금해졌다. 그래서 Spring Data Jpa Github Repo에 들어가서 issue에 검색을 해봤더니 다음과 같은 issue가 있었다. 해당 이슈를 보면 원래 clearAutomatically라는 옵션만 있었고 flushAutomatically는 없었던 것 같다. 근데 clearAutomatically만 달고 사용하다 보니 다음과 같은 이슈가 있었다고 한다. As stated in the reference documentation, the AUTO flush strategy may sometimes synchronize the current persistence context prior to a query execution. 참조 문서에 명시된 바와 같이, 자동 플러시 전략은 때때로 쿼리 실행 전에 현재 지속성 컨텍스트를 동기화할 수 있습니다. The clearAutomatically property drops all the pending changes in the EntityManager that are not related to the current update query (cause they are not automatically flushed). clearAutomatically 속성은 현재 업데이트 쿼리와 관련이 없는(자동으로 플러시되지 않기 때문에) EntityManager의 보류 중인 모든 변경 사항을 삭제합니다. That’s why I’m asking for a new property to force the EntityManager to flush changes 그렇기 때문에 새 속성을 요청하여 EntityManager가 변경 사항을 강제로 플러시하도록 하려고 합니다. 즉, 작성한 JPQL 쿼리와 관련된 엔티티만 자동으로 flush가 되고 나머지 엔티티는 자동으로 flush가 되지 않고 clearAutomatically 속성에 의해 삭제되어서 데이터베이스에 반영이 되지 않고 있던 것이다. 예를 들어, 우리의 likeCount와 관련한 PetFood 엔티티는 플러시가 되지만, Like 엔티티는 플러시 되지 않고 영속성 컨텍스트에서 지워진다. 그렇기 때문에 다음과 같이 flushAutomatically도 true로 설정해서 사용해 줘야 원하는 데로 작동할 수 있다. @Modifying(clearAutomatically = true, flushAutomatically = true) @Query(value = \"update PetFood p set p.likeCount = p.likeCount + 1 where p.id = :petFoodId\") void increaseLikeCount(Long petFoodId); Lock 기법 vs 트랜잭션 격리 수준 동시성 하면 Lock(낙관적 락, 비관적 락)이나 트랜잭션 격리 레벨 같은 키워드가 떠오르는데 둘은 어떤 차이일까? Lock은 특정한 데이터에 대한 동시 액세스를 방지하기 위한 매커니즘이고 트랜잭션 격리 수준은 트랜잭션 동안 읽기 일관성을 어떻게 처리할지에 대한 전략이다. 즉, Lock은 낙관적 락, 비관적 락을 이용해 다른 트랜잭션이 동일한 레코드를 업데이트하는 것을 막기 위한 방법으로 트랜잭션 격리 수준과는 관계가 없다. 또한, 트랜잭션 격리도 다음과 같은 네 가지 트랜잭션 격리 수준으로 Lock 과는 관련이 없다. READ UNCOMMITTED: 각 트랜잭션의 변경 내용이 다른 트랜잭션에서 조회되는 격리 수준 READ COMMITTED: 커밋이 완료된 트랜잭션의 변경사항만 다른 트랜잭션에서 조회되는 격리 수준 REPETABLE READ: 트랜잭션이 시작되기 전에 커밋 된 내용만 조회되는 격리 수준 SERIALIZABLE: 사용중인 레코드가 다른 트랜잭션에서 접근할 수 없는 격리 수준 우리가 사용하고 있는 MySQL의 기본 격리 수준은 REPEATABLE READ이다. MariaDB의 경우도 마찬가지이고 오라클은 READ COMMITTED을 사용한다. 현재 사용하고 있는 트랜잭션 격리 레벨을 높이면 해결할 수 있을까? 한 단계 위인 SERIALIZABLE을 적용하고 나면 일어나는 과정들을 한번 보자. 우선 각각의 트랜잭션에서 조회를 할 때 S Lock(Shared Lock) 이 걸리게 된다. 이후 카운터를 증가시키기 위해 update 쿼리가 날아간다. 이때 Transaction 1에서 값을 변경시키기 위해 X Lock(Exclusive Lock) 을 얻어야 되는데 Transaction 2에서 공유 락을 가지고 있기 때문에 대기 상태로 들어가게 된다. Transaction 2도 마찬가지로 값을 변경시키기 위해 X Lock을 얻어야 되어서 대기 상태로 들어가게 되어서 서로 데드락에 빠지게 된다. 즉, 트랜잭션 격리 레벨로는 이 문제를 해결할 수 없기 때문에 다른 방법을 찾아봐야 된다. 낙관적 락(Optimistic Lock) 낙관적 락은 트랜잭션들의 빈번한 충돌이 일어나지 않을 것이라고 가정하는 방법으로 데이터베이스의 락을 사용하지 않고 애플리케이션 레벨에서 동시성을 제어하는 방법이다. 별도의 락을 잡지 않으므로 비관적 락보다 성능상 이점이 있다. 애플리케이션 레벨에서 어떻게 처리한다는 걸까? 바로, 엔티티의 Version을 통해서 관리하는데 JPA 같은 경우 @Version 애노테이션을 이용해 쉽게 사용해 볼 수 있다. @Entity class PetFood { ... @Version private Integer version } 해당 엔티티가 변경될 때마다 version이 증가하는 방식이고 동작 과정은 다음과 같다. Transaction1에서 식품 정보를 조회(좋아요 0, version 0) Transaction2에서 식품 정보를 조회(좋아요 0, version 0) Transactoin1에서 좋아요 Count 증가 (version 0으로 update 쿼리 -&gt; 좋아요 1, version 1) Transactoin2에서 좋아요 Count 증가 (version 0으로 update 쿼리 날리지만 version0의 해당 식품은 없기 때문에 예외 발생) 충돌 시 ObjectOptimisticLockingFailureException 예외가 터지기 때문에 해당 예외를 잡아서 개발자가 직접 처리해주어야 한다. 해당 식품에 좋아요를 누르는 게 선착순처럼 동시에 수많은 트래픽이 몰리지도 않을 것 같아 충돌이 빈번하게 일어나진 않을 것 같다.(소규모 서비스이면 더더욱 더) 그래서 몇 번의 재시도 로직을 추가해 주거나 예외를 보내 다시 누르도록 처리해 줄 수도 있을 것 같다. 하지만, 위에서 FK가 개입되게 된다면 상황이 달라진다. 위에서 보여준 문제 상황을 좀 더 상세하게 봐보자. 사실, 좋아요 Entity가 Insert 되는 과정이 있었지만 위에서는 딱히 중요하지 않은 거 같아 생략했었다. 어떤 문제가 있다는 걸까? 공식 문서를 보면 다음과 같은 내용이 있다. MySQL extends metadata locks, as necessary, to tables that are related by a foreign key constraint. … For foreign key checks, a shared read-only lock (LOCK TABLES READ) is taken on related tables. MYSQL은 필요에 따라 외래 키 제약 조건과 관련된 테이블로 잠금을 확장한다 … 외래 키 확인을 위해 관련 테이블에 공유 읽기 전용(S Lock) 잠금이 적용된다. 즉, 좋아요를 생성하는 Insert 쿼리를 날릴 때 식품에 대한 S Lock이 걸리게 된다. 트랜잭션 1, 2에서 처음에 서로 S Lock이 걸리고 그 후에 count를 증가시키는 update를 날릴 때는 아까와 같이 서로 대기하게 되어 데드락이 발생하는 문제가 생기는 것이다. 그래서 해당 방법을 사용하고자 하면 외래 키 제약 조건을 없애는 방법이 있을 수 있겠다. 비관적 락(Pessimistic Lock) 비관적 락은 트랜잭션들의 빈번한 충돌이 발생할 것이라고 가정하는 방법으로 데이터베이스의 락을 사용해서 동시성을 제어하는 방법이다. 위에서 현재 FK로 인해 서로 데드락이 걸려있는 상황이다. 이때, 비관적 락을 이용하여 해결해 보자. 해결하려는 과정은 다음과 같다. 트랜잭션 1에서 조회할 때 비관적 락을 걸어주면 해당 트랜잭션이 커밋 또는 롤백이 될 때까지 X Lock을 얻게 되어 다른 트랜잭션에서 해당 레코드에 접근 자체를 할 수가 없게 된다. 즉, 트랜잭션 A의 과정이 완전히 끝나고 나서야 B가 실행되기 때문에 데이터의 정합성과 위에서의 FK 데드락 문제가 해결되게 된다. 비관적 락의 경우도 @Lock 애노테이션을 통해 적용해 줄 수 있다. 식품 정보를 조회하는 메서드에 @Lock(LockModeType.PESSIMISTIC_WRITE)을 붙여 조회할 때 X Lock을 걸 수 있게 해주면 된다. @Lock(LockModeType.PESSIMISITC_WRITE) PetFood findByIdForUpdate(Long id) 하지만, 하나의 트랜잭션이 끝날 때까지 다른 트랜잭션은 대기만 하기 때문에 많은 트랜잭션이 접근하는 경우 성능에 큰 이슈가 있을 것으로 예상된다. 얼마 전에 애쉬가 비관적 락으로 선착순 티켓 동시성을 해결한 글이 있는데 해당 글을 보고 시니어 분들의 멘션이 있었다. (역시 절대 고수 애쉬ㄷㄷ 부럽따.., 정말 잘 썻으니 한 번씩 읽어보자 👍🏻) 뭔가 최대한 다른 방법으로 해결할 수 있는 방법을 모색하면 좋을 것 같다는 이야기들을 많이 하셨다. 이 부분에 대해서는 시간이 나면 성능 테스트를 한번 해봐야겠다. 그래서 비관적 락도 일단 보류! 마무리하기 전에 아래 내용을 한번 읽어 보고 자신이 낙관적 락과 비관적 락을 잘 이해했는지 생각해 보자. 낙관적 락의 경우 트랜잭션을 커밋하기 전까지는 충돌 여부를 확인할 수 없다. 비관적 락의 경우 데이터 수정시 트랜잭션 즉시 트랜잭션 충돌여부를 알 수 있다. 정리 지금 현재 상황에서는 생각해 본 방법 중 첫 번째 방법이었던 직접 Update하는 방법이 가장 괜찮을 것 같다. 물론 아직 여기까지 프로젝트가 고도화되진 않았기 때문에 적용은 못하겠지만 앞으로 해당 문제와 관련된 상황을 만나게 된다면 좀 더 능숙하게 대처해 볼 수 있지 않을까 한다. 문제를 해결하는 과정은 다시 한번 정말 흥미롭다는 걸 느꼈고 해당 문제뿐 아니라 그걸 기반하는 지식들도 조금씩 더 깊게 배운 느낌이라 보람찼던 고민이었던 것 같다. 그리고 다시 한번 생각나는 “은 탄환은 없다”… *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse concurrency lock transaction",
    "url": "/woowacourse/2023-10-04-concurrency/"
  },{
    "title": "좀 더 우아하게 유지보수 가능한 테스트 코드로 개선하기",
    "text": "현재 우리 집사의고민팀 테스트 코드 커버리지는 90%정도 된다. 이 영광을 jacoco형님께 바친다. 물론, 높은 커버리지가 무조건 최고는 아닌 게 assertion으로 확인하지 않고 그냥 선언만 해도 단순히 높일 수도 있기 때문이다. 하지만, 높은 커버리지가 시스템을 제대로 테스트했다는 것을 의미하지는 않지만, 커버리지가 매우 낮다는 것은 시스템이 제대로 테스트되지 않았다는 것을 의미하기 때문에 최대한 높이는 것이 좋지 않을까? Jacoco: Java 코드의 커버리지를 체크하는 라이브러리 어쨌든, 이게 요점이 아니고 이렇게 테스트 코드가 점점 많아짐에 따라 테스트코드도 점점 관리할 필요를 느꼈다. 점점 프로덕션 코드가 늘어갈수록 테스트 코드도 늘어가는데 보통 테스트 코드에는 큰 신경들을 안 쓰는 게 느껴졌다. 물론, 나도 그랬고 그래서 테스트 코드를 펼쳐 볼 때마다 솔직히 불편했다. 테스트는 굉장히 중요하다고 생각한다. 테스트가 있기 때문에 우리가 좀 더 안심하고 프로덕션 코드를 짤 수 있고 리팩터링도 수월하게 할 수 있다. 그리고 테스트 코드가 팀의 문서라고 할 수도 있다. 테스트 코드를 이용해 복잡한 비즈니스 로직을 좀 더 쉽게 이해할 수 있기 때문이다. 그렇기 때문에 언제까지 이 친구랑 불편하게 살 수는 없다. 앞으로 좀 더 잘 지내볼 수 있게 유지 보수 가능한 테스트 코드로 개선해 보자. 적용하고 있는 것 먼저 현재 적용하고 있는 것 중 다음 프로젝트에도 적용해 보면 좋은 것들을 몇 개 적어보겠다. 테스트 작명 한글화 적용 나는 원래 테스트를 영어로만 작성했고 한글에 거부감을 가졌던 사람 중 한 명이다. 하지만, 최근에는 학습 측면에서 여러 가지 경험하면 좋을 것 같아 이번 프로젝트에서 한글을 사용하고 있는데 인식이 정말 많이 바뀌었다. 우선, 불편할 것 같았던 자동완성 기능도 크게 불편하지는 않았다. 웬만하면 변수명이 다 짧았으며 긴 경우 _을 입력하면 똑같이 사용할 수 있다. 테스트가 한글로 되어 있어 가독성이 전체적으로 크게 향상된 것이 이 단점을 덮고도 남을 장점이 아닌가 한다. // given var 모든_영양기준_만족_식품 = 모든_영양기준_만족_식품(brandRepository.save(아카나_식품_브랜드_생성())); 식품_기능성_추가(모든_영양기준_만족_식품, functionalityRepository.save(기능성_다이어트())); 식품_기능성_추가(모든_영양기준_만족_식품, functionalityRepository.save(기능성_튼튼())); 식품_주원료_추가(모든_영양기준_만족_식품, primaryIngredientRepository.save(주원료_닭고기())); petFoodRepository.save(모든_영양기준_만족_식품); var 요청_준비 = given(spec) .queryParam(\"size\", 20) .filter(성공_API_문서_생성(\"식품 필터링 없이 조회 - 성공(전체 조회)\")); // when var 응답 = 요청_준비.when() .get(\"/pet-foods\"); // then 응답.then() .assertThat().statusCode(OK.value()); 이제 진짜 테스트가 프로젝트의 스펙이 된 것이다. 하나하나 해석할 필요 없이 그냥 한글로 된 시나리오를 읽듯 자연스럽게 내려오면 된다. 영어로 되었을 때는 로직을 읽기 전에 머리를 한 번 더 거쳐야 되었지만 이제 직관적인 구성으로 한층 더 테스트와 가까워 질 수 있지 않을까. 물론, 오픈소스나 외국인과 같이하는 프로젝트에서는 사용하긴 좀 어려워 보인다. 또한, 위에서 RestAssured의 Request 부분을 다음과 같이 픽스처로 빼서 좀 더 확장성 있게 사용 해 줄 수도 있을 것 같다. public static ExtractableResponse&lt;Response&gt; GET_요청(String url) { return RestAssured.given() .when() .get(url) .then() .extract(); } public static ExtractableResponse&lt;Response&gt; PUT_요청(String url, Object body) { return RestAssured.given() .contentType(MediaType.APPLICATION_JSON_VALUE) .body(body) .when() .put(url) .then() .extract(); } Fixture 계속해서 given 코드를 짜다 보면 중복해서 생성되는 데이터들이 나온다. 이 중복을 제거하기 위해 Fixture를 사용해 볼 수 있다. public static PetFood 모든_영양기준_만족_식품(Brand brand) { return builder() .name(\"모든 영양기준 만족 식품\") .purchaseLink(\"purchaseLink\") .imageUrl(\"imageUrl\") .brand(brand) .hasStandard(HasStandard.builder().unitedStates(true).europe(true).build()) .reviews(new Reviews()) .build(); } Fixture를 생성하기 위해 팩토리 메서드를 사용할 수도 있지만 팩토리 메서드 같은 경우 프로덕션 코드에서 어떤 의도를 가지고 만든 거기 때문에 최대한 지양하고 순수한 생성자나 빌더가 더 좋다. Nested 프로젝트가 점점 커져갈수록 테스트 코드도 커져간다. 수많은 테스트 중 특정한 테스트의 수행 결과들을 찾기가 어렵다. 이때, Nested 애노테이션을 이용하여 테스트를 다음과 같이 계층형으로 구성할 수 있다. 같은 관심사의 테스트를 모아둘 수 있기 때문에 내가 원하는 테스트를 열어서 수행 결과들을 볼 수 있어 테스트 가독성이 향상되어 보기 한층 더 편했다. ParameterizeTest 여러 가지 인자 값들을 테스트하고 싶은데 여러 개의 메서드들을 생성하기는 비용이 너무 커진다. 하나의 메서드에서 모든 인자 값들을 테스트해 볼 수 없을까? @ParameterizedTest @MethodSource(\"별점과_평균_리스트_만들기\") void 식품의_평균_별점을_계산할_수_있다(List&lt;Integer&gt; 별점_리스트, double 예상_결과) { // given Reviews 리뷰리스트 = 별점으로_리뷰만들기(별점_리스트); // when double 계산_결과 = 리뷰리스트.calculateRatingAverage(); // then assertThat(계산_결과).isEqualTo(예상_결과); } private static Stream&lt;Arguments&gt; 별점과_평균_리스트_만들기() { return Stream.of( Arguments.of(emptyList(), 0), Arguments.of(List.of(1, 2, 3, 4, 5), 3.0), Arguments.of(List.of(2, 3, 3, 5, 5), 3.6), Arguments.of(List.of(1, 2, 4), 2.3), Arguments.of(List.of(4, 4, 4, 5, 5), 4.4), Arguments.of(List.of(1, 2, 5), 2.7) ); } 위와 같이 해당 method를 이용한 @MethodSource로도 가능하고 @ValueSource, @CsvSource 등 여러 가지 ParameterizeTest를 적극적으로 이용해 보자. 개선하려는 것 데이터 셋팅 각 테스트마다 데이터가 필요했었고 해당 데이터는 연관관계가 많았기 때문에 이를 각각의 테스트마다 적어주기는 너무 불편했어서 기존에는 이를 setUp으로 구성해 줬었다. 하지만, 이 부분은 개선할 때 제일 먼저 고치고 싶은 것 중 하나였다. 어떤 테스트이든지 처음에 데이터가 위처럼 세팅되어 있으니깐 해당 데이터에 영향을 안 받을 수가 없다. 새로 추가한다 해도 이미 beforeEach에서 데이터가 들어가서 영향을 받아 매우 불편했다. 즉, 모든 테스트에 결합성이 생긴 것이다. 위의 공통 데이터들은 테스트와 결합도가 생겨 이 데이터들을 수정하는 경우에 모든 테스트에 공통으로 영향을 줄 수 있기 때문에 지양하는 것이 좋을 것 같다. 또한, 테스트 클래스가 엄청 커졌을 때 아래에 있는 어떤 테스트를 파악해야 된다고 가정을 해보자. 해당 테스트의 given이 없기 때문에 setUp을 보기 위해 위, 아래 왔다 갔다 해야 되는데 이는 과연 문서로서의 역할을 할 수 있을까? (마찬가지로, 테스트를 작성할 때도 불편) 이를 각 테스트마다 생성할 수 있도록 분리해 주었고 픽스처를 이용해 최소한의 중복으로 할 수 있도록 작성해주었다. //given PetFood 모든_영양기준_만족_식품 = 모든_영양기준_만족_식품(brandRepository.save(아카나_식품_브랜드_생성())); PetFood 미국_영양기준_만족_식품 = 미국_영양기준_만족_식품(brandRepository.save(오리젠_식품_브랜드_생성())); PetFood 유럽_영양기준_만족_식품 = 유럽_영양기준_만족_식품(brandRepository.save(퓨리나_식품_브랜드_생성())); 식품_기능성_추가(모든_영양기준_만족_식품, functionalityRepository.save(기능성_튼튼())); 식품_기능성_추가(미국_영양기준_만족_식품, functionalityRepository.save(기능성_짱짱())); 식품_기능성_추가(유럽_영양기준_만족_식품, functionalityRepository.save(기능성_다이어트())); 식품_주원료_추가(모든_영양기준_만족_식품, primaryIngredientRepository.save(주원료_소고기())); 식품_주원료_추가(미국_영양기준_만족_식품, primaryIngredientRepository.save(주원료_돼지고기())); 식품_주원료_추가(유럽_영양기준_만족_식품, primaryIngredientRepository.save(주원료_닭고기())); petFoodRepository.saveAll(List.of(모든_영양기준_만족_식품, 미국_영양기준_만족_식품, 유럽_영양기준_만족_식품)); 각 테스트마다 생성하는 게 조금 귀찮긴 하지만 그래도 각 테스트 간 독립성을 보장하며 손쉽게 데이터들을 추가, 제거해 줄 수 있다. 테스트에서 @Transactional 테스트 코드에서 대부분 데이터 롤백의 기능을 사용하기 위해 @Transactional을 많이 사용할 것이다. 하지만, 트랜잭셔널은 롤백 기능 말고도 다양한 기능들이 있기 떄문에 위험성을 인지하고 사용해야 한다. 테스트에 트랜잭셔널을 붙이면 다음과 같은 문제가 생길 수 있다. 프로덕션 코드에 트랜잭셔널이 없어도 테스트 통과 테스트에서 생성된 트랜잭셔널 범위 안에서 작업했기 때문에 통과할 수 있었음 DB에서 쿼리가 터져도 테스트 통과 SQLException이 터지는 코드 작성했다고 가정 변경감지로 엔티티 변경 변경 감지의 경우 트랜잭션을 커밋할 때 변경 내용을 SQL로 바꾸어 데이터베이스에 반영하는 원리 테스트에 적용된 트랜잭셔널은 각 테스트가 끝날 때 롤백하는 것이 기본 설정 그렇기 때문에 사실상 SQL 호출이 되지 않음 그렇다면 과연 @Transactional을 사용하지 않고 직접 하나씩 다 지워야 되는 걸까..? 이때, 토비님의 글과 유투브를 보았고 다음과 같은 의견을 주셨다. @Transactional 대신 tearDown 등에서 db를 클리어 하는 작업은 불가능한 건 아니지만 별로 추천하고 싶지 않습니다. 테스트 이전 상태가 모든 데이터가 다 비어있는 것으로 하기도 하지만, 어느 정도 초기 데이터 상태를 db에 넣고 하는 경우도 많은데, 데이터를 클리어하는 작업에서 이를 정확하게 원복한다는게 롤백 방식을 쓰지 않으면 매우 귀찮고 실수하기 쉽습니다. 초기 데이터가 달라지기라도 하면 모든 db 정리하는 코드를 또 다 고쳐야 하는데, 거기에 오류가 있으면 테스트가 다 깨지거나, 실패해야 할 다음 테스트가 성공하게 만들 수도 있겠죠. 그래서 아주 간단한 경우가 아니면 권장하지 않습니다. 아니면 테스트 하나 수행할 때마다 db 전체를 다 날리고 초기화 하는 작업을 하는 방법도 있긴한데, 애플리케이션이 커지면서 테스트가 매우 느려질테니 결국 테스트를 덜 만들거나 잘 하지 않게 될 겁니다. 단점이 더 많은 거죠. 또한, 스프링 개발팀도 사용을 적극적으로 추천하고 있다고 하고 김영한 님께도 여쭤보았더니 다음과 같은 말을 하셨다고 ㅋㅋㅋ “그러면 실용성이 너무 떨어지잖아요. 몇가지 조심하면 되는데 그것 때문에 오만가지 불편함을 감수하면서 초가삼간 다 태울 수 없으니…” 그렇다면 어떻게 최대한 예방할 수 있을까? 테스트를 웬만큼 잘 작성해도 애플리케이션 코드를 완벽하게 검증할 수는 없다는 사실을 인식해야 하고, 개발자가 작성하는 테스트, 단위 테스트나 통합 테스트 외에 실제 환경과 유사하게 환경을 구성하고 진행하는 인수 테스트, e2e 테스트, 혹은 http api 테스트 같은 것을 추가로 진행해야 한다고 한다. 또한, 코드에서 발생할 수 있는 전형적인 오류 같은 것들은, 코딩 가이드를 잘 작성해서 따르게 하거나 코드 리뷰에서 확인할 수 있도록 하고, 사용 가능하다면 각종 정적 분석 도구의 힘을 빌어서 어떤 작업을 수행하는 위치에 제한을 걸어주는 등을 통해서 검증이 되도록 해야 한다. 예를 들어 위에서 말한 1번 문제 같은 경우 코드 리뷰에서 잘 확인해 주거나 인수테스트(@Transactional 없는 @SpringBootTest) 등을 통해 실제로 작동되는지 확인을 해줄 수 있을 것 같다. 두 번째 문제 같은 경우 flush를 명시적으로 사용해서 롤백으로 끝나는 테스트가 실제로 쿼리를 DB에 날리도록 해주는 작업이 필요하다.(토비님이 항상 팀원 처음 교육할 때 제일 강조하는 거라고 한다.) PetFood petFood = new PetFood(\"이름\", \"브랜드\"); petFoodRepository.save(petFood); petFood.updateBrand(\"뉴 브랜드\"); entityManager.flush(); 이런 문제들을 머릿속에 잘 인식하고 테스트를 작성해야 될 것 같고 그리고 이에 대한 가이드가 매우 중요할 것 같다. 테스트 케이스 추가하기 한 테스트에 많은 경우의 수가 있지만 귀찮아서 전부 안한 경우도 꽤 있던 걸로 기억하는데 어떤 케이스를 넣으면 좋을까? 항상 테스트할 때는 암묵적이거나 아직 드러나지 않은 요구사항이 있는지 계속 염두에 두고 고민해야 한다. 이에 대해서 해피 케이스와 예외 케이스가 있을 수 있는데 말 그대로 요구사항을 만족하는 케이스가 해피 케이스고 만족하지 않는 경우가 예외 케이스이다. 이런 케이스를 테스트할 떄는 경계값을 이용할 수 있다. 예를 들어, 10 이상의 요구사항이 있을 때 해피 케이스를 테스트할 때는 10을, 예외 케이스로는 9를 테스트하는 것이 좋다. 이를 기반으로, 좀 더 꼼꼼한 테스트를 위해 케이스들을 보충하자. 메서드명(or DisplayName) 좀 더 명확하게 // void A를_추가() { // } void A를_추가하면_장바구니에_담긴다() { } 위와 같이 메서드명이 좀 더 길더라도 명사의 나열보다는 문장형이 더욱 좋다. 어떤 상태가 주어졌을 때 어떤 행위를 가했고 어떤 상태변화가 있었다는 결과를 명시할 수 있기 때문에 좀 더 이해하기 쉽다. // void ~하면_실패한다() { // } void ~하면_생성할_수_없다() { } 또한, 성공, 실패라는 테스트 현상에 집중하지 말고 도메인 관점에서 기술하자. 테스트 환경 통합하기(Context Caching) 스프링 서버가 한번 뜰 때 수행하는 것들이 많아 시간이 어느 정도 걸린다. 이 서버가 뜨는 횟수가 많아진다면 전체 테스트 시간이 굉장히 길어진다. 테스트를 자주 수행하려면 빨라야 되는데, 만약 테스트가 2분 이상씩 걸리면 과연 실행하고 싶을까? 프로파일 환경이라던가 설정, 목빈 같은 띄우는 환경이 조금이라도 달라지면 스프링 서버가 새로 뜨게 된다. 동일한 환경에서 수행될 수 있도록 이런 것들을 조금 모아줘보자. 컨텍스트 캐싱 원리가 궁금하면 다음 문서를 참고하자 우선 기존에 스프링 서버가 몇번 뜨는지 보자. 기존에는 12번의 서버가 재실행되고 있었다. 지금 대부분의 테스트들은 AcceptanceTest, ServiceTest, RepositoryTest를 상위에 두고 이를 받도록 하고 있는데도 12번이면 뭔가 어디서 누락되거나 미묘하게 환경이 달라지는 부분이 있다는 것이다. 새로운 스프링 서버가 뜨고 있는 클래스들을 추적해서 굳이 뜰 필요 없는 컨테이너들을 제거해 주거나 동일한 환경이 뜰 수 있도록 세팅을 맞춰주었다. 그렇게 하고 나니깐 5번으로 줄어든 걸 확인할 수 있다. 그렇게 하고 두 개의 빌드 시간을 비교해 보았다. 흠.. 지금은 크게 체감이 안되지만 나중에 테스트가 점점 많아지면 더 유의미해질 것 같다. before after *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse test junit",
    "url": "/woowacourse/2023-10-01-better-test/"
  },{
    "title": "쿼리 카운터를 이용하여 좀 더 효율적으로 개발하기",
    "text": "최근에 쿼리를 개선하는 과정에 다음과 같은 불편함이 있었다. 쿼리가 몇 개가 나가고 있는지 수동으로 위로 올리며 직접 체크 그러다 API 여러 개 겹치면 지금 보려는 쿼리가 어디인지 모르겠고 그럴 때마다 클리어하고 다시 날리고… 또한, 다음과 같은 상황에서도 유용하게 사용해 볼 수 있을 것 같다. 비정상적으로 많이 나가는 쿼리는 없는지 N+1 발생한지 체크 +최근에 리플렉션에 대해 학습을 했는데 어디 적용해볼 수 있을까 하던 차에 적용해 볼 수 있을 거 같아서 재밌어 보이기도 했다.. 그래서 앞으로 유용하게 사용할 수 있을 것 같아 적용하기로 결정. Hibernate StatementInspector 우선 사용하고 있는 프레임워크에서 이런 기능을 제공하는지 먼저 확인해 볼 필요가 있다. 이미 있으면 만들 필요는 굳이 없기 때문에. 사용하고 있는 ORM 프레임워크인 Hibernate에서 실행된 각 SQL 명령을 검사하고 처리할 수 클래스를 제공하긴 한다. 하지만, 생각해 보면 네이티브 쿼리를 날리는 경우도 있을 수 있고 Hibernate에 종속적이고 싶진 않았다. 결국, 자바의 표준 데이터 접근 기술인 JDBC 단에서 카운트를 세는 게 맞다고 판단 후 어떻게 이를 셀 수 있을지 생각해 보다가 이런 로직은 비즈니스 로직이 아닌 기능으로 AOP와 접목시키면 좋을 것 같다고 생각했다. 어떻게 만들어 볼까? 아이디어가 머릿속에 바로 떠오르면 좋겠지만 복잡한 기술들이 많이 섞여있어 ‘팟’하고 떠오르진 않았다. 그래서 알고리즘 풀 듯이 슈도코드를 한번 작성해 보면서 만들어봤다. Count를 어떻게 셀까? Jdbc에 쿼리가 날아가는 과정을 한번 보자 Datasource에서 connection을 얻어 그 connection을 이용하여 쿼리를 실행 그림 출처: 김영한님의 데이터 접근 핵심 원리 그 과정에서 prepareStatement가 실행되는 걸 확인 Connection에 AOP를 적용해 prepareStatement가 실행될 때 잡아서 카운트하자 프록시 구현? 근데 한 가지 문제점이 있다. AOP는 스프링 빈에만 적용할 수 있기 때문에 Connection에 적용하지 못한다. 하지만, 스프링 부트의 경우 DataSource는 스프링 빈으로 자동 등록되기 때문에 여기에 AOP를 적용할 수 있다. 그렇기 때문에 다음과 같이 DataSource에 AOP를 적용하고 추가 기능을 확장할 Connection은 프록시 객체로 만들자 프록시는 타겟의 기능을 확장하거나 타깃에 대한 접근을 제어하기 위한 목적으로 사용하는 클래스를 말한다. 근데 Connection을 프록시로 구현을 하려면 Connection의 모든 메서드를 구현해 줘야 되는데 이걸 언제 다 구현할까? 그래서 다이나믹 프록시라는 기술을 이용하여 간단하게 프록시를 생성해 주자. 다이나믹 프록시: 동적인 시점(런타임 시점)에 프록시를 자동으로 만들어서 적용해주는 기술 쿼리 카운터 구현하기 자 앞에 서론이 길었는데 이제 구현을 해보자. Counter 객체 생성 @Getter @Component @RequestScope public class QueryCounter { private int count; public void increaseCount() { count++; } } 카운트 역할을 하는 이 객체의 생명주기는 특정 스레드의 요청 동안만 사용된다. ThreadLocal을 써서 저장, 삭제해 줄 수도 있겠지만 Spring에서 RequstScope를 지원해 주기 때문에 이를 활용해 준다. ConnectionProxyHandler 생성 Connection 다이나믹 프록시 구현을 위해 handler를 먼저 만들어야 된다. 원하는 동작(카운트)을 설정할 수 있도록 InvocationHandler를 구현해 준다. public class ConnectionProxyHandler implements InvocationHandler { private static final String QUERY_PREPARE_STATEMENT = \"prepareStatement\"; private final Object connection; private final QueryCounter queryCounter; public ConnectionProxyHandler(Object connection, QueryCounter queryCounter) { this.connection = connection; this.queryCounter = queryCounter; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { countQuery(method); return method.invoke(connection, args); } private void countQuery(Method method) { if (isPrepareStatement(method) &amp;&amp; isRequest()) { queryCounter.increaseCount(); } } private boolean isPrepareStatement(Method method) { return method.getName().equals(QUERY_PREPARE_STATEMENT); } private boolean isRequest() { return RequestContextHolder.getRequestAttributes() != null; } } invoke 메서드가 실제 target의 메서드 호출을 가로채기 때문에 이 부분에 추가적으로 적용할 기능을 추가하면 된다. prepareStatement 메서드가 실행될 때마다 count를 추가해 줬다. 현재 QueryCounter가 RequestScope를 사용하고 있기 때문에 isRequest 분기가 없으면 다음과 같은 에러가 발생하니 꼭 추가해 주자. Scope ‘request’ is not active for the current thread; consider defining a scoped proxy for this bean if you intend to refer to it from a singleton; AOP 적용, Connection 다이나믹 프록시 구현 DataSource가 getConnection() 메소드를 호출할 때 Connection을 추가 기능이 장착된 프록시 객체로 덮어씌워줘야 된다. 방금 만든 Handler를 이용하여 Connection 다이나믹 프록시를 구현해 보자. 우선 자바에서 다이나믹 프록시는 Java.lang.reflect.Proxy 클래스의 newProxyInstance() 메소드를 이용하여 생성할 수 있다. 첫 번째 파라미터: 프록시 클래스를 정의하는 클래스 로더 두 번째 파라미터: 구현할 프록시 클래스의 인터페이스 목록 세 번째 파라미터: 메소드 호출을 전달하는 호출 핸들러 @Aspect @Component @RequiredArgsConstructor public class QueryCounterAop { private final QueryCounter queryCounter; @Around(\"execution(* javax.sql.DataSource.getConnection(..))\") public Object getConnection(ProceedingJoinPoint proceedingJoinPoint) throws Throwable { Object connection = proceedingJoinPoint.proceed(); return Proxy.newProxyInstance( connection.getClass().getClassLoader(), connection.getClass().getInterfaces(), new ConnectionProxyHandler(connection, queryCounter) ); } } AOP를 상세히 설명하기에는 글이 너무 길어질 것 같아서 AOP의 키워드나 동작과정은 콩하나의 테코톡을 한번 보고 오시면 좋을 것 같습니다. 👍🏻 쿼리 카운트 로깅 인터셉터 추가 자, 이제 마지막으로 앞에서 계산한 카운터를 인터셉터를 이용해서 로깅해주자. @Slf4j @Component @RequiredArgsConstructor public class LoggingInterceptor implements HandlerInterceptor { private static final String QUERY_COUNT_LOG = \"METHOD: {}, URL: {}, STATUS_CODE: {}, QUERY_COUNT: {}\"; private static final String QUERY_COUNT_WARN_LOG = \"쿼리가 {}번 이상 실행되었습니다!!!\"; private static final int WARN_QUERY_COUNT= 8; private final QueryCounter queryCounter; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) { int queryCount = queryCounter.getCount(); log.info(QUERY_COUNT_LOG, request.getMethod(), request.getRequestURI(), response.getStatus(), queryCount); if (queryCount &gt;= WARN_QUERY_COUNT) { log.warn(QUERY_COUNT_WARN_LOG, WARN_QUERY_COUNT); } } } 위처럼 구현하고 나면 다음과 같이 쿼리 개수를 편하게 확인 가능하다 🙌 참고: https://docs.jboss.org/hibernate/orm/6.2/javadocs/org/hibernate/resource/jdbc/spi/StatementInspector.html https://docs.oracle.com/javase/8/docs/api/java/sql/Connection.html https://docs.oracle.com/javase/7/docs/api/java/lang/reflect/Proxy.html#newProxyInstance(java.lang.ClassLoader,%20java.lang.Class[],%20java.lang.reflect.InvocationHandler) *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse query-counter query efficient",
    "url": "/woowacourse/2023-09-23-query-counter/"
  },{
    "title": "조인(Join) 한방 쿼리와 여러 개 쿼리",
    "text": "최근에 프로젝트에서 리팩토링과 동시에 쿼리 개선을 하고 있다. 조인 한방 쿼리로 한 번에 끝내는 게 여러 번 네트워크 타는 것보다 좋을 것 같아서 한방 쿼리로 수정하고 있다가 다음과 같이 조인이 덕지 덕지 붙어있는 코드를 보고 뭔가 이상함을 감지했다. select * from a join b join c join d join e 뭔가 아닌 거 같은데..? 물론 한방 쿼리가 좋을 때도 있지만 항상 한방 쿼리가 좋진 않을 것인데 언제 한방 쿼리를 사용하고 언제 여러 개 쿼리를 사용할 수 있을까 궁금증이 생겼다. 조인(Join)으로 한방 쿼리 vs 쿼리 여러 개 No Silver Bullet - 은총알은 없다. 어떤 성능 시나리오에서든 어느 솔루션이 더 빠른지 확인하기 위해 직접 솔루션을 테스트하고 측정해야 한다. 즉, 베스트 상황은 없다. 하지만, 일반적으로 어떤 상황에서 쓰면 좋을지는 통계가 있을 것이다. 이를 기반으로 한번 살펴보자. 일대일 관계 일반적으로 일대일 관계 같은 많은 외부 레코드를 가리키지 않는 경우 다음과 같은 성능을 나타낸다. 조인이 가장 빠른데 그 이유는 여러 개 쿼리가 발생하면 데이터베이스에 대한 각 쿼리에 고정 비용이 발생하기 때문이다. 일대일 관계 같은 경우 중복도 발생하지 않아서 부담 없이 조인을 사용할 수 있을 것 같다. 일대다에서는 절대 조인을 쓰지 말란 것이 아니다. 위에서 말한 것에 핵심 포인트는 일대일뿐 아니라 일대다에서도 많은 외부 레코드를 가리키지 않는 경우이다. 한방 조인으로 결괏값 주기 vs 여러 번 통신 + 애플리케이션에서 조합의 트레이드오프를 잘 고려해 봐야 된다. 일대다 관계 하지만, 참조된 레코드 중 다수가 동일할 수 있는 일대다 관계 같은 경우 조인 시에 중복이 엄청나게 발생할 수 있으니 잘 생각해 봐야 된다. 예를 들어보자. 1개의 게시물이 있고 게시물에는 2개의 댓글과 2개의 태그가 있다고 가정해 보자 SELECT post.id, comment.id, tag.id FROM post LEFT JOIN comment on post.id = comment.post_id LEFT JOIN tag on tag.post_id = post.id; -- post_id comment_id tag_id -- 1 1 1 -- 1 1 2 -- 1 2 1 -- 1 2 2 이 경우 2 * 2로 4개의 결과가 나왔다. 여기서 댓글과 태그가 4개로 늘어난다면 어떻게 될까? SELECT post.id, comment.id, tag.id FROM post LEFT JOIN comment on post.id = comment.post_id LEFT JOIN tag on tag.post_id = post.id; -- post_id comment_id tag_id -- 1 1 1 -- 1 1 2 -- 1 1 3 -- 1 1 4 -- 1 2 1 -- 1 2 2 -- 1 2 3 -- 1 2 4 -- 1 3 1 -- 1 3 2 -- 1 3 3 -- 1 3 4 -- 1 4 1 -- 1 4 2 -- 1 4 3 -- 1 4 4 각 행의 곱인 4 * 4 = 16개의 행이 나오게 된다. 그럼 만약에 각 데이터의 수가 1000개씩만 되어도 나오는 행의 수는 몇 개일까..? 여기서 더 많은 테이블과 레코드들을 추가하면 대부분 중복된 데이터로 가득 찬 수많은 행으로 문제가 확대가 된다. 실제 테스트를 해보자. 실험 환경은 다음과 같다. Mac M1 Pro 16G 512GB 실험 데이터 수는 게시글 1개, 댓글 1000개, 태그 1000개 먼저 조인을 이용하여 100만(1000 * 1000)개의 테이블을 생성해 결괏값 100만 개를 내어줄 때는 12초가 걸렸다. 하지만, 두 개의 쿼리(1000 + 1000)로 분리하여 결괏값 100만 개를 내어 줄 때는 고작 0.2초밖에 안 걸린 걸 볼 수 있다. 여러 개 테이블의 경우도 벤치마크를 한번 보자. 두 경우 모두 동일한 결과(6 x 50 x 7 x 12 x 90 = 2268000)를 얻지만 여러 개의 쿼리가 훨씬 더 빠르다. 5개의 조인을 사용한 단일 쿼리 쿼리: 8.074508초 결과 크기: 2268000 연속 쿼리 5개 결합된 쿼리 시간: 0.00262초 결과 크기: 165(6 + 50 + 7 + 12 + 90) 또한, 중복 데이터로 인해 기하급수적으로 더 많은 메모리를 사용한다. 적은 테이블의 수를 조인하는 경우 크게 나쁘지 않을 수 있지만 늘어날수록 행의 수가 기하급수적으로 늘기 때문에 잘 고려해 보자. 적합한지 여부 조인을 사용해야 하는지는 조인이 적합한 지에 따라 판단해 볼 수도 있다. 위의 예시로 사용한 테이블에서 댓글과 태그는 게시글과는 관련이 있지만 서로는 관련이 없다. 이 경우 두 개의 별도 쿼리를 사용하는 것이 더 좋다. 태그와 댓글을 결합하려고 하면 둘 사이에는 직접적인 관계가 없는데도 가능한 모든 조합이 생성된다. 또한, 두 쿼리를 병렬로 수행하여 이득을 얻을 수도 있다. 하지만, 여기서 만약에 다음과 같은 상황을 생각해 보자. 게시물에 댓글을 달고 댓글 작성자의 이름까지 원한다면? 조인을 고려해 볼 수 있다. 훨씬 더 자연스러운 쿼리일 뿐 아니라 데이터베이스 시스템에서 이와 같은 쿼리를 최적화하려고 노력하고 있다고 한다. 서브쿼리(SubQuery) 조인 대신에 서브 쿼리를 이용해서 쿼리를 짤 수도 있을 거 같은데 서브 쿼리와 조인의 성능을 비교해 보자. 우선 MYSQL 5.5 버전에서 서브 쿼리는 못 쓸 정도라고 평할 정도로 제대로 수행되지 않았다. 테스트 데이터 메인 테이블 100만 건 서브 테이블1 (Index O) 1000건 서브 테이블2 (Index X) 1000건 수행 시간 MYSQL 5.5 + 서브 쿼리 + No Index에서 100만 건 &amp; 1천 건 조회에 3분이 소요 MYSQL 5.5 + 서브 쿼리 + Index에서 100만 건 &amp; 1천 건 조회에 1.8초 소요 MYSQL 5.5 + 조인 + No Index에서 100만 건 &amp; 1천 건 조회에 11초 소요 MYSQL 5.5 + 조인 + Index에서 100만 건 &amp; 1천 건 조회에 0.139초 소요 MYSQL 특정 버전(5.5이하)에서는 서브 쿼리 대신 조인이 압도적으로 빠른 걸 볼 수 있다. 5.6에서는 서브 쿼리 성능 개선이 많이 이루어졌다. 수행 시간 MYSQL 5.6 + 서브 쿼리 + No Index에서 100만 건 &amp; 1천 건 조회에 19초 소요 MYSQL 5.6 + 서브 쿼리 + Index에서 100만 건 &amp; 1천 건 조회에 0.18초 소요 3분에서 19초, 11초에서 0.18초 개선될 정도로 5.6에서는 최적화가 이루어졌지만 아직 모든 서브 쿼리가 다 최적화가 된 것은 아니라고 한다. 즉, 웬만하면 최대한 조인을 이용하고 조인을 이용하기 어렵다면 5.6 이상은 서브 쿼리를 사용하자. 5.5 이하는 절대 사용하지 않고 차라리 쿼리를 나눠서 2번(메인 쿼리, 서브 쿼리) 실행하고 애플리케이션에서 조립하자. (서브 쿼리가 조인보다 빠를 때도 있음, 스칼라 서브 쿼리 캐싱 어쩌고저쩌고..) 각 버전 서브 쿼리의 자세한 동작 원리나 서브 쿼리 최적화 적용 조건을 보고 싶다면 동욱 님의 포스트 참고! 결론 사실 위의 내용은 어느 정도 통계에 기반에서 이렇다 할 뿐이지 자신의 상황에서는 전혀 다르게 동작(고려해야 할 변수가 많기 때문에)할 수도 있다. 그러므로 참고만 하고 EXPLAIN 같은 실행계획이나 직접 테스트하여 측정해 사용하도록 하자. Don’t guess, measure! 참고: https://stackoverflow.com/questions/1067016/join-queries-vs-multiple-queries https://jojoldu.tistory.com/520 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse query join sub-query",
    "url": "/woowacourse/2023-09-20-query/"
  },{
    "title": "변경하려는 인터페이스가 호환이 되지 않는 건에 대하여(어댑터 패턴)",
    "text": "문제 상황 통합 검색 기능이 이미 DB를 이용하여 구현되어 있다고 가정하자. 그래서 현재 다음과 같은 구조로 되어있는데 게시글 수가 폭발적으로 증가하면서 SQL을 이용한 like 검색 속도 성능에 문제가 발생하기 시작했다. 그래서 해당 문제를 해결하기 위해 FS라는 오픈 소스 검색 서버를 도입하기로 했다. 문서를 보니 FSClient 모듈을 사용하면 FS와 쉽게 연동할 수 있다고 한다. 그런데 문제는 FSClient에서 제공하는 인터페이스와 현재 사용하고 있는 SearchService 인터페이스가 맞지 않는 것이다. 현재 수많은 클래스에서 SearchService를 사용하고 있기 때문에 SearchService 대신 FSClient를 사용하도록 변경하는 것은 많은 비용이 들기 때문에 어렵다. 이 문제를 어떻게 해결해볼 수 있을까? 어댑터 패턴(Adapter Pattern) 클라이언트가 요구하는 인터페이스와 재사용하려는 모듈의 인터페이스가 호환되지 않을 때 사용할 수 있는 패턴이 바로 어댑터 패턴이다. 이 어댑터 패턴을 적용하면 다음과 같은 구조가 되게 된다. SearchServiceFSAdapater 클래스는 FSClient를 SearchService 인터페이스에 맞춰 주는 책임을 갖는다. SearchServiceFSAdapter의 search() 메서드는 FSClient 객체를 실행하고 그 결과를 SearchService 인터페이스에 맞는 리턴 타입으로 변환 해준다. 코드로 한번 보자 public class SearchServiceFSAdapter implements SearchService { private FSClient fsClient = new FSClinet(); public SearchResult search(String keyword) { FSQuery fsQuery = new FSQuery(keyword); //(1) QueryResponse queryResponse = fsClient.query(fsQuery); //(2) SearchResult result = convertToResult(queryResponse); //(3) return result; } } private SearchResult convertToResult(QueryResponse queryResponse) { ... //queryResponse에서 -&gt; SearchResult로 만들어 주기 위한 코드 List&lt;SearchDocument&gt; docs = ... return new SearchResult(docs); } keyword를 FSClient가 요구하는 형식으로 변환 FSClient의 query() 메서드를 실행 FSClient의 결과를 SearchResult로 변환 이렇게 하면 Client 부분의 코드 수정 없이 DB 기반 통합 검색에서 FSClient를 이용한 통합 검색으로 구현을 변경할 수 있게 되었다. 방금 위처럼 구현한 방법을 객체 위임 방식 어댑터라고 한다. 또한, 상속을 이용해서 구현할 수도 있는데 이를 클래스 상속 방식 어댑터라고 한다. 상속으로도 한번 구현해 보자 public class SearchServiceFSAdapter extends FSClient { public SearchResult search(String keyword) { FSQuery fsQuery = new FSQuery(keyword); QueryResponse queryResponse = super.query(fsQuery); SearchResult result = convertToResult(queryResponse); return result; } } private SearchResult convertToResult(QueryResponse queryResponse) { ... //위와 같음 } 코드가 크게 달라진 부분은 없고 단지 FSClient에 정의된 메서드를 호출하는 방식으로 코드를 작성하게 된다. 적용된 예시 어댑터 패턴이 적용된 예로는 SLF4J라는 로깅 API가 있다. SLF4J는 단일 로깅 API를 사용하면서 자바 로깅, log4j, LogBack 등 다양한 로깅 프레임워크를 유동적으로 사용할 수 있도록 해주는데, 이때 SLF4J가 제공하는 인터페이스와 각 로깅 프레임워크를 맞춰주기 위해 어댑터 패턴을 사용하고 있다. 어댑터 패턴을 사용하면 개방 폐쇄 원칙을 따를 수 있도록 도와준다. 만약 로깅 프레임워크를 Logback으로 교체하고 싶다면 Logback을 slf4j-api 패키지의 Logger로 맞춰 주는 새로운 어댑터만 구현해 주면 되는데 이때, slf4j-api 패키지의 Logger를 사용하는 코드는 전혀 영향을 받지 않는다. 참고: 개발자가 반드시 정복해야 할 객체 지향과 디자인 패턴 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse adapter-pattern",
    "url": "/woowacourse/2023-09-14-Adapter/"
  },{
    "title": "우테코 - 톰캣 구현하기 미션 회고 (feat. 나도 톰캣 컨트리뷰터..?)",
    "text": "레벨 1, 2 때 미션을 하면서 느낀 점이 우선 구현(리팩터링을 약간 곁들인)을 하고 나서 추가시간을 들여 리팩터링을 하는 게 좋을 것 같다고 느꼈는데 이번 미션에 적용해 봄으로써 확실히 이게 내 개발 라이프 사이클인 것을 체감했다. 이전에는 구현을 하면서 동시에 리팩터링도 빡세게 하려다 보니 오히려 비즈니스 로직과 확장성이 머릿속에서 겹쳐서 뇌에 과부하가 와 시간이 굉장히 오래 걸렸다. 하지만, 이번에는 처음에 구현에 초점을 많이 맞추고 구현이 끝나고 난 뒤 추가적으로 리팩터링을 하였는데 시간적으로도 여유로웠고 관심사를 분리하니깐 머릿속으로도 여유로웠다. 한 가지 아쉬웠던 점은 이번에는 좀 극적으로 많이 구현에만 집중을 하고 리팩터링을 마지막에 몰아서 했었는데 그것보다는 구현을 하다가 리팩터링 할 때가 딱 보일 때, 그 적절한 때 하는 것이 베스트 인 것 같다. 물론 그 적절한 때를 찾기가 쉽지는 않겠지만 연습하다 보니 조금씩 감이 잡혀가는 것 같다. 요즘 설계에 관심이 많은데 다음 미션 때는 패키지 의존 쪽에 대해서도 고려를 해서 짜보면 좋을 것 같다. 다음 미션이 MVC 구현이라 요구사항이 빡세서 가능할진 모르겠지만… ㅋㅋ 자 이제 미션을 되돌아 보자~ 전체 코드는 해당 저장소에 있습니다. 톰캣을 구현해라 ㅖ…? 갑자기 톰캣을 구현하라고 하니깐 뭐부터 해야 될지 막막했다 ㅋㅋㅋ 나는 그동안 편리한 스프링 부트를 사용하였고, 그 안에 내장 톰캣이 알아서 돌아가고 있었기 때문에 막상 톰캣이 어떻게 돌아가는지는 생각해 본 적은 없는 것 같다. 그래서 톰캣이 어떻게 작동되는지부터 파악할 필요가 있었다. 1. 클라이언트 커넥션 수락 커넥터(Connector)는 클라이언트와의 통신을 처리하는데 서버의 특정 TCP 포트 번호에서 연결을 수신하고 요청 처리와 응답을 생성한다고 한다. 해당 구현 부분을 보면 다음과 같다. private void connect() { try { process(serverSocket.accept()); } catch (IOException e) { log.error(e.getMessage(), e); } } 대충 예측해 보면 serverSocket이 수락한다는 거 같은데 어떤 메서드일까? Listens for a connection to be made to this socket and accepts it. The method blocks until a connection is made. 이 소켓에 연결이 이루어질 때까지 기다렸다가 연결을 수락한다고 나와 있다. 클라이언트가 요청을 보내면 기다리던 serverSocket이 연결을 수락하고, 클라이언트와 데이터를 주고받을 수 있게 된다. 2. 요청 메시지 수신 이제 연결을 했으니 클라이언트에게 받은 데이터를 읽어야겠죠? @Override public void process(final Socket connection) { try (final var inputStream = connection.getInputStream(); final var outputStream = connection.getOutputStream()) { ... } catch (IOException | UncheckedServletException e) { log.error(e.getMessage(), e); } } InputStream을 통해 HTTP 요청 메시지를 네트워크로부터 읽는다. InputStream은 바이트 기반 입력 스트림 최상위 추상 클래스이고 XXXInputStream이라는 네이밍을 가진 하위 클래스들이 있다. 그리고 문자 단위 입력을 위한 최상위 입력 스트림 클래스인 Reader(ex.BufferedReadaer)를 통해 좀 더 편리하게 문자를 받아볼 수도 있다. 3. 요청 처리, 리소스 매핑과 접근, 응답 생성 클라이언트에게 받은 데이터로 어떤 요청(ex. “POST /login”)을 처리할지 어떤 리소스(ex. login.html)에 접근할지를 정할 수 있을 것이고 이를 기반으로 응답을 생성할 것이다. @Override public void process(final Socket connection) { try (final var inputStream = connection.getInputStream(); final var outputStream = connection.getOutputStream()) { HttpRequest httpRequest = HTTP_REQUEST_PARSER.convertToHttpRequest(inputStream); HttpResponse httpResponse = new HttpResponse(); Controller controller = FRONT_CONTROLLER.handle(httpRequest); controller.process(httpRequest, httpResponse); ... } catch (IOException | UncheckedServletException e) { log.error(e.getMessage(), e); } } 위의 코드는 완성본이긴 하지만 슈도 코드를 살펴보자. InputStream을 통해 읽어들인 메시지를 기반으로 httpRequest를 생성하게 되고 해당 HttpRequest 안에는 HttpMethod, path, protocol 등… 요청 정보가 있다. 그리고 FRONT_CONTROLLER라는 클래스에서 해당 요청안의 path나 method를 보고 어떤 controller를 사용해야 될지 매핑을 해주고 해당 controller 로직을 처리한다. 해당 로직을 처리하게 되면 httpResponse 안에 각종 응답 정보(status code, header, body)들이 들어가게 되고 응답을 반환할 준비가 완료된다. 4. 응답 반환 응답을 반환할 준비가 되었으니 이제 응답을 반환할 수 있다. @Override public void process(final Socket connection) { try (final var inputStream = connection.getInputStream(); final var outputStream = connection.getOutputStream()) { HttpRequest httpRequest = HTTP_REQUEST_PARSER.convertToHttpRequest(inputStream); HttpResponse httpResponse = new HttpResponse(); Controller controller = FRONT_CONTROLLER.handle(httpRequest); controller.process(httpRequest, httpResponse); outputStream.write(httpResponse.joinResponse().getBytes()); outputStream.flush(); } catch (IOException | UncheckedServletException e) { log.error(e.getMessage(), e); } } httpReseponse.joinResponse()를 각종 응답 정보들을 String으로 Join 한 뒤 바이트로 변환해서 write 해주고 OutputStream을 통해 응답 헤더를 포함한 HTTP 응답 메시지를 생성한다. OutputStream은 바이트 기반 출력 스트림 최상위 추상 클래스이고 XXXOutputStream이라는 네이밍을 가진 하위 클래스들이 있다. 구현 과정 우선 요구사항은 다음과 같다. 요구사항 1단계 - HTTP 구현하기 GET /index.html 응답하기 CSS 지원하기 Query String 파싱 2단계 - 로그인 구현하기 HTTP Status Code 302 POST 방식으로 회원가입 Cookie에 JSESSIONID 값 저장하기 Session 구현하기 3단계 - 리팩토링 HttpRequest 클래스 구현하기 HttpResponse 클래스 구현하기 4단계 - 동시성 확장하기 Executors로 Thread Pool 적용 동시성 컬렉션 사용하기 처음에 구현에만 집중했기 때문에 메인 로직이 모두 Http11Processor 클래스에 모여있었다. 그래서 코드 줄 수를 보면 무려 260줄이… 첫 Http11Processor 코드 package org.apache.coyote.http11; import nextstep.jwp.db.InMemoryUserRepository; import nextstep.jwp.exception.UncheckedServletException; import nextstep.jwp.exception.UserNotFoundException; import nextstep.jwp.model.User; import org.apache.coyote.Cookie; import org.apache.coyote.HttpStatus; import org.apache.coyote.Processor; import org.apache.coyote.Sessions; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.File; import java.io.IOException; import java.net.Socket; import java.net.URL; import java.nio.file.Files; import java.util.List; import java.util.Map; import java.util.UUID; import java.util.regex.Pattern; public class Http11Processor implements Runnable, Processor { private static final Logger log = LoggerFactory.getLogger(Http11Processor.class); private static final String STATIC_DIRECTORY = \"static\"; private static final String SPACE = \" \"; private static final String QUERY_STRING_SEPARATOR = \"\\\\?\"; private static final String MULTIPLE_QUERY_STRING_SEPARATOR = \"&amp;\"; private static final String KEY_VALUE_SEPARATOR = \"=\"; private static final String LINE_FEED = \"\\r\\n\"; private static final String HTML_SUFFIX = \".html\"; private static final int PATH_INDEX = 1; private static final int PROTOCOL_INDEX = 2; private static List&lt;String&gt; STATIC_PATH = List.of(\".css\", \".js\"); private final Socket connection; public Http11Processor(final Socket connection) { this.connection = connection; } @Override public void run() { log.info(\"connect host: {}, port: {}\", connection.getInetAddress(), connection.getPort()); process(connection); } @Override public void process(final Socket connection) { try (final var inputStream = connection.getInputStream(); final var outputStream = connection.getOutputStream()) { byte[] bytes = new byte[2048]; inputStream.read(bytes); final String request = new String(bytes); final String response = createResponse(request); outputStream.write(response.getBytes()); outputStream.flush(); } catch (IOException | UncheckedServletException e) { log.error(e.getMessage(), e); } } private String createResponse(String request) throws IOException { String path = getPath(request); String method = getMethod(request); String prevPath = path; Cookie cookie = new Cookie(); Map&lt;String, String&gt; cookies = cookie.getCookies(request); String jsessionid = cookies.get(\"JSESSIONID\"); if (method.equals(\"GET\")) { path = processGetRequest(path, jsessionid); } else if (method.equals(\"POST\")) { if (path.equals(\"/login\")) { path = processLogin(request); } else if (path.equals(\"/register\")) { path = processRegister(request); } } String status = getStatus(prevPath, path); String protocol = getRequestElement(request, PROTOCOL_INDEX); String contentType = getContentType(path); String content = getContent(path); String contentLength = \"Content-Length: \" + content.getBytes().length; String response = protocol + SPACE + status + SPACE + LINE_FEED + getJSessionId(request) + SPACE + LINE_FEED + contentType + SPACE + LINE_FEED + contentLength + SPACE + LINE_FEED + getLocationIfRedirect(status, path) + LINE_FEED + content; return response; } private String getJSessionId(String request) { Cookie cookie = new Cookie(); Map&lt;String, String&gt; cookies = cookie.getCookies(request); if (!cookies.containsKey(\"JSESSIONID\")) { return cookie.createCookie(); } return \"\"; } private String processLogin(String request) { String path; String[] splitRequestBody = getRequestBody(request); String account = splitRequestBody[0].split(KEY_VALUE_SEPARATOR)[1]; String password = splitRequestBody[1].split(KEY_VALUE_SEPARATOR)[1]; try { User user = InMemoryUserRepository.findByAccount(account).orElseThrow(UserNotFoundException::new); addSession(request, user); path = getRedirectPath(password, user); log.info(user.toString()); } catch (UserNotFoundException e) { path = \"/401.html\"; } return path; } private void addSession(String request, User user) { Cookie cookie = new Cookie(); Sessions sessions = new Sessions(); Map&lt;String, String&gt; cookies = cookie.getCookies(request); String jsessionid = cookies.get(\"JSESSIONID\"); sessions.add(jsessionid, user); } private static String[] getRequestBody(String request) { String[] splitRequest = request.split(LINE_FEED); String requestBody = splitRequest[splitRequest.length - 1].trim(); String[] splitRequestBody = requestBody.split(MULTIPLE_QUERY_STRING_SEPARATOR); return splitRequestBody; } private static String processRegister(String request) { String[] splitRequestBody = getRequestBody(request); String account = splitRequestBody[0].split(KEY_VALUE_SEPARATOR)[1]; String email = splitRequestBody[1].split(KEY_VALUE_SEPARATOR)[1]; email = email.replace(\"%40\", \"@\"); String password = splitRequestBody[2].split(KEY_VALUE_SEPARATOR)[1]; InMemoryUserRepository.save(new User(account, password, email)); return \"/index.html\"; } private String getLocationIfRedirect(String status, String path) { if (status.startsWith(\"302\")) { return \"Location: \" + path + SPACE + LINE_FEED; } return \"\"; } private String getMethod(String request) { return getRequestElement(request, 0); } private String getStatus(String prevPath, String path) { if (!isSamePage(prevPath, path) &amp;&amp; !prevPath.equals(path)) { return HttpStatus.REDIRECT.getHttpStatusCode() + SPACE + HttpStatus.REDIRECT.getHttpStatusMessage(); } return HttpStatus.OK.getHttpStatusCode() + SPACE + HttpStatus.OK.getHttpStatusMessage(); } private static boolean isSamePage(String prevPath, String path) { return (prevPath + HTML_SUFFIX).equals(path); } private String processGetRequest(String path, String jSessionId) { if (isRequest(path)) { if (haveQueryString(path)) { path = processLogin(path); return path; } if (path.equals(\"/login\")) { Sessions sessions = new Sessions(); if (sessions.isAlreadyLogin(jSessionId)) { return \"/index.html\"; } } return path + HTML_SUFFIX; } return path; } private String processLogin(String path) { String queryString = splitQueryString(path)[1]; String[] splitQueryString = queryString.split(\"&amp;\"); String account = splitQueryString[0].split(KEY_VALUE_SEPARATOR)[1]; String password = splitQueryString[1].split(KEY_VALUE_SEPARATOR)[1]; try { User user = InMemoryUserRepository.findByAccount(account).orElseThrow(UserNotFoundException::new); path = getRedirectPath(password, user); log.info(user.toString()); return path; } catch (UserNotFoundException e) { return \"/401.html\"; } } private String getRedirectPath(String password, User user) { String path; if (user.checkPassword(password)) { path = \"/index.html\"; } else { path = \"/401.html\"; } return path; } private boolean haveQueryString(String path) { Pattern pattern = Pattern.compile(QUERY_STRING_SEPARATOR); return pattern.matcher(path).find(); } private String getContentType(String path) { String contentType = \"Content-Type: \"; if (isStaticPath(path)) { return contentType + \"text/css;charset=utf-8\"; } return contentType + \"text/html;charset=utf-8\"; } private boolean isStaticPath(String path) { for (String staticPath : STATIC_PATH) { if (path.endsWith(staticPath)) { return true; } } return false; } private String getContent(String path) throws IOException { if (path.equals(\"/\")) { return \"Hello world!\"; } URL resource = getClass().getClassLoader().getResource(STATIC_DIRECTORY + path); return new String(Files.readAllBytes(new File(resource.getFile()).toPath())); } private String getPath(String request) { return getRequestElement(request, PATH_INDEX); } private boolean isRequest(String path) { return !isStaticPath(path) &amp;&amp; !path.endsWith(HTML_SUFFIX); } private String[] splitQueryString(String path) { return path.split(QUERY_STRING_SEPARATOR); } private String getRequestElement(String request, int index) { return request.split(SPACE + \"|\" + LINE_FEED)[index]; } } Oh My Goodness… 🤮🤮🤮 하나의 클래스에 책임과 역활이 너무 많다. 하나씩 분리해보자 HttpRequestParser, HttpRequest 분리 HTTP 요청 메시지를 보면 위와 같이 매우 복잡하게 되어있다. start-line, header, empty line, message body가 있고 또 그 안에서도 나뉜다. 그래서 구현할 때 제일 거슬렸던 게 읽는 부분이었다. 읽을 때마다 split을 하고 split에 몇 번째를 가져오고 또 그걸 split을 하고 ㅋㅋㅋ 그래서 이 부분을 가장 먼저 분리하자고 마음먹었다. 그렇게 다음과 같이 HttpRequestParser을 통해 데이터를 읽고 HttpRequest를 만들어 낼 수 있도록 분리하였다. HttpRequestParser, HttpRequest 코드 package org.apache.coyote.http; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStream; import java.io.InputStreamReader; import java.util.HashMap; import java.util.Map; public class HttpRequestParser { private static final int KEY_INDEX = 0; private static final int VALUE_INDEX = 1; public HttpRequest convertToHttpRequest(InputStream inputStream) throws IOException { InputStreamReader inputStreamReader = new InputStreamReader(inputStream); BufferedReader bufferedReader = new BufferedReader(inputStreamReader); String firstLine = bufferedReader.readLine(); Map&lt;String, String&gt; header = readHeader(bufferedReader); return new HttpRequest(new StartLine(firstLine), header, readMessageBody(bufferedReader, header)); } private Map&lt;String, String&gt; readHeader(BufferedReader bufferedReader) throws IOException { Map&lt;String, String&gt; header = new HashMap&lt;&gt;(); String line = bufferedReader.readLine(); while (line != null &amp;&amp; !line.isBlank()) { String[] split = line.split(\":\"); header.put(split[KEY_INDEX], split[VALUE_INDEX].strip()); line = bufferedReader.readLine(); } return header; } private String readMessageBody(BufferedReader bufferedReader, Map&lt;String, String&gt; header) throws IOException { String messageBody = \"\"; String contentLengthName = HttpHeader.CONTENT_LENGTH.getName(); if (header.containsKey(contentLengthName)) { int contentLength = Integer.parseInt(header.get(contentLengthName)); char[] body = new char[contentLength]; bufferedReader.read(body, 0, contentLength); messageBody = new String(body); } return messageBody; } } package org.apache.coyote.http; import java.util.Arrays; import java.util.Map; import java.util.stream.Collectors; public class HttpRequest { private static final String KEY_VALUE_DELIMITER = \"=\"; private static final int KEY_INDEX = 0; private static final int VALUE_INDEX = 1; private StartLine startLine; private Map&lt;String, String&gt; header; private Map&lt;String, String&gt; cookies; private String messageBody; public HttpRequest(StartLine startLine, Map&lt;String, String&gt; header, String messageBody) { this.startLine = startLine; this.header = header; cookies = findCookies(); this.messageBody = messageBody; } private Map&lt;String, String&gt; findCookies() { return header.entrySet().stream() .filter(entry -&gt; entry.getKey().equals(\"Cookie\")) .map(entry -&gt; entry.getValue().split(\"; \")) .flatMap(Arrays::stream) .map(line -&gt; line.split(KEY_VALUE_DELIMITER)) .collect(Collectors.toMap(line -&gt; line[KEY_INDEX], line -&gt; line[VALUE_INDEX])); } public boolean containsCookie(String key) { return cookies.containsKey(key); } public String getCookie(String key) { return cookies.get(key); } public void addHeader(String key, String value) { header.put(key, value); } public HttpMethod getMethod() { return startLine.getMethod(); } public String getPath() { return startLine.getPath(); } public Map&lt;String, String&gt; getQueryStrings() { return startLine.getQueryStrings(); } public HttpProtocol getProtocol() { return startLine.getProtocol(); } public Map&lt;String, String&gt; getHeader() { return header; } public String getMessageBody() { return messageBody; } } RequestBody를 읽을 때 무한루프에 빠지는 현상 HttpRequest를 좀 더 편리하게 읽기 위해 BufferedReader의 readLine()을 통해 읽었다. 하지만, 어느 부분에서 계속해서 무한 루프가 걸려서 브라우저가 계속 대기하는 현상이 일어났다. 해당 부분을 찾기 쉽지 않아 모든 곳에 디버깅을 걸어가며 확인한 결과 BufferedReader가 RequestBody 부분을 읽을 때 제대로 읽지 못하고 무한 루프가 발생하는 것을 발견하였다. 흠.. 수많은 삽질과 추론을 하다가 BufferedReader의 readLine()을 읽게 되었는데 다음과 같은 설명이 적혀있다. Reads a line of text. A line is considered to be terminated by any one of a line feed (‘\\n’), a carriage return (‘\\r’), a carriage return followed immediately by a line feed, or by reaching the end-of-file (EOF). 텍스트 한 줄을 읽는데 줄 바꿈(‘\\n’), 캐리지 리턴(‘\\r’), 캐리지 리턴 다음 바로 줄 바꿈(‘\\r\\n’)이 오거나 EOF에 도달해야 종료된 것으로 간주한다는 것이다. 이걸 보고 HttpRequest의 내용을 쳐다봤다. start-line + CRLF + header + CRLF + message-body인데 message-body의 끝부분을 보면 위에서 해당하는 어떠한 것도 없다. 그래서 끝난지 모르고 계속해서 무한 루프를 돌고 있던 것이다! 🫢 그래서 RequestBody는 다음과 같이 read를 이용하여 contentLength 만큼 추가로 더 읽어주었다. private String readMessageBody(BufferedReader bufferedReader, Map&lt;String, String&gt; header) throws IOException { String messageBody = \"\"; String contentLengthName = HttpHeader.CONTENT_LENGTH.getName(); if (header.containsKey(contentLengthName)) { int contentLength = Integer.parseInt(header.get(contentLengthName)); char[] body = new char[contentLength]; bufferedReader.read(body, 0, contentLength); messageBody = new String(body); } return messageBody; } HttpResponseBuilder, HttpResponse 분리 응답도 위와 같이 start-line, header, message body를 재구성 해줘야 되기 때문에 굉장히 중복된 부분이 많았다. 그래서 그다음 분리 대상으로 삼았고 HttpResponseBudiler를 이용해 HttpResponse를 생성할 수 있도록 분리하였다. 그리고 헤더를 추가할 때 기존의 헤더가 있으면 그 헤더에 추가적으로 추가할 수 있게 Header라는 추상 클래스를 만들고 콤마 구분자 헤더(ex. Cache-Control: no-cache, no-store, must-revalidate, max-age=0)와, 세미콜론 구분자 헤더(ex. Set-Cookie: a=b; c=d)를 분리했다. HttpResponseBuilder, HttpResponse, Header 코드 package org.apache.coyote.http; import java.io.IOException; public class HttpResponseBuilder { private static final String LINE_FEED = \"\\r\\n\"; private static final String SPACE = \" \"; private HttpResponseBuilder() { } public static void buildStaticFileOkResponse(HttpRequest httpRequest, HttpResponse httpResponse, String path) throws IOException { try { httpResponse.updateFileMessageBody(path); } catch (NullPointerException e) { buildStaticFileNotFoundResponse(httpRequest, httpResponse); } String status = joinStatus(HttpStatus.OK.getHttpStatusCode(), HttpStatus.OK.getHttpStatusMessage()); String protocol = httpRequest.getProtocol().getName(); String startLine = joinStartLine(status, protocol); httpResponse.updateStartLine(startLine); httpResponse.addHeader(HttpHeader.CONTENT_TYPE.getName(), ContentType.findType(path)); httpResponse.addHeader(HttpHeader.CONTENT_LENGTH.getName(), String.valueOf(httpResponse.getMessageBody().getBytes().length)); } public static void buildStaticFileRedirectResponse(HttpRequest httpRequest, HttpResponse httpResponse, String redirectPath) throws IOException { String status = joinStatus(HttpStatus.REDIRECT.getHttpStatusCode(), HttpStatus.REDIRECT.getHttpStatusMessage()); String protocol = httpRequest.getProtocol().getName(); String startLine = joinStartLine(status, protocol); httpResponse.updateStartLine(startLine); httpResponse.updateFileMessageBody(redirectPath); httpResponse.addHeader(HttpHeader.LOCATION.getName(), redirectPath); httpResponse.addHeader(HttpHeader.CONTENT_TYPE.getName(), ContentType.HTML.getType()); httpResponse.addHeader(HttpHeader.CONTENT_LENGTH.getName(), String.valueOf(httpResponse.getMessageBody().getBytes().length)); } private static String joinStatus(String statusCode, String statusMessage) { return statusCode + SPACE + statusMessage; } private static String joinStartLine(String status, String protocol) { return protocol + SPACE + status + SPACE + LINE_FEED; } public static void buildStaticFileNotFoundResponse(HttpRequest httpRequest, HttpResponse httpResponse) throws IOException { String status = joinStatus(HttpStatus.NOT_FOUND.getHttpStatusCode(), HttpStatus.NOT_FOUND.getHttpStatusMessage()); String protocol = httpRequest.getProtocol().getName(); String startLine = joinStartLine(status, protocol); httpResponse.updateStartLine(startLine); httpResponse.updateFileMessageBody(\"/404.html\"); httpResponse.addHeader(HttpHeader.CONTENT_TYPE.getName(), ContentType.HTML.getType()); httpResponse.addHeader(HttpHeader.CONTENT_LENGTH.getName(), String.valueOf(httpResponse.getMessageBody().getBytes().length)); } public static void buildCustomResponse(HttpRequest httpRequest, HttpResponse httpResponse, String content) { String status = joinStatus(HttpStatus.OK.getHttpStatusCode(), HttpStatus.OK.getHttpStatusMessage()); String protocol = httpRequest.getProtocol().getName(); String startLine = joinStartLine(status, protocol); httpResponse.updateStartLine(startLine); httpResponse.updateMessageBody(content); httpResponse.addHeader(HttpHeader.CONTENT_TYPE.getName(), ContentType.HTML.getType()); httpResponse.addHeader(HttpHeader.CONTENT_LENGTH.getName(), String.valueOf(httpResponse.getMessageBody().getBytes().length)); } } package org.apache.coyote.http; import java.io.File; import java.io.IOException; import java.net.URL; import java.nio.file.Files; import java.util.Arrays; import java.util.HashMap; import java.util.List; import java.util.Map; public class HttpResponse { private static final String KEY_VALUE_DELIMITER = \"=\"; private static final String LINE_FEED = \"\\r\\n\"; private static final String SPACE = \" \"; private static final String STATIC_DIRECTORY = \"static\"; private String startLine; private Map&lt;String, Header&gt; headers = new HashMap&lt;&gt;(); private String messageBody; public void updateStartLine(String startLine) { this.startLine = startLine; } public void updateMessageBody(String messageBody) { this.messageBody = messageBody; } public void updateFileMessageBody(String path) throws IOException { URL resource = getClass().getClassLoader().getResource(STATIC_DIRECTORY + path); messageBody = new String(Files.readAllBytes(new File(resource.getFile()).toPath())); } public void addHeader(String key, String value) { Header header = headers.computeIfAbsent(key, ignore -&gt; new CommaSeperatedHeader()); header.add(value); } public void addHeader(String key, List&lt;String&gt; values) { Header header = headers.computeIfAbsent(key, ignore -&gt; new CommaSeperatedHeader()); header.addAll(values); } public void addCookie(String key, String value) { Header header = headers.computeIfAbsent(HttpHeader.COOKIE.getName(), ignore -&gt; new SemicolonSeperatedHeader()); header.add(key + KEY_VALUE_DELIMITER + value); } public String joinResponse() { return startLine + joinCookie() + joinHeaderWithoutCookie() + LINE_FEED + messageBody; } private String joinHeaderWithoutCookie() { String headersWithoutCookie = this.headers.entrySet().stream() .filter(entry -&gt; !entry.getKey().equals(HttpHeader.COOKIE.getName())) .map(entry -&gt; entry.getKey() + \": \" + entry.getValue().getValues()) .reduce((header1, header2) -&gt; header1 + SPACE + LINE_FEED + header2) .orElse(\"\"); return headersWithoutCookie + SPACE + LINE_FEED; } private String joinCookie() { if (isStaticPath() || !headers.containsKey(HttpHeader.COOKIE.getName())) { return \"\"; } String cookieHeader = headers.get(HttpHeader.COOKIE.getName()).getValues(); String cookieHeaderResponse = Arrays.stream(cookieHeader.split(\"; \")) .map(line -&gt; \"Set-Cookie: \" + line) .reduce((cookie1, cookie2) -&gt; cookie1 + SPACE + LINE_FEED + cookie2) .orElse(\"\"); return cookieHeaderResponse + SPACE + LINE_FEED; } private boolean isStaticPath() { String contentType = headers.get(HttpHeader.CONTENT_TYPE.getName()).getValues(); return ContentType.isStaticFile(contentType); } public String getStartLine() { return startLine; } public Map&lt;String, Header&gt; getHeaders() { return headers; } public String getMessageBody() { return messageBody; } } package org.apache.coyote.http; import java.util.Collection; public abstract class Header { protected final Collection&lt;String&gt; values; protected Header(Collection&lt;String&gt; values) { this.values = values; } public void add(String value) { values.add(value); } public void addAll(Collection&lt;String&gt; values) { this.values.addAll(values); } abstract String getValues(); } package org.apache.coyote.http; import java.util.ArrayList; public class CommaSeperatedHeader extends Header { protected CommaSeperatedHeader() { super(new ArrayList&lt;&gt;()); } @Override String getValues() { return String.join(\", \", values); } } package org.apache.coyote.http; import java.util.ArrayList; public class SemicolonSeperatedHeader extends Header { protected SemicolonSeperatedHeader() { super(new ArrayList&lt;&gt;()); } @Override String getValues() { return String.join(\"; \", values); } } FrontController, Controller 분리 그렇게 request와 response 부분을 분리하니 앞뒤는 깔끔했지만 중간 부분이 굉장히 더러웠다. 특히 엄청나게 많은 분기문 + if 중첩문이 합쳐져서 가독성이 매우 구렸다. 이를 해결하기 위해서는 method와 path에 따라 해당 로직을 처리해주는 클래스를 매핑해주는 객체가 필요했다. 그리고 이 객체가 반환해 주는 클래스를 추상화한 객체까지. 그렇게 여러 분기의 Controller와 이 Controller를 공통화할 Controller Interface, 이 컨트롤러를 매핑해줄 객체가 생성되었다. 매핑해주는 객체 이름을 FrontController으로 지은 이유는 여러 Controller 앞에서 요청을 받아서 해당 Controller로 매핑해주기 때문이었다. FrontController, Controller 코드 package nextstep.jwp.presentation.handler; import nextstep.jwp.presentation.Controller; import nextstep.jwp.presentation.LoginController; import nextstep.jwp.presentation.NotFoundController; import nextstep.jwp.presentation.RegisterController; import nextstep.jwp.presentation.RootController; import nextstep.jwp.presentation.StaticController; import org.apache.coyote.http.HttpRequest; import java.util.HashMap; import java.util.List; import java.util.Map; public class FrontController { private static final List&lt;String&gt; STATIC_PATH = List.of(\".css\", \".js\", \".ico\", \".html\", \".svg\"); private static final Controller NOT_FOUND_CONTROLLER = new NotFoundController(); private static final Controller STATIC_CONTROLLER = new StaticController(); private final Map&lt;String, Controller&gt; mappingControllers = new HashMap&lt;&gt;(); public FrontController() { mappingControllers.put(\"/\", new RootController()); mappingControllers.put(\"/login\", new LoginController()); mappingControllers.put(\"/register\", new RegisterController()); } public Controller handle(HttpRequest httpRequest) { String path = httpRequest.getPath(); if (isStaticPath(path)) { return STATIC_CONTROLLER; } if (mappingControllers.containsKey(path)) { return mappingControllers.get(path); } return NOT_FOUND_CONTROLLER; } private boolean isStaticPath(String path) { return STATIC_PATH.stream().anyMatch(path::endsWith); } } package nextstep.jwp.presentation; import org.apache.coyote.http.HttpRequest; import org.apache.coyote.http.HttpResponse; import java.io.IOException; public interface Controller { void process(HttpRequest httpRequest, HttpResponse httpResponse) throws IOException; } 하나씩 분리한 결과 이렇게 하나하나 책임을 나눠주기 위해 분리한 결과 엄청나게 뚱뚱하던 Processor가 다음과 같이 성공적으로 다이어트를 하게 되었다. package org.apache.coyote.http11; import nextstep.jwp.exception.UncheckedServletException; import nextstep.jwp.presentation.Controller; import nextstep.jwp.presentation.handler.FrontController; import org.apache.coyote.Processor; import org.apache.coyote.http.HttpRequest; import org.apache.coyote.http.HttpRequestParser; import org.apache.coyote.http.HttpResponse; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.IOException; import java.net.Socket; public class Http11Processor implements Runnable, Processor { private static final Logger log = LoggerFactory.getLogger(Http11Processor.class); private static final FrontController FRONT_CONTROLLER = new FrontController(); private static final HttpRequestParser HTTP_REQUEST_PARSER = new HttpRequestParser(); private final Socket connection; public Http11Processor(final Socket connection) { this.connection = connection; } @Override public void run() { log.info(\"connect host: {}, port: {}\", connection.getInetAddress(), connection.getPort()); process(connection); } @Override public void process(final Socket connection) { try (final var inputStream = connection.getInputStream(); final var outputStream = connection.getOutputStream()) { HttpRequest httpRequest = HTTP_REQUEST_PARSER.convertToHttpRequest(inputStream); HttpResponse httpResponse = new HttpResponse(); Controller controller = FRONT_CONTROLLER.handle(httpRequest); controller.process(httpRequest, httpResponse); outputStream.write(httpResponse.joinResponse().getBytes()); outputStream.flush(); } catch (IOException | UncheckedServletException e) { log.error(e.getMessage(), e); } } } 현재 분리한 나의 구조는 다음과 같다. 근데 이렇게 하나하나 분리해서 구현하다 보니깐 자연스럽게 Servlet의 구조와 비슷하게 되는 것 같았다. 이미 머리가 스프링에 너무 절여져서 그런가…? 뭐가 되었든 굉장히 의미있는 경험이었고 Tomcat과 좀 더 친해진 거 같다. 벌써부터 다음 미션인 MVC 구현하기가 매우 기대된다. 🤗 실제 톰캣은 어떻게 구현되어 있을까? 미션을 시작할 때 톰캣을 보지 않고 구현하고 나중에 실제 톰캣 코드와 비교해 보고 싶었다. 과연 실제 톰캣은 어떻게 구현되어 있을까? 내가 처리한 것들을 어떻게 효율적으로 처리하고 있는지, 엣지 케이스는 어떻게 처리해 주고 있는지 궁금했다. 우선 톰캣 패키지를 보면 Catalina, Coyote 패키지가 있다. Catalina 패키지는 서블릿 컨테이너 역할을 하며, Coyote 패키지는 톰캣에 TCP를 통한 프로토콜을 지원하는 역할을 한다. 처음에 요청이 들어오면 Connector를 통해 연결이 되고 Coyote 패키지의 Http11Processor 클래스 service() 메서드가 실행이 된다. class Http11Processor { @Override public SocketState service(SocketWrapperBase&lt;?&gt; socketWrapper) throws IOException { parse(); ... getAdapter().service(reuqest, reaponse); ... } } 요청으로 들어온 부분에 대해 파싱이 일어나고 그리고 getAdapter().service(request, response) 메서드를 호출한다. 해당 메서드를 호출하게 되면 Catalina 패키지에 있는 StandardWrapperValve 클래스의 invoke() 메서드가 실행되게 되고 다음과 같은 로직을 통해 알맞은 서블릿을 할당한다. StandardWrapperValve 클래스는 개별 서블릿 정의를 나타내는 Wrapper 인터페이스의 표준 구현이다. class StandardWrapperValve { // Allocate a servlet instance to process this request @Override public void invoke(Request request, Response response) throws IOException, ServletException { ... try { if (!unavailable) { servlet = wrapper.allocate(); } ... } ... } 이렇게 서블릿을 할당하고 나면 해당 요청에 대한 filter chain을 호출한다. 해당 filter chain을 호출했을 때 다음에 호출할 필터가 있으면 계속해서 호출하며 작업을 처리하고 없으면 할당된 servlet의 service() 메서드를 실행하며 마무리한다. 그렇게 서블릿의 해당 로직을 처리하고 나서 응답을 반환하면 끝이 난다. // Call the filter chain for this request // NOTE: This also calls the servlet's service() method Container container = this.container; try { if ((servlet != null) &amp;&amp; (filterChain != null)) { // Swallow output if needed if (context.getSwallowOutput()) { try { SystemLogHandler.startCapture(); if (request.isAsyncDispatching()) { request.getAsyncContextInternal().doInternalDispatch(); } else { filterChain.doFilter(request.getRequest(), response.getResponse()); } } finally { String log = SystemLogHandler.stopCapture(); if (log != null &amp;&amp; log.length() &gt; 0) { context.getLogger().info(log); } } } else { if (request.isAsyncDispatching()) { request.getAsyncContextInternal().doInternalDispatch(); } else { filterChain.doFilter(request.getRequest(), response.getResponse()); } } } } ... 해당 과정을 간단하게 그림으로 나타내면 다음과 같다. 실제 구현되어 있는 톰캣과 흐름을 비교해 보면 비슷한 걸 확인할 수 있다. 근데 실제 톰캣 코드를 보다 보면 코드가 굉장히 길고 지저분한 걸 볼 수 있다. 대형 오픈소스 프로젝트니 어쩔 수 없겠지만 과정을 하나하나 추적하며 직접 찾아보는 게 너무 힘들었다ㅜㅜ 내가 FrontController라고 지은 객체는 톰캣에서는 Container(참고로, StandardWrapper가 Container 인터페이스를 implements 하고 있다) 와 Controller는 Servlet과 대칭되고 있는걸 확인할 수 있었다. 나는 이미 스프링에 머리 절여진 것 확인 🫠 톰캣 컨트리뷰트 코드를 분석 하는데 솔직히 말하면 코드가 정말 가독성이 좋지 않아 보기 힘들었다. 이미 오픈소스 프로젝트를 참여하고 있어서 오픈 소스 코드나 문화에 대해 익숙했기 때문에 톰캣 오픈소스도 기여해 볼 수 있지 않을까 생각했다. 그래서 이전에 보다가 조금 불편한 부분인 상수 컨벤션 불일치와 스위치문에 매직 넘버 사용 부분을 리팩터링 해 PR을 제출했다. Unify constant delimiters and Refactoring for better readability 상수 컨벤션을 변경하면 3rd-party와 통합이 중단될 위험성도 있기 때문에 정말 타당한 이유가 없는 한은 변경하고 싶지 않다고 하여 상수 컨벤션 부분은 롤백 하였다. 하지만, 매직 넘버 부분은 받아들여져 결국 Merge가 되었다! 물론 오픈소스에 처음으로 기여한 것은 아니라 미친 듯이 설렌 건 아니지만 그래도 숨길 수 없는 입꼬리.. 🫢 참고: 톰캣 컨트리뷰터 전비버 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse tomcat",
    "url": "/woowacourse/2023-09-12-tomcat/"
  },{
    "title": "ConcurrentHashMap에 대해 알아보자",
    "text": "ConcurrentHashMap을 알아보기 전에 왜 ConcurrentHashMap을 사용해야 되는지, 다른 Map 구현체와의 차이에 대해서 살펴보자. Map 구현체 Map 인터페이스의 구현체로 HashMap, HashTable, ConcurrentHashMap 등이 있다. 이 셋은 과연 어떤 차이가 있을까? Key, Value에 Null을 허용? 우선 HashMap 같은 경우 key와 value에 null값을 허용한다. @Test void hashmap() { Map&lt;String, String&gt; hashMap = new HashMap&lt;&gt;(); hashMap.put(\"a\", null); hashMap.put(null, \"a\"); assertThat(hashMap.get(null)).isEqualTo(\"a\"); assertThat(hashMap.get(\"a\")).isNull(); } HashTable은 key와 value에 null이 들어가게 되면 NullPointerException이 발생한다. @Test void hashtable() { Hashtable&lt;String, String&gt; hashtable = new Hashtable&lt;&gt;(); assertThrows(NullPointerException.class, () -&gt; hashtable.put(null, \"a\")); assertThrows(NullPointerException.class, () -&gt; hashtable.put(\"a\", null)); } ConcurrentHashMap도 Hashtable과 같이 key와 value에 null이 들어가게 되면 NullPointerException이 발생한다. @Test void concurrentHashmap() { Map&lt;String, String&gt; concurrentHashMap = new ConcurrentHashMap&lt;&gt;(); assertThrows(NullPointerException.class, () -&gt; hashMap.put(\"a\", null)); assertThrows(NullPointerException.class, () -&gt; hashMap.put(null, \"a\")); } 동기화 보장? HashMap 같은 경우 thread-safe 하지 않아 싱글 스레드 환경에서 사용해야 한다. 동기화 처리를 하지 않기 때문에 데이터 탐색하는 속도는 HashTable과 ConcurrentHashMap보다 빠르다. 하지만, 안정성이 떨어지기 때문에 멀티 스레드 환경에서는 사용하면 좋지 않다. HashTable과, ConcurrentHashMap은 thread-safe 하기 때문에 멀티 스레드 환경에서도 사용할 수 있다. HashTable은 synchronized 키워드를 이용해서 스레드간 동기화 락을 걸어 멀티 스레드 환경에서도 안전하지만, 스레드간 동기화 락은 매우 느리다. 하지만, ConcurrentHashMap 같은 경우 Entry 아이템별로 락을 걸어 성능이 HashTable 보다 빠르기 때문에 멀티 스레드 환경에서는 ConcurrentHashMap을 사용하고 싱글 스레드 환경이 보장된다면 HashMap을 쓰자 (ConcurrentHashMap의 동작 원리는 좀 있다 설명) //HashTable public synchronized V put(K key, V value) { ... } public synchronized V remove(Object key) { ... } ... HashTable과 ConcurrentHashMap의 Lock을 거는 원리를 그림으로 표현하면 다음과 같다. ConcurrentHashMap 자 이제 Concurrent HashMap을 왜 사용해야 되는지에 대해서 알았다. 간단히 요약하자면, Thread safe 하기 때문에 멀티 스레드 환경에서도 안전하고 멀티 스레드 환경에서도 Entry 아이템별로 락을 걸기 때문에 성능이 좋다. 여기서 궁금한 게 그럼 HashTable은 왜 있을까? 이걸 알기 위해서는 나온 순서를 따져봐야 된다. HashTable은 Java 1.0에 등장하여 제공되었는데 속도가 느리고 데이터를 처리할 수 있는 메서드가 부족했다. Java 1.2가 등장하면서 속도가 빠른 HashMap이 제공되었는데 하지만 HashMap은 동기화가 지원되지 않기 때문에 멀티 스레드에서 쓰기 어렵다. 그래서 자바 1.5부터 ConcurrentHashMap이 등장했는데 HashTable 클래스의 단점을 보완하면서도 HashMap 클래스와 다르게 멀티 스레드 환경에서도 사용할 수 있는 클래스다. +추가적으로 다음과 같이 HashMap을 사용하고도 동기화 문제를 해결할 수도 있다. Collections.synchronizedMap(new HashMap&lt;&gt;); ConcurrentHashMap 동작 원리 이번에는 ConcurrentHashMap의 get()과 put() 메서드에 대해서만 알아보겠다. 나머지가 궁금하면 직접 분석해보자! get() public V get(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null; } get() 메서드를 보면 synchronized 키워드가 존재하지 않는 걸 볼 수 있다. 즉, get() 메서드에서는 동기화가 일어나지 않고 동시에 put(), remove() 등 메서드와 동시에 수행될 수 있다. put() public V put(K key, V value) { return putVal(key, value, false); } /** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; K fk; V fv; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else if (onlyIfAbsent // check first node without acquiring lock &amp;&amp; fh == hash &amp;&amp; ((fk = f.key) == key || (fk != null &amp;&amp; key.equals(fk))) &amp;&amp; (fv = f.val) != null) return fv; else { V oldVal = null; synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value); break; } } } else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } else if (f instanceof ReservationNode) throw new IllegalStateException(\"Recursive update\"); } } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } 위의 코드를 살펴보면 부분적으로 synchronized가 적용된 것을 볼 수 있다. 즉, 항상 동기화 처리가 되는 것이 아니라 해당 조건에서만 동기화 처리가 되기 때문에 성능이 향상된다. put() 메서드는 크게 else 위아래로 2가지 분기로 나눠지는데 첫 번째 분기가 새로운 노드가 들어갈 배열의 인덱스가 비어 있는 경우이고 두 번째 분기는 이미 기존 노드가 있는 경우이다. //첫 번째 분기 final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; K fk; V fv; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value))) break; } ... } static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) { return (Node&lt;K,V&gt;)U.getObjectAcquire(tab, ((long)i &lt;&lt; ASHIFT) + ABASE); } static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) { return U.compareAndSetObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); } 첫 번째 분기는 빈 bucket에 노드를 삽입하는 경우 Compare and Swap을 이용하여 lock을 사용하지 않고 새로운 Node를 삽입한다. Compare and Swap: 비교 및 스왑은 동시 알고리즘을 설계할 때 사용되는 기법으로 변수의 값을 예상 값과 비교하고 값이 같으면 변수의 값을 새로운 값으로 바꾼다. ConcurrentHashMap의 내부 가변 배열 table을 돌면서 해당 bucket을 가져온다. 해당 bucket이 비어있으면 casTabAt()을 이용해 Node를 담고 있는 volatile 변수에 접근하고 null이면 Node을 생성해 넣는다. 두 번째 분기는 이미 Bucket에 Node가 존재하는 경우 synchronized를 이용해 다른 스레드가 접근하지 못하도록 lock을 걸어 다른 thread가 같은 hash bucket에 접근이 불가능해진다.(여기가 핵심) //두 번째 분기 ... else { V oldVal = null; synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value); break; } } } else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } else if (f instanceof ReservationNode) throw new IllegalStateException(\"Recursive update\"); } } ... 현재 f는 위에서 이미 비어있는지 확인을 하고 왔기 때문에 비어있지 않은 bucket을 의미한다. 해당 버킷에서 동일한 Key 이면 Node를 새로운 노드로 바꾼다. 해시 충돌이면 Seperate Chaining이나 TreeNode에 추가한다. 그렇다면 어떻게 동시성을 테스트 해볼 수 있을까? 이번 우테코 톰켓 구현하기 미션에서 SessionManager를 ConcurrentHashMap으로 변경하라는 요구사항이 있었다. 물론 검수가 완료되었으니깐 해당 클래스로 등장했겠지만 처음 써보는 입장에서 ConcurrentHashMap이 동시성을 보장해 주는지 테스트가 하고 싶은 법… private static final int maxThreads = 10; private static final HashMap&lt;String, Integer&gt; hashMap = new HashMap&lt;&gt;(); private static final Hashtable&lt;String, Integer&gt; hashtable = new Hashtable&lt;&gt;(); private static final ConcurrentHashMap&lt;String, Integer&gt; concurrentHashMap = new ConcurrentHashMap&lt;&gt;(); private static final Map&lt;String, Integer&gt; synHashMap = Collections.synchronizedMap(new HashMap&lt;&gt;()); @Test @RepeatedTest(1000) void concurrencyTest() { ExecutorService executorService = Executors.newFixedThreadPool(maxThreads); for (int i = 0; i &lt; maxThreads; i++) { executorService.execute(() -&gt; { for (int j = 0; j &lt; 10000; j++) { String key = String.valueOf(j); hashMap.put(key, j); hashtable.put(key, j); concurrentHashMap.put(key, j); synHashMap.put(key, j); } }); } executorService.shutdown(); try { executorService.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"hashMap size : \" + hashMap.size()); // -&gt; 10205 assertThat(hashMap.size()).isNotEqualTo(10000); System.out.println(\"hashtable size : \" + hashtable.size()); // -&gt; 10000 assertThat(hashtable.size()).isEqualTo(10000); System.out.println(\"concurrentHashMap size : \" + concurrentHashMap.size()); // -&gt; 10000 assertThat(concurrentHashMap.size()).isEqualTo(10000); System.out.println(\"synHashMap size : \" + synHashMap.size()); // -&gt; 10000 assertThat(synHashMap.size()).isEqualTo(10000); } 결과를 보면 HashMap은 동기화가 이뤄지지 않아 size가 올바르지 않게 나오는 걸 확인해 볼 수 있다. Thread safe한 Map 간 성능 테스트 한 김에 각 Thread safe 한 map들의 성능들은 어느 정도 차이가 나는지 테스트해보자. @Test @RepeatedTest(5) void performanceTest() throws InterruptedException { Hashtable&lt;Integer, Integer&gt; hashtable = new Hashtable&lt;&gt;(); Map&lt;Integer, Integer&gt; synHashMap = Collections.synchronizedMap(new HashMap&lt;&gt;()); ConcurrentHashMap&lt;Integer, Integer&gt; concurrentHashMap = new ConcurrentHashMap&lt;&gt;(); long hashtableTime = measure(hashtable); long synHashMapTime = measure(synHashMap); long concurrentHashMapTime = measure(concurrentHashMap); System.out.println(\"hashTableTime = \" + hashtableTime); System.out.println(\"synHashMapTime = \" + synHashMapTime); System.out.println(\"concurrentHashMapTime = \" + concurrentHashMapTime); System.out.println(); } private static long measure(Map&lt;Integer, Integer&gt; map) throws InterruptedException { ExecutorService executorService = Executors.newFixedThreadPool(maxThreads); int count = 200000; long startTime = System.nanoTime(); for (int i = 0; i &lt; maxThreads; i++) { executorService.submit(() -&gt; { for (int j = 0; j &lt; count; j++) { int value = ThreadLocalRandom.current().nextInt(); map.put(value, value); map.get(value); } }); } executorService.shutdown(); try { executorService.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS); } catch (InterruptedException e) { e.printStackTrace(); } long endTime = System.nanoTime(); return (endTime - startTime) / count * maxThreads; } 스레드 10개와 200,000개의 데이터를 이용해서 성능을 비교하니 다음과 같이 나왔다. 5번 반복한 결과이다. 부분적으로 Lock을 거는 ConcurrentHashMap의 성능이 매우 좋은 걸 볼 수 있다. 이렇게 잘 만든 Java 개발자님들 Respect 🫡 hashTableTime = 54870 synHashMapTime = 61450 concurrentHashMapTime = 37080 hashTableTime = 46170 synHashMapTime = 50840 concurrentHashMapTime = 34470 hashTableTime = 50980 synHashMapTime = 54490 concurrentHashMapTime = 25080 hashTableTime = 57710 synHashMapTime = 56680 concurrentHashMapTime = 24020 hashTableTime = 51490 synHashMapTime = 53030 concurrentHashMapTime = 22930 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse Concurrent-Hashmap",
    "url": "/woowacourse/2023-09-09-Concurrent-Hashmap/"
  },{
    "title": "집사의고민 2, 3차 데모데이 회고",
    "text": "2, 3차 데모데이 회고를 해보려고 한다. 왜 2주 차는 회고를 안 했냐고 하면 그동안 많이 바쁘기도 했고 무엇보다도 기획이 아직 확실하게 안 잡혔었기 때문에 회고를 남기기 애매했다. 2차 데모데이 발표를 내가 했지만 기획에 대해 많이 의문이 안 풀린 상태였다. 그래도 3차 데모데이를 기반으로 기획과 방향성이 잘 잡혀서 지금에서야 제대로 회고를 하게 되어서 다행이다ㅎㅎ 2, 3주 차 동안 어떤 해프닝들이 있었는지 한번 살펴보자 2차 데모데이 1차 데모데이 후 변경 점 1차 데모데이 때 구현할 기능으로 식품 상세나 소셜 로그인에 대해 얘기를 했었는데 이를 구현하지 않고 우선순위를 미뤘다. 우선 서비스의 핵심 가치를 잡는 게 우선이라고 생각했기 때문이다. 1차 데모데이 PPT에는 맞춤 리뷰에 초점을 맞춰서 발표 자료를 작성했었는데 끝나고 나서 회고를 해보니깐 사실 우리 서비스 핵심 가치는 사용자가 원하는 식품을 찾을 수 있게 도움을 주는 것이고 이 맞춤 리뷰는 그를 위한 부가적인 기능에 불과한 것이라고 생각했다. 사실 맞춤 리뷰는 다른 서비스와 차별을 주기 위한 기능이었는데 1차 기획을 할 때 다른 서비스와 어떻게 차별을 줄 수 있을까에 관심을 가지다 보니깐 그쪽으로 포커싱이 되었던 것 같다. 결국 맞춤 리뷰는 식품을 찾는 데 도움을 줄 수 있는 부가적인 정보일 뿐 우선 식품을 찾는 것에 집중하기로 했다. 그리고 이번 2차 데모데이 때는 타겟을 좁혀서 서비스 차원에서 맞춤성을 제공하려고 우선 다이어트를 위한 반려 식품으로 진행하고 후에 추가적으로 확장하려고 계획했다. 2차 데모데이 구현한 핵심 기능 이번 2차 데모데이 때 구현한 핵심 기능으로는 다음과 같이 다이어트 특화 식품 필터 버튼을 누르면 전체 식품에서 다이어트 식품을 보여주는 기능밖에 없다. 기획이 완전하게 잡히지 않아서 그에 대해 회의를 많이 하다 보니 기능은 많이 구현 못한 거 같아 아쉬웠다. 하지만 좀 있다 설명할 3차 때부터 정말 달라지니 기대해도 좋다ㅋㅋㅋ 사료에 대한 데이터는 어디서? 반려동물 사료에 대한 데이터를 가져올 방법이 없어서 참 곤란했다. 그렇다고 막 크롤링 해올 수도 없는 노릇이고.. 이것 때문에 프로젝트 주제를 바꿀 생각까지 하였다. 실제로 이틀 정도 새로운 프로젝트 주제에 대해 생각해 보았고 의논해 보려고 코치님들께 요청까지 드렸었다! 구구와 얘기한 결과 정말 실제 데이터들을 사용 못 한다면 mock 데이터를 사용해서 하는 것도 괜찮다고 하셔서 우선 최후의 보루로 mock 데이터를 생각하는 방향으로 얘기하였다. 하지만, 정말 다행이게도 몇몇 회사들에게 이메일을 보내놨었는데 데이터를 사용해도 된다는 허락을 받아서 실제 데이터로 운영할 수 있게 되었다!! 😭😭 (물론, 거절하거나 안 온 곳이 더 많음ㅠ) Merge 전략 집고팀의 branch와 Merge 전략에 대해 궁금하다면 이게 머지? 브랜치 Merge 전략 방법 글을 참고 CI/CD 현재 우리 CI/CD 구조를 보면 다음과 같은데 간단히 설명해보면 코드를 푸시하고 나면 github action으로 제대로 build 되는지 확인 그 후 머지가 되면 Jenkins로 trigger Jenkins에 구축해 놓은 Pipeline 실행 git clone으로 레포 클론 받은 후 빌드 해서 jar파일 생성 scp를 이용해 jar파일과 배포 스크립트 prod ec2로 전송 배포 스크립트 실행 API 문서화 보통 Spring Rest Docs를 사용하는 이유는 테스트 코드의 강제화와 비즈니스 로직에 API 문서 관련 코드를 넣지 않아도 되는 장점으로 많이 사용하고 있을 것이다. 하지만 우리는 Swagger의 직관적인 UI와 API 테스트 기능도 포기하기 아쉬웠다. 과연 우리는 어떤 선택을 했을까? 어느 하나를 포기할 수 없던 우리는 둘 다 쓰는 것을 택하였다. 아래 사진은 우리가 rest docs와 swagger를 같이 적용하고 난 뒤에 얻은 UI들이다. 이 과정을 통해 우리는 rest docs의 장점인 테스트 코드 강제화와 비즈니스 코드 오염을 방지하였고 동시에 swagger의 장점인 직관적인 UI와, api 테스트도 얻게 되었다. 적용 방법은 로지가 작성한 restassured 와 restdocs에 swagger ui 곁들이기를 참고하면 된다! 3차 데모데이 자 이제 대격변이 일어난 3차 데모데이를 한번 살펴보도록 하자!! 😎 1, 2차 피드백 적용 우선, 이 과정에서 가장 큰 역활을 한 갓첵스에게 감사를 드립니다.. 🙌 🙌 🙌 다음과 같은 피드백들이 있었고 이를 좀 더 신경써서 서비스를 다듬었다. 초보집사라면, 내 반려동물에 맞는 사료를 찾아나가는 기준을 모를 수도 있을 거 같다. 그 선택을 도와주는 픽쳐도 있지 않을까? 클릭 한번으로 어떤 과정을 처리해주는지 조금 더 드러나면 좋을 것 같다. 결정에 필요한 거를 상세정보에서 다루면 좋을 것 같다. 의사결정에 도움이 되는 것들을 신경쓰면 좋을 것 같다. 사용자 피드백을 통해서 핵심가치를 뾰족하게 만들어 나갈 것이다라고 했는데 이 사이클이 길면 길수록 맞춤이 어렵다. 반려동물을 키우는 크루들이 많기 때문에 이런 가설과 실험을 2주 단위가 아니라 하루에도 바로바로 실험할 수 있는 사이클을 만들면 조금 더 원하는 방향으로 맞춰나가면 좋을 듯. 다른 팀 개발속도는 신경쓰지 말고 규모가 크진 않더라도 정말 유용한 서비스를 만들면 좋을 것 같아요. 사용자들이 뭔가 내가 키우는 반려동물이 다이어트가 필요한 사람들이 쓰게 되는 거잖아요 그럼 서비스를 통해 사료를 구매했어요 그럼 서비스의 유용함을 언제 느낄 수 있을까요? 단시간에 다이어트의 유용성을 느낄 수 없을 것 같은데.. 피드백 받는 텀이 너무 길지 않을까 서비스 유용함을 사용자들이 느껴야 알 수 있을 텐데 어떻게 피드백을 받을지 어떤 피드백을 받고 싶은지에 대한 계획이 있을지 궁금합니다. 맞다. 우리가 현재 선택한 타겟은 피드백 주기가 길다는 문제가 있었기에 타겟 재탐색이 필요했고 식품을 고르는데 고통이 많은 초보 집사로 변경하였다. 초보집사 정보 수집 곤란 반려동물 입맛에 맞는 사료 보는 눈 없음 등 전문성 특히 부족 그리고 식품 상세정보에서 제공하는 정보들도 많이 바뀌게 되었다. 기존에는 원재료 정보나 영양성분, 알레르기 정보 등 제공하려고 했었다. 하지만 데이터 수집도 어렵고 사용자가 과연 상세한 성분을 얼마나 볼까 의문이 들었다. 그래서 새로운 방식의 신뢰성을 제공할 수 있는 정보들을 찾게 되었다. 어떻게 바뀌게 되었는지 한번 보자. 우선 예전에 한 설문조사를 정리해보면 집사들의 고통은 다음과 같다. 정보 수집 곤란 리뷰 신뢰성 문제(전문성 부족, 광고) 사료 품질(성분표기) 신뢰성 문제 반려동물 입맛에 맞는 사료 선택의 어려움 반려동물 특성에 맞는 사료 선택의 어려움(ex. 노령묘+신장질환) 그리고 전문가가 사료를 고르는 기준은 다음과 같다고 한다. AAFCO(미국), FEDIAF(유럽) 영양소 기준 만족 기호성 및 대변상태 확인 -&gt; 가장 추천하는 기준 신뢰할 수 있는 국가의 브랜드(미국, 유럽, 호주) 역사가 오래된 브랜드 브랜드에 전용 연구센터가 존재하는지 브랜드에 상주하는 수의사가 있는지 나이, 견종, 기저질환(알레르기 등) 고려하기 그래서 다음과 같은 느낌으로 제안이 되었고 결국 식품 상세 정보에 영양소 기준(AAFCO, FEDIAF) 만족, 기본적인 식품 정보, 브랜드 정보(창립 연도, 연구센터 유무, 상주 수의사 여부) 리뷰에 기호성, 대변 상태가 들어가게 되었다. 이제 실제로 구현한 기능들을 한번 봐보자. 3차 데모데이 구현한 핵심 기능 노션에 정리한 집사의고민 기능 명세인데 3차에 다 몰린 게 ㅋㅋㅋㅋㅋㅋㅋ 그동안 얼마나 기획으로 고생을 많이 한지 체감이 된다… 하 너무 힘들었다. 소셜로그인 식품 상세 리뷰 기능 식품 목록 필터링 과 같은 기능들을 추가했다. 드디어 기능들을 구현할 수 있어 행복한 3차가 아닌가 싶다. 기능들의 실제 구현 사진을 보고 싶다면 아래 토글을 펼쳐보자! 핵심 기능 상세히 보기 4차 구현 예정 핵심 기능 자신의 반려동물에 맞게 사료를 필터링 해주기 위해 사용자가 반려동물 정보를 등록할 수 있는 기능을 추가할 것같다. 그리고 리뷰와 식품 상세를 고도화 하려고 한다. 현재 예상하고 있는 바로는 다음과 같다. 반려 동물 정보 등록 (선택) 반려동물 사진 반려동물 이름 반려동물 성별 반려동물 나이 견종 몸무게 등.. 리뷰 고도화 반려동물 정보를 확인할 수 있다 리뷰 필터링 다른 유저의 리뷰에 ‘도움이 돼요’ 반응을 추가 리뷰 요약 정보(평점, 기호성, 변상태, 이상반응)를 확인할 수 있다. 리뷰 정렬 식품 상세 고도화 정보 설명기능 - 툴팁으로 구현 예정 이 정보들이 ‘왜’ 중요한지 의문을 가질 수 있기 때문 2, 3차 데모데이 후 회고 3차 데모데이 부분에 가장 핵심적으로 변화가 일어난 부분만 적어내려고 프로덕션 환경 구축, https, 협업 에피소드 등 다른 부분은 적지 않았다. 이 부분은 나중에 시간이 되면 추가적으로 작성하려고 한다. 드디어 Level3에서는 마지막 데모데이만 남게 되었는데 그전에 기획을 확정하고 서비스를 구현해나갈 수 있어서 정말 다행이다. 진짜 프로젝트를 바꾸려고 다른 주제를 생각했을 때만 해도 정말 우리 프로젝트가 어떻게 될지 가늠이 안되었고 어질어질했다. 그리고 1, 2차 데모데이 때도 기획이 아직 제대로 이해가 안 되었고..? 뭔가 말장난 아닌가라는 생각이 많이 들었었다. 실제로 피드백 부분에서 상당수 제대로 답하지 못하였다. 하지만, 3차 때부터 1, 2차 피드백 기반으로 많은 변화가 있었고 그에 따라 안정화되기 시작하고 개발도 같이 스무스하게 진행되었다. 그렇게 3차 데모데이가 잘 마무리되었고 팀원들이 정리해 준 3차 피드백을 한번 보면(코로나로 인해 데모데이 날 참석을 못 해 직접 못 들음ㅠㅜㅠ) 지표들이 제가 반려동물을 키우진 않지만 식품이나 화장품 창립연도 굉장히 유의미한 지표가 맞거든요. 유효한 지표를 잘 찾아내신 것 같아서 궁금했습니다 3차 때 자신감이 좀더 생긴 것 같다. 좀더 믿음이 있는 그런 느낌을 받았어요. 2차와 3차 데모데이 사이 이때 뭔가 전환점 같은게 있었는지 그런 것을 극복한 경험같은게 있는지 경험해주면 좋겠다 방향성이 좀더 명확하고 앞으로 이 서비스는 초보 집사들의 좋은 먹거리를 ~ 방향성을 잘 잡힌 것 같고 매력적인 것 같다. 완벽하게 해낸 건 아니겠지만 뭔가 1, 2차 때 한계를 어느 정도 극복해낸 거 같아 매우 감격스럽다.. 3차 데모데이 당일 팀원들과 함께 기쁨을 만끽하지 못한 것이 매우 아쉽다ㅠ 코로나로 인해 그날 24시간 동안 기절을 해버려서 어떤 상황인지 전혀 몰랐다. 1차 데모데이 회고 때 적었던 말이다. 물론 잘하면 베스트겠지만 누구나 잘할 순 없다. 일단 열심히 달려보자. 잘이 따라오면 더 좋고 아니면 뭐라도 따라오지 않을까? 달렸는데 뭐가 안 따라온다? 제대로 안 달렸는 게 아닌가라는 생각이 최근에 들어서 든다. 이번에 열심히 달렸는데 그래도 뭔가가 조금 따라온 거 같아 4차를 할 수 있는 원동력이 된 거 같다. 물론, 3차 때 받은 지적도 잘 보완해서 4차 잘 마무리할 수 있도록 다시 달려야겠다. 오늘의 교훈: 아 기획자님들 존경합니다.. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse team-project demo",
    "url": "/woowacourse/2023-08-15-2,3th-demo/"
  },{
    "title": "실제 서비스 운영 중 DB 스키마가 변경되면 어떡하지?",
    "text": "이전 프로젝트들에서 배포 후에는 DB 스키마를 변경하고 유지 보수한 적이 없어서 생각을 못 했다. 생각해 보면 귀찮아서 ALL DROP 후 CREATE 때린 거 같기도 하고ㅋㅋㅋ. 근데 만약 실제 운영하는 서비스이고 실제 데이터가 들어있다면? 하나라도 삭제되거나 잘못 변경되면 큰일 나기 때문에 신경 쓸게 많을 것이다. 일일이 각 배포 환경 돌아다니며 직접 schema를 변경할 수도 있겠지만 여간 귀찮은 게 아닐 것이다. 그리고 그러다 실수하면? 물론 실수한 게 문제가 아니다. 사람은 누구나 실수를 할 수 있다고 생각한다. 그런 환경이 나오지 않도록 하는 게 중요한 것 같다. 이와 관련해서 Flyway가 떠오를 것이다. 전부터 Flyway가 무엇인지는 알고 있었지만 미리 적용하진 않았다. 필요성을 체감을 하고 그때 도입을 하자라고 생각을 했었기 때문이다. 그러다 드디어 우리 집사의고민에 적용할 때가 왔기 때문에 Flyway에 대해 한번 알아보자 Flyway Flyway는 오픈소스 데이터베이스 마이그레이션 툴이라고 하는데 쉽게 말하면 데이터베이스 형상관리 툴이라고 생각하면 좀 더 쉬울 거 같다. 우리 소스 코드 같은 경우는 git 형상관리 툴을 이용하여 코드를 잘 관리하고 있는데 데이터베이스도 Flyway를 통해 잘 관리해 줄 수 있다. 적용 과정 실제 적용해보는 과정을 보면서 알아가봅시다. 프로젝트 생성 우선 Spring Initializr로 가서 와 같이 의존성을 추가한 후 generate 버튼을 눌러 프로젝트를 생성해서 열어줍니다. 저렇게 하면 다음과 같은 의존성이 나오게 됩니다. implementation 'org.springframework.boot:spring-boot-starter-data-jpa' implementation 'org.flywaydb:flyway-core' implementation 'org.flywaydb:flyway-mysql' implementation 'org.springframework.boot:spring-boot-starter-web' runtimeOnly 'com.mysql:mysql-connector-j' testImplementation 'org.springframework.boot:spring-boot-starter-test' application.yml 생성 그리고 엔티티를 만들기 앞서 환경설정을 해줍니다. spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/flyway-test username: root password: root jpa: hibernate: ddl-auto: none # Flyway 활성화 flyway: enabled: true MYSQL 데이터베이스 생성 직접 MYSQL을 설치하여도 되지만 저는 간단하게 하기 위해 컨테이너로 띄워줬습니다. 다음과 같이 docker-compose.yml 파일을 생성하고 docker-compose up -d를 이용해 실행시켜주면 간단하게 mysql을 띄울 수 있습니다. version: \"3\" services: mysql-db: image: mysql:8.0 volumes: - ./mysql:/var/lib/mysql ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: flyway-test platform: linux/x86_64 기본 엔티티 생성 다음과 같은 기본 엔티티가 있습니다. package entity; import jakarta.persistence.Entity; import jakarta.persistence.GeneratedValue; import jakarta.persistence.GenerationType; import jakarta.persistence.Id; @Entity public class Member { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String email; private String password; private String age; private String bio; } V1 마이그레이션 스크립트 생성 Flyway는 마이그레이션 스크립트의 버전 순서대로 SQL 스크립트를 실행한다. 우선, 제일 첫 번째 스크립트므로 다음과 같은 내용을 /resources/db/migration 위치에 V1__init.sql 파일명으로 생성해 준다. (언더스코어(_)가 2번!) CREATE TABLE member ( id BIGINT AUTO_INCREMENT, email VARCHAR(255), password VARCHAR(255), age VARCHAR(255), PRIMARY KEY (id) ); 이렇게 애플리케이션을 실행시켜주시면 member 테이블을 생성하고 데이터베이스 이력을 관리하는 테이블인 flyway_schema_history가 생성됩니다. 컬럼을 보시면 현재 version1이라고 잘 설정된 게 보입니다. 엔티티 변경 그렇게 기능 개선이나 유지 보수를 하다 보면 엔티티 구조가 변경될 수 있겠죠? package entity; import jakarta.persistence.Entity; import jakarta.persistence.GeneratedValue; import jakarta.persistence.GenerationType; import jakarta.persistence.Id; @Entity public class Member { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String email; private String password; private String age; private String bio; //새로 추가된 필드 } 그러면 저희는 간단하게 그다음 버전의 마이그레이션 스크립트만 작성하면 됩니다! V2 마이그레이션 스크립트 생성 V1 스크립트와 마찬가지로 동일한 위치(/resources/db/migration)에 다음과 같은 내용으로 V2__add_bio.sql 파일을 생성해 줍니다. 내용에는 전체 내용을 다 적는 게 아니라 변경사항만 적어주도록 합니다. 여기서 마이그레이션 스크립트 명명법이 궁금할 텐데 조금 있다 설명하겠습니다. ALTER TABLE member ADD COLUMN bio VARCHAR(255); 작성 후 애플리케이션을 실행시켜주면 다음과 같이 version2의 히스토리와 멤버 테이블이 변경된 걸 확인할 수 있습니다. 즉, 이제는 스키마가 변경되어도 직접 배포 서버 DB에 들어가 수정하는 게 아니라 변경 사항을 코드로 관리할 수 있게 된 것입니다. 🙌 마이그레이션 스크립트 명명법 버전 변경 마이그레이션(V), 실행 취소 마이그레이션(U), 반복 가능한 마이그레이션(R) 등 파일 유형을 결정하는 접두사가 있고 이 접두사는 파일 이름 앞에 붙는다. 다음으로 버전 번호가 오는데 이 버전 번호는 원하는 모든 형식이 가능하지만, 특정 마이그레이션에 대해 고유해야 하며(버전이 지정된 마이그레이션과 실행 취소 마이그레이션은 공통 버전 번호를 공유해야 함) 논리적으로 순서가 맞아야 한다. 그다음 밑줄(_) 두 개를 추가하여 파일의 기능적 명명 측면과 순수하게 설명적인 측면을 구분한다. 이 이후에는 그냥 텍스트로 단어 사이에 밑줄을 사용하면 공백으로 번역된다. 이 부분이 flyway_schema_history의 description 부분에 들어가는 텍스트이다. 그렇다면 기존 테이블과 데이터가 있는 경우에는? 위의 경우를 보면 아직 운영 서버를 띄우기 전부터 flyway 사용을 결정하여 init부터 주입하는 경우이다. 그렇다면 이미 운영서버가 돌아가고 있고(기존 테이블과, 데이터 존재) 아직 flyway가 적용되어 있지 않은 경우는 어떻게 해야 될까?? (처음에 바로 도입하지 않는 이상 다 이렇지 않을까?) baselineOnMigrate 똑같이 상황을 주기 위해 flyway_schema_history 테이블을 드랍하고 일단 마이그레이션 스크립트들도 삭제해 보자. 현재 멤버 테이블만 남아있는 상태이고 처음 flyway를 연동시킨 상태이다. 이 상태로 한번 실행해 보자. 다음과 같은 에러가 발생한다. baseline-on-migrate의 경우 기본 값이 false인데 schema history가 없고 기존 schema도 없으면(우리가 위에서 실습했던 방법, baseline-on-migrate를 따로 안 건듬) 잘 실행되었다. 하지만 여기서 기존 schema가 있는 경우 따로 설정해 주지 않을 시 에러가 뜬다. 그래서 기존의 데이터베이스 스키마를 Flyway의 버전 관리 아래로(초기 버전 설정) 가져오기 위해 baseline-on-migrate를 true로 설정해준다. spring: flyway: baseline-on-migrate: true 그렇게 애플리케이션을 실행하게 되면 다음과 같이 version1에 flyway가 기존의 스키마를 baseline으로 지정한다. 그리고 버전에 맞게 마이그레이션 스크립트를 잘 넣어주면 flyway가 잘 작동하는 걸 볼 수 있다. 근데 지금 이렇게 하면 한 가지 찝찝한 곳이 있다. 지금 version1을 baseline에 쓰고 있기 때문에 앞으로 /resources/db/migration에 V2부터 넣어야 된다는 것이다. 실제로 V1 스크립트를 넣고 실행을 해봐도 안 되는 것을 확인할 수 있을 것이다. 그래서 나중에 본 사람은 왜 V1은 없을까 하고 의문을 품을 수 있을 것 같다.(무엇보다 2부터 시작하는 게 불편함ㅎ) 이는 flyway의 baseline-version의 기본값이 1부터이기 때문인데 이를 0으로 바꿔주면 된다. spring: flyway: baseline-version: 0 이렇게 되면 이제 편안하게 V1, V2, V3… 적용할 수 있다! 참고: https://documentation.red-gate.com/fd/ https://www.blog.ecsimsw.com/entry/Flyway%EB%A1%9C-DB-Migration *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse flyway database",
    "url": "/woowacourse/2023-08-06-flyway/"
  },{
    "title": "이게 머지? 브랜치 Merge 전략 방법",
    "text": "이번에 집사의고민 팀에서 브랜치를 정하고 어떻게 병합할지에 대해 얘기가 나왔다. 스쿼시 어쩌구저쩌구 리베이스 어쩌구저쩌구… 이런 용어가 나오길래 아 아직도 깃허브에 대해 모르는 게 많구나 다시 한번 느끼고 Merge 전략에 대해 정리하고 가고자 한다. Merge 전략 Merge Commit Merge Commit은 일반적인 브랜치 병합 전략으로 두 개의 브랜치를 병합할 때 새로운 커밋을 생성한다. 그림으로 보면 다음과 같다. Merge 된 커밋(#4)으로부터 뒤로 돌아가면서 부모를 모두 찾아 브랜치를 구성 #4는 부모로 #3와 main을 가짐 #3은 #2를, #2는 #1을, #1은 main을 부모로 가져 main -&gt; #1 -&gt; #2 -&gt; #3 -&gt; #4의 구조가 히스토리로 남게 됨 Merge Commit은 불필요한 commit message가 생기고 merge 순서와 commit 순서가 별도로 기록되어 history 관리가 어렵다는 단점이 있으나 머지 기록을 남긴다는 게 오히려 어떤 관리 포인트가(ex) n차 스프린트) 될 수도 있지 않을까 한다. Squash and Merge Squash and Merge 전략은 여러 개의 커밋을 하나로 압축하여 병합하는 전략으로 브랜치의 모든 커밋이 단일 커밋으로 압축되어 기존 브랜치에 병합된다. 그림으로 보면 다음과 같다. 커밋 #1, #2, #3는 main을 부모로 가진 단일 커밋 병합 후 작업한 브랜치의 커밋들은 메인 브랜치와 연관을 가지지 않는다. main에선 기능별로 합쳐진 깔끔한 history를 가져 히스토리 관리는 쉬우나 rollback이 어렵다는 단점이 있다. Rebase and Merge Rebase and Merge는 현재 브랜치의 변경 내용을 다른 브랜치의 최신 상태에 병합하는 전략으로 Merge Commit과 달리 새로운 커밋을 생성하지 않는다. 그림으로 보면 다음과 같다. Base를 main의 최신 커밋(#5, New Base)으로 다시 설정 커밋 a, b, c의 관계를 그대로 유지한 채 메인 브랜치에 그대로 추가 Commit 순서가 아닌 Merge 순서대로 기록되어 다른 PR의 커밋 메시지와 섞이지 않아 rollback이 용이하며 commit 단위의 히스토리가 남겨지게 된다. 하지만, rebase에 익숙하지 않은 경우 어려움이 발생할 수 있다. 그래서 어떤 방식을 선택? 우선 우리팀은 main, develop, feature, hotfix 브랜치가 있는데 다음과 같은 병합 전략을 선택하기로 했다. feature -&gt; develop Squash and Merge 방식 지저분한 커밋 내역을 하나의 커밋으로 묶어어 develop으로 병합하면서 기능 단위로 커밋 develop -&gt; main 롤백할일이 생길 수도 있으니 Squash and Merge 방식은 제외 기본적으로 Merge Commit 방식 매번 스프린트마다 완성되면 main으로 머지해 배포할 계획이니 N차 스프린트 기록을 남기기 위해 Merge commit 일반적으로 넣는 경우 말고 추가적으로 넣어야 될 때 따로 기록할 필요 없으니 Rebase and Merge 방식 참고: https://meetup.nhncloud.com/posts/122 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse merge-strategy",
    "url": "/woowacourse/2023-07-17-Merge-Strategy/"
  },{
    "title": "집사의고민 1차 데모데이까지의 회고(feat.구글 스프린트)",
    "text": "드디어 기다리고 기다리던 팀 프로젝트가 시작되었다. 이번에 하게 된 프로젝트로는 반려동물 먹거리 제품별 성분 정보와 맞춤 리뷰를 제공하는 서비스로 서비스명은 집사의고민으로 선택되었다. 줄이면 집고(zipgo)로 어떤 사료를 집고..라는 뜻으로도 해석할 수 있어 매우 만족 중 기획 시작 와.. 기획이 이렇게 힘든지 몰랐다. 2주 동안 하루도 빠짐없이 회의를 하면서 커뮤니케이션을 한거 같은데 진짜 매일 머리가 부서질 거 같았다(아닌척하느라 연기 좀 는 듯) 어떤 과정이 있었는지 한번 살펴보자~ 구글 스프린트 1차 데모데이까지 시간은 얼마 남지 않았고 해야 할 것은 매우 많았다. 그래서 이에 대해 고민이 많았는데 그때 로지가 구글 스프린트라는 방법론을 제안해 주었고 우리 상황에 매우 적합해 보여 진행하였다. 솔직히 제대로 된 기획을 처음 해봐서 어떻게 시작해야 될지 얼타고 있었는데 구글 스프린트를 제안해 준 로지에게 감사드립니다.(+그레이 선생님께도) 구글 스프린트란? 구글 스프린트는 구글 수석디자이너 제이크 냅이 구글에 입사하고 팀 프로세스 개선을 위해 노력한 끝에 만들어낸 5일 만의 아이디어를 검증하는 획기적인 협업 프로세스로 5일간 MAP - SKECTCH - DECIDE - PROTOTYPE - TEST라는 프로세스를 만들어 낸다. 대부분의 좋은 생각은 단체에서 나오는 게 아니라 개인이 몰입할 때 만들어지고 개인의 몰입은 상황이 급박할수록 더 집중도가 높아지는 것을 알게 되어 만든 방법론이라고 한다.(혼자서 한다는 게 아님, 우선 개인에게 집중할 시간을 주고 그것을 충분히 말할 시간이 따로 있음) 지도 그리기 같은 아이디어를 생각했지만 분명 각자가 생각하고 있는 아이디어의 구체적인 그림은 다를 것이다. 그래서 이 생각을 서로 공유하고 함께 맞추지 않으면 나중에 의견의 불일치가 발생할 것이기 때문에 먼저 지도 그리기를 통해 의견을 조율하는 시간을 가졌다. 여기서 가장 중요한 것은 절대로 더 나은 것을 찾기 위해 결정하는 자리가 아니고 각자의 생각을 자유롭게 펼치는 시간이라는 것이다. 그래서 다음과 같은 방법들을 사용해서 진행했다. 이렇게 함으로 써 서로의 의견들을 다 한눈에 볼 수 있기 때문에 불필요한 시간 낭비를 줄이고 좋은 의견 쪽으로 자연스럽게 흘러갈 수 있었다. 우리 서비스의 궁극적인 목적, 가장 중요한 가치 우선 우리 서비스의 궁극적인 목적과 가장 중요한 가치 대해 생각해 보기로 하였고 그렇게 위와 같은 의견들이 도출되었다. 중간중간에 낙서같은 게 보이는데… 그만큼 사이가 좋습니다ㅎ 워드 클라우드 목적과 가치에 대해서 이야기한 후 나온 생각들을 키워드들로 추출하는 워드 클라우드 과정을 거쳤다. 처음부터 추상적인 단어로 먼저 얘기하면 막연한 얘기가 되지만 구체적인 각자의 생각을 충분히 나눈 후 만드는 추상화는 하나의 단어마다 우리들의 컨텍스트가 담겨있다고 한다! 그렇게 우린 다음과 같은 키워드들로 추상화 되었다. 신뢰도 편리함 접근성 개인화 다양성 퍼스널 테이스트 어떻게 하면 ~ 할 수 있을까 질문 찾고 답하기! 지금까지 작성한 목적, 가치, 워드 클라우드를 통해 추출한 것들을 기반으로 질문과 답변을 통해 구체화하는 시간이다. 궁금하거나 말하고 싶은 내용들을 질문하고 그에 대한 대답을 생각해 볼 수 있다. 여기서 중요한 것은 결정을 유도하는 질문은 금하는 게 좋고, “어떻게 하면 우리의 결과를 더 잘 전달할 수 있을까?” 같은 구체적인 방법을 요구하나 열려있는 질문의 형태가 좋다. 또한, 추상적인 답변은 좋지 않고 가급적 레퍼런스를 이용해서 구체적이고 시각적인 형태를 바탕으로 답하는 게 좋다. 서비스에 넣고 싶은 요소 찾기 그러고 나서 어떻게 하면 ~ 할 수 있을까로 구체화한 요소들을 통해 어떤 장치나 기능들이 필요한지 작성하는 시간을 가졌다. 일단 자기가 넣고 싶은 기능 마구마구 넣으면 됨ㅋㅋ 화면과 유저스토리를 통해서 지도 완성하기 위에서 찾아낸 장치와 기능들을 같은 화면 기준으로 분류하면 스토리보드가 나오게 되고 유저 스토리 같은 경우도 사용자가 우리 서비스에서 뭘 하고 싶은지 적어보면서 완성시켜나갔다. 지도 그리기 과정을 하고 직접 느낀 점으로는 그저 내가 고민하고 내 생각을 말하는 데 더 집중할 수 있게 된다는 것이다. 그냥 의견을 주고받으면 서로 눈치 보는 일이 많았는데 이 방법을 하면 자유롭게 주고받을 수 있는 게 참 좋은 거 같다. 그렇게 자유롭게 주고받다가 다 같이 좋다고 느끼는 방향으로 자연스럽게 의견이 모아진다. 스케치와 결정 그렇게 지도 그리기를 통해 도출된 스토리보드를 기반으로 서로가 생각하는 페이지를 그려보았다. 하… 디자인 왜 이렇게 어렵냐.. 나로 말할 거 같으면 어릴 때부터 타고난 똥 손으로 내 손을 거쳐간 작품들은 모두 폐기되었다;;(이번에도 한 개도 안 됨ㅋㅋㅋㅜ) 어쨌든 이렇게 그려진 페이지를 보며 서로 설명하는 시간을 가진 후에 어떤 디자인을 할 것인지 채택하였다. BDD를 통한 설계와 태스크 분배 BDD(Behavior-Driven-Development)를 통해 각각의 선정된 디자인 페이지에서 어떤 이벤트가 발생했는지 정의해볼 수 있다. 이를 통해 빠진 기획 요소를 찾고 개발 분량을 가늠하고 체크 태스크를 분배하고 진척사항을 체크해볼 수 있다. BDD에 대해 더 궁금하다면 다음글 참고 하지만, 우리는 1차 데모데이까지 시간이 부족해 이 부분은 하지 못했는데 다음 주에 할 예정이다. 아직 우리 서비스의 기능에 대해 확신이 안 왔는데 이 부분을 통해 좀 더 명세화할 수 있지 않을까 싶다. 1차 데모데이 후 회고 그렇게 위와 같은 산출물들이 나왔는데 얼핏 보면 엄청나게 많은 걸 준비하고 결과를 낸 것처럼 보인다. 하지만 사실 1차 데모데이를 하고 느낀 거지만 중간중간 빈틈도 꽤 있고, 시간에 쫓겨 급하게 하다 보니 세심한 부분까지 신경을 못 쓴 것 같다. 그리고 한 번에 많은 걸 정리하다 보니 제대로 생각 정리도 안됐고. 그 결과 발표를 마친 후 QnA에서 많은 질문을 받았는데 제대로 된 대답을 못한 질문이 많았다. 근데 그렇다고 대충 시간을 허투루 썼냐? 아니다. 우리는 처음이라 방법론까지 공부해 적용해 보며 많은 시간을 투자하며 기획을 하는데 최선을 다했다. 특히 기획도 하고 또 발표 PPT를 만들기 위해 새벽까지 고생한 팀원들에게 박수를 쳐주고 싶다. “중요한 건 열심히 하는 게 아니라 잘하는 것”이라는 말이 있다. 하지만, 처음부터 어떻게 다 잘할 수 있겠는가? 사실 이 열심히 한다는 말에는 “잘”하겠다는 뜻이 당연히 내포되어 있다. 일반적으로 ‘열심’이 없이는 좋은 결과도 나올 수 없을 것이다. 우리는 앞으로 잘하기 위해 늘 그랬듯이 열심히 달릴 것이다. 받은 QnA를 다 기록해놓았기 때문에 이를 기반으로 분석하고 보완할 계획이다. 1차 데모데이를 하고 생각한 건데 왜 그동안 다른 크루들이나 코치님들께 궁금한 것들이나 고민하고 있던 것들을 묻지 않고 스스로 합리화했는지..? 지금까지 누군가에게 잘 묻지 않는 습관 때문에 그런 거 같은데 이제부터는 누군가에게 의견을 묻는 것에 좀 더 자유로워져야겠다. 참고: 테오의 스프린트 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse team-project demo",
    "url": "/woowacourse/2023-07-09-1th-demo/"
  },{
    "title": "제네릭(generics)과 와일드카드(wildcards)에 대해 알아보자",
    "text": "Level2 방학 동안 그동안 했던 것들을 돌아보기 위해 Good Code, Bad Code를 사서 맛보고 있는데 매우 맛있다. 해당 책은 높은 품질의 코드를 짜기 위해서 여러 가지 전략들을 제시 해 준다. 그 중 코드를 재사용 가능하고 일반화할 수 있게 작성하라라는 부분을 읽던 중 “제네릭의 사용을 고려하라”라는 방법을 보게 되었다. 특정 타입에 의존하면 일반화가 제한된다고 한다. 예를 들어, 단어 맞히기 게임을 개발한다고 생각해 보자. 게임 참여자들이 각각 단어를 제출한 다음 돌아가며 한 단어씩 동작으로 설명하면 다른 참여자들이 어떤 단어인지 맞혀야 한다. 이를 무작위 큐를 이용해 다음과 같이 구현해 볼 수 있을 것이다. class RandomizedQueue { private final List&lt;String&gt; values = []; //새로운 단어(문자열) 추가 void add(String value) { values.add(value); } //큐로부터 무작위로 한 항목을 삭제하고 그 항목을 반환 String getNext() { if (values.isEmpty()) { return null; } int randomIndex = Math.randomInt(0, values.size()); values.swap(randomIndex, values.size() -1); return values.removeLast(); } } 위처럼 코드를 짜게 되면 문자열(String)로 표현될 수 있는 단어를 저장하는 특정한 타입의 문제는 해결할 수 있지만 다른 타입의 동일한 하위 문제를 해결할 수 있을 만큼 일반화되어 있지 않다. 즉, 단어가 아닌 사진을 보고 설명하고 그 외 나머지는 다 동일한 게임을 다른 팀에서 개발한다고 생각해 보면 위의 코드는 문자열을 사용하도록 하드 코딩되어 있기 때문에 재사용할 수 없다. 위의 코드를 일반화하여 사용하려면 다음과 같이 제네릭을 이용해서 개선해볼 수 있다. class RandomizedQueue&lt;T&gt; { private final List&lt;T&gt; values = []; void add(T value) { values.add(value); } T getNext() { if (values.isEmpty()) { return null; } int randomIndex = Math.randomInt(0, values.size()); values.swap(randomIndex, values.size() -1); return values.removeLast(); } } 이렇게 하면 이제 RandomizedQueue 클래스는 어떤 것이라도 저장할 수 있기 때문에 단어를 사용하는 게임 버전에서는 RandomizedQueue&lt;String&gt; words = new RandomizedQueue&lt;String&gt;(); 사진을 사용하는 게임 버전에서는 다음과 같이 쉽게 사용할 수 있게 된다. RandomizedQueue&lt;Picture&gt; words = new RandomizedQueue&lt;Picture&gt;(); 이처럼 제네릭을 사용하면 코드를 재사용하여 좋은 코드를 작성할 수 있어 매력적이게 보였다. 하지만, 지금까지 제네릭을 프로젝트에서 제대로 적용해 본 적은 없었기 때문에 제네릭에 대해 자세히 알아볼 필요성을 느꼈다. 이 포스팅을 기반으로 후에 잘 적용할 수 있기를! 제네릭(Generics)이란? 제네릭이란 JDK 5.0부터 도입된 기능으로, 컴파일 타임 타입 안전성을 제공하면서 다양한 타입의 객체에서 작동할 수 있게 해준다. 제네릭을 사용함으로 써 컬렉션 프레임워크에 컴파일 타임 타입 안정성을 추가하고 캐스팅의 번거로움을 없앨 수 있다. 제네릭이 등장하기 전 제네릭이 왜 등장했을까? 제네릭이 등장하기 전에는 다음과 같은 문제점들이 있었다. 필수적인 캐스팅(형변환) 런타임 시점 에러 발생 List myIntList = new LinkedList(); myIntList.add(new Integer(0)); Integer value = (Integer) myIntList.iterator().next(); 컴파일러는 이터레이터가 객체를 반환한다는 것만 보장할 수 있기 때문에 정수형 변수에 대한 할당이 필요하다면 (Integer)와 같은 형 변환이 필요했다. 이 형 변환은 복잡성을 유발할 뿐 아니라, 프로그래머가 실수할 수 있으므로 런타임 오류가 발생할 가능성도 있다. 이를 제네릭을 사용하여 특정 데이터 타입을 포함하도록 목록을 제한하는 것으로 나타낼 수 있다면 어떨까? List&lt;Integer&gt; myIntList = new LinkedList&lt;Integer&gt;(); myIntList.add(new Integer(0)); Integer value = myIntList.iterator().next(); 세 번째 줄에 형 변환이 사라지고, List 옆에 타입 매개변수가 추가되었다. 이렇게 함으로 써 형 변환을 제거할 수 있게 되었고 이제 컴파일러는 컴파일 시점에 프로그램의 타입 정확성을 확인할 수도 있게 되었다. 제네릭(Generics) 제네릭을 적용하여 클래스를 생성해보자. 다음과 같이 클래스명 옆에 화살 괄호가 추가되고 그 안에 타입 매개 변수가 위치하게 된다. class GenericList&lt;T&gt; { private List&lt;T&gt; values = new ArrawyList&lt;&gt;(); public void add(T value) { values.add(value); } } GenericList&lt;String&gt; stringList = new GenericList&lt;&gt;(); GenericList&lt;Integer&gt; IntegerList = new GenericList&lt;&gt;(); 또한, 여러개의 매개 변수도 받을 수 있다. class Generics&lt;T, E&gt; { private T generic1; private E gerneric2; } Generics&lt;String, Integer&gt; generic = new Generics&lt;String, Integer&gt;(); 제네릭 메서드(Generic Methods) 클래스의 메소드 안에서만 제네릭을 사용할 수도 있는데 그렇게 되면 타입 매개 변수의 범위가 메서드 내로 제한되게 된다. public class GenericMethod { public static &lt;T&gt; void print(T info) { System.out.println(info); } } GenericMethod.printInfo(\"String\"); GenericMethod.printInfo(100); 공변과 불공변 와일드카드를 알아보기 전에 공변과 불공변이 뭔지 알아보자 공변(Covariant): 타입 B가 타입 A의 하위 타입일 때, T&lt;B&gt; 가 T&lt;A&gt;의 하위 타입인 경우 ex) Object[] objects = new Integer[10]; 불공변(Invariant): 타입 B가 타입 A의 하위 타입일 때, T&lt;B&gt; 가 T&lt;A&gt;의 하위 타입이 아닌 경우 ex) List&lt;Object&gt; list = new ArrayList&lt;Integer&gt;(); //컴파일 에러 발생 와일드카드(Wildcards) 컬렉션의 모든 요소를 출력한다고 생각해보자. 제네릭이 나오기 이전에는 다음과 같다. void printCollection(Collection c) { Iterator i = c.iterator(); for (k = 0; k &lt; c.size(); k++) { System.out.println(i.next()); } } 그리고 제네릭을 사용하게 되면 코드는 다음과 같을 것이다. void printCollection(Collection&lt;Object&gt; c) { for (Object e : c) { System.out.println(e); } } List&lt;String&gt; list = Arrays.asList(\"a\", \"b\"); printCollection(list); //컴파일 에러 발생 첫 번째 코드는 모든 종류의 컬렉션을 매개변수로 사용하여 호출할 수 있는 반면, 제네릭은 불공변이기 때문에 두 번째 코드로 작성해놓더라도 실제로 모든 타입에서 공통적으로 사용할 수 없는 문제점이 있었다. 즉, 제네릭을 사용함으로 써 더 유용하지 못하게 되었다는 것인데 이를 극복하기 위해 나온 것이 바로 와일드카드이다. 모든 타입의 상위 타입인 와일드카드(&lt;?&gt;)를 사용해서 우리는 다음과 같이 사용해 모든 타입의 컬렉션을 호출할 수 있다. void printCollection(Collection&lt;?&gt; c) { for (Object e : c) { System.out.println(e); } } 하지만 또 와일드카드로 선언함으로써 생기는 문제점이 있었는데 Collection&lt;?&gt; list = new ArrayList&lt;String&gt;(); list.add(new Object()); // 컴파일 타임 에러 발생 해당 list의 요소 타입이 무엇인지 모르기 때문에 우리는 객체를 추가할 수 없다. add() 메서드로 추가하기 위해서는 컬렉션의 요소 타입인 E 타입 혹은 E 타입의 하위 유형을 전달해서 추가할 수 있다. 하지만 타입 매개변수가 ?인 경우 알 수 없는 타입(unknown type)을 나타내기 때문에 우리는 아무것도 전달할 수 없다.(null은 예외적으로 가능) 하지만, List&lt;?&gt;가 주어졌을 때 get()을 호출하고 결과는 사용할 수 있다. 왜냐하면 결과 유형은 알 수 없는 유형이지만 항상 객체라는 것을 알고 있기 때문이다. 한정적 와일드카드(Bounded Wildcards) 그렇기 때문에 한정적 와일드카드(Bounded Wildcards)라는 기능이 존재하는데 이 한정적 와일드카드를 이용하여 타입의 범위를 제한해 위의 문제점을 해결할 수 있다. 우선 다음과 같이 클래스가 정의되어 있다고 생각해 보자 public class Shape { ... } public class Circle extends Shape { ... } public class Rectangle extends Shape { ... } 상한 경계 와일드 카드(Upper Bounded Wildcards) 상한 경계 와일드카드는 extends를 이용하여 상위 타입을 정의해 주므로 써 상한 경계를 설정해줄 수 있다. 그렇게 하면 제한적으로 다음과 같이 꺼낼 수 있게 된다. void printCollection(Collection&lt;? extends Shape&gt; c) { for (Shape e : c) { System.out.println(e); } for (Object e : c) { System.out.println(e); } //컴파일 에러 발생 for (Circle e : c) { System.out.println(e); } //컴파일 에러 발생 for (Rectangle e : c) { System.out.println(e); } } 위와 같이 Shape로 상한 경계를 준 경우 Shape 이상(부모)인 클래스로 꺼내는 경우 모두 가능하지만, 그 아래(자식) 클래스로 꺼내는 것은 불가능하다. &lt;? extedns Shape&gt;으로 가능한 타입은 Shape와 그 자식 클래스이므로 Shape 이상 클래스로 꺼내는 것은 문제가 없다. 하지만, 그 자식 클래스로 꺼내는 경우는 Circle 인지 Rectangle 인지 알 수 없으므로 컴파일 에러가 발생한다. 상한 경계 와일드카드를 사용했을 때 추가하는 경우를 한번 보자. void addShapes(Collection&lt;? extends Shape&gt; c) { c.add(new Shape()); //컴파일 에러 발생 c.add(new Object()); //컴파일 에러 발생 c.add(new Rectangle()); //컴파일 에러 발생 c.add(new Circle()); //컴파일 에러 발생 } 컬렉션에 추가하는 경우 모든 타입에 대해 컴파일 에러가 발생한다. 왜냐하면 &lt;? extends Shape&gt;으로 가능한 타입은 Shape와 모든 그 자식 클래스인데 c가 정확히 어떤 타입인지 모르기 때문이다. 만약 Rectangle로 타입 매개변수가 된 컬렉션인 경우 Shape가 들어올 위험이 있을 수 있다. 그래서 추가하는 경우에는 상한 경계가 아닌 하한 경계 와일드카드를 사용해 볼 수 있다. 하한 경계 와일드 카드(Lower Bounded Wildcards) 하한 경계 와일드카드는 super를 이용하여 하위 타입을 정의해 주므로 써 하한 경계를 설정해줄 수 있다. 그렇게 하면 제한적으로 다음과 같이 추가할 수 있게 된다. void addShapes(Collection&lt;? super Shape&gt; c) { c.add(new Object()); //컴파일 에러 발생 c.add(new Shape()); c.add(new Rectangle()); c.add(new Circle()); } 위와 같이 Shape로 하한 경계를 준 경우 Shape 이하(자식)인 클래스들을 추가하는 것은 가능하지만 Shape 그 위(부모)의 클래스들을 추가하는 것은 불가능하다. &lt;? super Shape&gt;으로 가능한 타입은 Shape와 그 부모 클래스이므로 Shape 이하 클래스로 추가하는 것은 문제가 없다. 하지만, 그 부모 클래스로 추가하는 경우는 상한 경계처럼 상위 타입이 추가될 위험이 있으므로 컴파일 에러가 발생한다. 하한 경계 와일드카드를 사용했을 때 꺼내는 경우를 한번 보자. void printCollection(Collection&lt;? super Shape&gt; c) { //컴파일 에러 발생 for (Shape e : c) { System.out.println(e); } //컴파일 에러 발생 for (Circle e : c) { System.out.println(e); } //컴파일 에러 발생 for (Rectangle e : c) { System.out.println(e); } for (Object e : c) { System.out.println(e); } } &lt;? super Shape&gt;으로 가능한 타입은 Shape와 Shape의 부모 클래스이므로 정확한 부모 타입를 알 수 없어 컴파일 에러가 발생한다. 예를 들어 Shape로 타입 매개변수가 된 컬렉션인 경우 그 부모 클래스로 꺼낼 수 없을 것이다. 하지만, Object 같은 경우 모든 객체의 부모임이 확실하므로 컴파일 에러가 발생하지 않는다. +그렇다면 언제 extends, super…? 이펙티브 자바에서는 펙스(PECS)라는 공식이 나온다. producer-extends, consumer-super 즉, 매개변수화 타입 T가 생성자라면 &lt;? extends T&gt;를 사용하고, 소비자라면 &lt;? super T&gt;를 사용하라는 것이다. //컬렉션의 원소를 꺼내 와일드카드 타입 객체 생성(produce) void printCollection(Collection&lt;? extends Shape&gt; c) { for (Shape e : c) { System.out.println(e); } } //컬렉션에 와일드카드 타입 원소 추가함으로 객체 소비(consume) void addShape(Collection&lt;? super Shape&gt; c) { c.add(new Shape()); } 참고: https://docs.oracle.com/javase/tutorial/extra/generics/index.html *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.;",
    "tags": "woowacourse generics wildcards",
    "url": "/woowacourse/2023-06-25-generics/"
  },{
    "title": "우테코 - 지하철 미션 회고",
    "text": "밀린 회고들을 방학이 돼서야 쓰게 됐다.. 지하철 미션 때부터 번아웃이 온 거 같더니 회복할 틈도 없이 미션, 글쓰기, 테코톡, 근로 등에 치여 엄청 바쁘게 살아온 것 같다. 뭔가 엄청나게 이끌려서 수동적으로 산거 같은 느낌이 든다. 레벨 1때부터 느꼈지만 역시 체력이 정말 중요하다고 느꼈고 그 체력이 나는 바닥임을 느꼈다. 레벨 3때부터는 좀 더 체계적으로 루틴을 잡고 그 루틴을 잃지 않도록 절대적으로 노력해야겠다. 이번 지하철 미션은 하기 전부터 겁을 먹었던 녀석이다. 우테코 최종 코딩 테스트 연습을 할 당시에도 다른 여러 문제를 풀었지만 이 문제만은 보기 전부터 풀기 싫어 PASS를 했으니.. 실제로 미션에 들어간 후 첫날은 도메인 설계에만 시간을 다 썼지만 실패하고 집에 가서 생각을 해왔다. 실패를 한 요인을 분석해 보면 도메인을 짜는데 자꾸 나중에 성능을 생각하다 보니 쓸데없는 사족이 들어가게 되었고 그 사족들이 하나둘씩 붙다 보니 뭘 만들고 있던 건지 복잡해져 머리가 자꾸 꼬였던 것 같다. 왜 코드 구현과 리팩터링 단계를 따로 두는 건지 이전에는 몰랐지만 이번에 확실히 알 것 같다. 글도 한 번에 쓰는 습관이 있었는데 한 번에 쓰려니 부담감이 엄청 커 스트레스를 많이 받았다. 오늘 안에 이걸 다 끝내야 된다는 강박감 때문에 밥도 안 먹고 쓰고 억지로 내용을 만들어 쓰고 그러다 보니 몸에 또 무리가 오고 반복한 게 아닐까. 이처럼 한 번에 모든 걸 하려는 건 좋지 않은 것 같다. 우리는 기계가 아닌 사람이니까… 근데 가끔 그런 괴물들이 존재하곤 하더라.. 이제 사족은 끝내고 미션 회고를 해보자! Repository, DIP 적용 이전 미션까지는 Dao만 사용하고 Repository의 필요성이 느껴지지 않아 사용하지 않았지만 이번에는 도입하지 않으면 Service에서 하는 역할이 매우 많아질 거라 생각하였고 그에 따라 가독성 저하 및 책임 분리가 잘되지 않을 것이라 판단해서 적용하게 되었다. 만약 적용하지 않았다면 2번째 코드처럼 되었을 것이다. Dao vs Repository의 차이가 궁금하면 다음 글을 한번 보고 오자! //Repository 적용시 public Long addSection(final SectionCreateDto sectionCreateDto) { final Line line = lineRepository.findById(sectionCreateDto.getLineId()); final Station upStation = stationRepository.findById(sectionCreateDto.getUpStationId()); final Station downStation = stationRepository.findById(sectionCreateDto.getDownStationId()); final Section section = Section.of(upStation, downStation, sectionCreateDto.getDistance()); line.add(section); lineRepository.updateLine(line); return line.getId(); } //Repository 적용x public Long addSection(final SectionCreateDto sectionCreateDto) { final LineEntity lineEntity = lineDao.findById(sectionCreateDto.getLineId()) .orElseThrow(() -&gt; new IllegalArgumentException(\"존재하지 않는 노선 이름입니다.\")); final List&lt;Section&gt; sortedSections = getSortedSections(sectionCreateDto.getLineId()); final Line line = new Line(lineEntity.getId(), lineEntity.getLineName(), Sections.values(sortedSections)); final StationEntity stationEntity = stationDao.findById(sectionCreateDto.getUpStationId()) .orElseThrow(() -&gt; new IllegalArgumentException(\"해당 이름의 역이 존재하지 않습니다.\")); final Station upStation = new Station(stationEntity.getId(), stationEntity.getStationName()); final StationEntity stationEntity = stationDao.findById(sectionCreateDto.getDownStationId()) .orElseThrow(() -&gt; new IllegalArgumentException(\"해당 이름의 역이 존재하지 않습니다.\")); final Station downStation = new Station(stationEntity.getId(), stationEntity.getStationName()); final Section section = Section.of(upStation, downStation, sectionCreateDto.getDistance()); line.add(section); sectionDao.deleteAllByLineId(line.getId()); sectionDao.insertAll(line.getSections().stream() .map((section) -&gt; new SectionEntity( line.getId(), section.getUpStation().getId(), section.getDownStation().getId(), section.getDistance())) .collect(Collectors.toList())); return line.getId(); } private List&lt;Section&gt; getSortedSections(long lineId) { final SectionsSorter sectionsSorter = SectionsSorter.use(mapToSections(lineId)); return sectionsSorter.getSortedSections(); } 그리고 현재 계층형 아키텍처를 사용하기 때문에 도메인 계층이 영속성 계층에 의해 크게 영향을 받는다. 여기서 도메인과 영속성 간의 결합을 줄여주기 위해 추가적으로 DIP(Dependency Inversion Principal) 를 적용했다. DIP는 의존 역전 원칙으로 고수준 모듈이 저수준 모듈의 구현에 의존해서는 안 되고 저수준 모듈이 고수준 모듈에서 정의한 추상 타입에 의존해야 된다는 것이다. 고수준 모듈: 어떤 의미 있는 단일 기능을 제공하는 모듈 저수준 모듈: 고수준 모듈의 기능을 구현하기 위해 필요한 하위 기능의 실제 구현 고수준 모듈이 저수준 모듈에 의존할 때 문제점은 다음과 같다. 쿠폰을 이용한 가격 계산 모듈(고수준)이 개별적인 쿠폰(저수준) 구현에 의존하게 되면 아래처럼 새로운 쿠폰 구현이 추가되거나 변경될 때마다 가격 계산 모듈이 변경되는 상황이 발생한다. public int calculate() { ... if (someCondition) { CouponType1 type1 = ... } else { // 쿠폰2 추가에 따라 // 가격 계산 모듈 변경 CouponType2 type2 = ... ... } } 이런 상황은 프로그램 변경을 어렵게 만드는데 우리가 원하는 것은 구체적인 사항(저수준)이 변경되더라도 고수준 모듈이 변경되지 않는 것이다. 이것을 의존 역전 원칙으로 인해 해결할 수 있다. 어떻게 할 수 있을까? 일반적으로 위와 같이 도메인에서 영속성 쪽으로 의존하고 있을 것이다. 여기서 의존성을 역전 시키려면 Repository를 인터페이스로 바꾸고 Domain 패키지로 옮긴다. 그러고 나서 실제 구현체를 영속성 계층에서 구현하게 하면 의존성을 뒤집을 수 있게 되고 영속성 계층에서 수정이 있더라도 도메인 계층에는 영향을 주지 않게 된다. DTO 변환위치? 우리는 Controller와 View 사이 데이터를 주고받을 때 별도의 DTO를 사용해서 통신한다. 도메인 객체를 View에 바로 전달할 수도 있지만 DTO를 사용함으로 써 주요 비즈니스 규칙을 외부에 노출시키지 않을 수 있고 필요한 부분만 넘길 수 있다. 또한, View와 의존하지 않고 순수하게 도메인 로직만 담당할 수 있게 된다. DTO를 사용한다면 이 DTO와 Domain 간 객체 변환 위치가 있을 것이다. 과연 어디에 있는 게 적절할까? Service 계층? 우선 서비스 계층에서 변환한다고 생각해 보자. Controller에서 DTO가 넘어올 것이고 그 DTO를 Service 메소드의 파라미터로 받을 것이다. 그리고 그 DTO를 domain으로 객체 변환이 일어나게 된다. 이 방법의 단점은 무엇이 있을까? 서비스에서는 View에서 Controller로 넘긴 DTO를 그대로 사용하기 때문에 특정 Controller와의 결합도가 높아진다. 그래서 해당 서비스의 재사용성이 낮아진다는 단점이 있다. Controller 계층? 그럼 컨트롤러에서 변환하면 어떨까? 응답할 때를 생각해 보자. 도메인 모델이 컨트롤러까지 넘어오게 되고 여기서 Dto로 변환하는 작업이 일어날 것이다. 어떤 문제점이 있을까? View에 반환될 필요가 없는 데이터까지 객체(Domain Model)에 포함되어 표현 계층까지 넘어 온다. 또한, 여러 Domain Model을 조합해서 DTO를 만들어야 될 경우 결국 응용 로직이 컨트롤러에서까지 사용될 수 있다. 또한, 요청할 때 View에서 전달받은 정보만으로 Domain 객체를 구성할 수 없을 때도 있을 것이다. 예를들어, ID만 받은 경우 ID외의 정보를 Repository를 통해 조회하고 나서야 Domain을 구성할 수 있다. 그럴때는 Controller에서 바로 Repository를 접근하는 것이 맞을까? 그렇다면 나는 어떤 선택을 했는가? Controller에서 변환 시 여러 문제점들과 Presentation 레이어에서 우리가 다루는 중요한 도메인을 알고 있는 것 자체가 찝찝했다. 그리고 Tecoble에서 “여러 종류의 컨트롤러가 한 서비스를 사용하는 경우보단 보통 한 종류의 컨트롤러가 서비스를 사용하기 때문에 그렇게 엄격히 제안할 필요가 있냐는 의견도 있다. 그래서 보통 코드를 보면 서비스로 DTO 진입을 허용하되 서비스 메소드 상위에서 DTO 체크 및 도메인 변환을 하고 변환 한 후에는 도메인 만 사용하도록 구현 되어있다.” 와 같은 피드백을 보았다. 그래서 결국 Service 계층에서 변환해 주는 걸 택하였다. 하지만 여전히.. Controller와 Service의 결합도가 높아진다는 문제점이 나를 찝찝하게 만들었다 ㅋㅋㅋ 양방향 매핑 둘의 결합도를 끊어주기 위해서는 Service에 맞는 적절한 DTO를 받아 줄 수 있을 것 같다. 즉, View &lt;–&gt; Controller에서 DTO를 받은 후 그 DTO로 다시 Controller &lt;–&gt; Service 통신을 위한 DTO를 만들어 주는 것이다. 응답의 경우도 마찬가지. @PostMapping(\"/members\") public void create(@RequestBody CreateRequestDto createRequestDto) { memberService.create(createRequestDto.toServiceCreateDto()); ... } 이 부분을 적용했냐라고 하면? 적용하진 않았다. 사실 이 경우가 가장 이상적이지 않나 생각이 들긴 한다. 하지만 그만큼 비용이 엄청나다. 계층 간 통신을 할 때마다 DTO가 생성되기 때문에 매우 복잡해 보인다. 그래서 프로젝트 규모가 커지거나 분리가 필요할 때 사용하는 것이 맞지 않나 생각이 든다! 과연 레벨3 팀 프로젝트 때 적용할 일이 있으려나..? 기대가 된다 ㅎㅎ +각각의 DTO를 하나로 통합? 초반에 CRUD 로직을 구현하다 보면 비슷한 형식의 데이터를 받기 때문에 DTO를 하나로 통합해서 사용할지 분리해서 쓸지 고민을 했던 적이 있던 것 같은데 그 이유는 바로 중복이 있기 때문이다. public class CreateDto { private String name; private String email; private String password; ... } public class UpdateDto { private String name; private String email; private String password; ... } 클린 아키텍처에서 중복은 나쁜 것이지만, 이 중복이 진짜 중복인지 아니면 우발적 중복인지 구분되어야 한다고 한다. 여기서 진짜 중복은 한 인스턴스가 변경되면, 동일 변경을 그 인스턴스의 모든 복사본에 적용해야하는 케이스이고 가짜 중복은 중복으로 보이는 두 코드 영역이 서로 다른 속도와 다른 이유로 변경이 되는 것을 이야기 한다. 여기서 우리가 사용하는 각각의 CRUD DTO가 현재는 중복되어 있지만 언제 각각의 사정(서로 다른 속도와 다른 이유)으로 변경될지 모르기 때문에 분리하는게 좋을 것 같다. 좋은 API 설계를 위한 방법 API 설계할 때마다 URI는 어떻게 짜고 response는 어떻게 줄지 깊게 생각해 본 적이 없어 한번 정리해보고 가면 좋을 것 같아 정리해 본다. URI는 직관적으로 URI를 보는 것만으로도 어떤건지 바로 이해할 수 있어야 한다. 길게 만드는 것보다 최대 2 depth 이하로 만드는 것이 적당하다. /students/1 URI에 리소스명은 동사보다 명사를 사용한다. 리소스에 대해서 행동을 정의하는 형태를 사용한다. 잘못된 예시 HTTP Post: /getStudents HTTP Post: /setStudentsTeacher 적절한 예시 HTTP Get: /dogs HTTP Post: /students/{tom}/teacher/{merry} 리소스간의 관계를 표현 리소스간에는 서로 연관관계가 있을 수 있다. 어떻게 URI로 이 관계들을 표현하면 좋을까? 서브리소스로 표현하는 방법 예를들어 학생이 가지고 있는 과목들의 목록을 표현해보면 /”리소스명”/”리소스 id”/”관계가 있는 다른 리소스명” HTTP Get: /students/{studentId}/courses 일반적으로 소유 “has”의 관계를 묵시적으로 표현할 때 사용 서브 리소스에 관계를 명시 하는 방법 관계의 명이 복잡하다면 관계명을 명시적으로 표현할 수 도 있다. 예를들어 학생이 좋아하는 과목의 목록을 표현해보면 HTTP Get: /students/{studentId}/likes/courses 관계의 명이 애매하거나 구체적인 표현이 필요할 때 사용 에러처리 에러 처리의 기본은 HTTP Response Code를 사용한 후, Response body에 error detail을 서술하는 것이 좋다고 한다. 예를 들어, Twillo의 Error Message의 경우 다음과 같다. { “status”:”401”, ”message”:”Authenticate”, ”code”:200003, ”more info”:”http://www.twillo.com/docs/errors/20003\" } 에러 코드와 해당 에러 코드 번호에 대한 상세 Error link도 제공한다. 이는 개발자나 Trouble Shooting하는 사람에게 많은 정보를 제공해서, 조금 더 디버깅을 손쉽게 한다. API 버전 관리 API 정의에서 중요한 것 중 하나는 버전 관리라고 한다. 이미 배포된 API의 경우 계속해서 서비스를 제공하면서, 새로운 기능이 들어간 새로운 API를 배포할 때는 하위 호환성을 보장하면서 서비스를 제공해야 하기 때문에, 같은 API라도 버전에 따라서 다른 기능을 제공하도록 하는 것이 필요하다. api.server.com/account/v2.0/groups salesforce.com/services/data/v20.0/sobjcets/Account DTO 필드가 하나인 경우 역직렬화 실패..? 평화롭게 클라이언트에서 온 정보를 @RequestBody를 통한 역직렬화를 하던 중 갑자기 역직렬화가 동작하지 않는다..? 코드에는 문제가 없고.. 뭔가 역직렬화를 하는 과정에서 문제가 생긴 거 같다. 다른 것과 비교끝에 지금 문제가 생긴 곳에서 사용하는 DTO의 필드는 한 개라는 점을 발견했고 이것과 관련이 있지 않을까 하고 찾아보았더니 인수가 1개인 생성자 사례의 모호성으로 인해 잘 알려진 문제라고 한다. //기존 코드 @PostMapping public ResponseEntity&lt;Void&gt; create(@RequestBody LineCreateDto lineCreateDto) { final Long lineId = lineService.createLine(lineCreateDto); return ResponseEntity.created(URI.create(\"/lines/\" + lineId)).build(); } public class LineCreateDto { private final String name; public LineCreateDto(final String name) { this.name = name; } public String getName() { return name; } } 왜 인수가 1개일 때는 작동하지 않을까? 인수가 1개인 생성자는 모호하기 때문에 바인딩에는 2가지 가능한 모드가 있다고 한다. 속성 기반: 입력은 JSON Object여야 하며, 속성은 생성자 인자와 이름이 일치해야 합니다. 속성 값은 일치하는 인자로 표시된 유형으로 역직렬화됩니다. 이 모드는 인자 수에 관계없이 모든 생성자에서 작동합니다. // accepts JSON value like: \"some text\" // and serializes back to JSON String public class StringWrapper { private String value; @JsonCreator(mode = JsonCreator.MODE.DELEGATING) public StringWrapper(String v) { value = v; } // commonly serialized like so: @JsonValue public String getValue() { return value; } } 위임: 입력은 모든 JSON 유형이 될 수 있으며, 생성자 인자 하나(유일한)의 유형으로 역직렬화됩니다(이름은 상관없지만 유형은 일치해야 함). 이 모드는 인수가 1개인 생성자에서만 작동합니다(실제로 @JacksonInject를 사용하는 특수한 경우가 있으며 정확한 규칙은 “주입되지 않은 매개변수가 하나뿐이어야 한다”는 것입니다). // accepts JSON value like: { \"id\" : \"xc974\" } public class IdToken { private String id; @JsonCreator(mode = JsonCreator.MODE.PROPERTIES) public IdToken(@JsonProperty(id\") String id) { this.id = id; } // will serialize as JSON object with this property: public String getId() { return id; } } @JsonCreator 같은 명시적 선언이 없을 경우 어떤 것을 쓰려는지 모호하다는 것인데 즉 “string-value” 같은 위임 모드인지 {“name” : “value”} 같은 속성 기반 모드인지 추측하려고 할 것이다. 인수가 1개인 경우 Default값이 위임 모드이기 때문에 Object 값을 기대하는 것이 아니라 String을 기대한다. 그렇기 때문에 우리가 원하는 대로 작동이 되지 않았던 것. 그냥 json으로 String을 보내면 잘 작동하는 걸 확인할 수 있다. 이를 어떻게 해결해볼 수 있을까? 기본 생성자 적용 생성자 위에 @JsonCreator(mode = Mode.PROPERTIES) 적용 생성자 위에 @ConstructorProperties(value = {value}) 적용 +리뷰어님께 받은 코드 리뷰에 대해 관심이 있으면 다음 PR들을 참고! 1단계 - 지하철 정보 관리 기능 2단계 - 경로 조회 기능 참고: REST API 이해와 설계 - #2 API 설계 가이드 https://cowtowncoder.medium.com/jackson-2-12-most-wanted-3-5-246624e2d3d0 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse spring subway",
    "url": "/woowacourse/2023-06-12-Subway/"
  },{
    "title": "우테코 - 장바구니 미션 회고",
    "text": "그동안 너무 바빠서 미션 회고를 정말 오랜만에 쓴다.. 역시 한번 일을 미루면 계속 미루게 되는 거 같아 될 수 있으면 빨리빨리 끝내는 게 제일 좋은 것 같다. 늦은 만큼 빠르게 본문으로 가보자! Save 후 어떤 걸 반환값으로 하는게 좋을까? public Long save(final ProductEntity productEntity) { final SqlParameterSource parameterSource = new BeanPropertySqlParameterSource(productEntity); return simpleJdbcInsert.executeAndReturnKey(parameterSource).longValue(); } 데이터베이스에 단순히 저장만 하고 반환값으로 아무것도 안 줄 수도 있을 것이다. 실제로 처음에 이번 미션에서 딱히 저장하고 나서 반환한 값을 쓸 일이 없을 거 같아 void로 반환하였다. 하지만 테스트하는 과정에서 연관된 객체 간의 관계를 설정해 줄 때 반환값이 없는 경우 직접 찾아 넣어줘야 되거나 각 객체의 id를 1로 초기화시키는 쿼리를 넣어서 id를 예측해서 넣어줘야 되는 번거로움이 생겼다. @Test void saveCart(){ //반환값 없는 경우 productDao.save(new ProductEntity(\"치킨\", \"testUrl\", 20000)); memberDao.save(new MemberEntity(\"testEmail\", \"testPassword\")); //dao에서 저장한 객체를 찾거나 예측해서 id를 연관객체에 설정해줘야 됨 h2CartDao.save(new CartEntity(?, ?)); } 위처럼 하는 경우 추가로 조회하는 불필요한 일이나 매 테스트마다 id를 초기화(여러 테스트가 있을 경우 rollback 해줘도 데이터는 초기화되지만 id 시작 값은 초기화되지 않음) 해서 넣어줘야 되기 때문에 id에 종속적이라 판단하였고 save 한 후 반환 값을 주는 게 좋을 것이라 생각하였다. 그리고 프로덕션 코드에서도 비즈니스 로직이 더 복잡해지면 이 저장한 값을 바탕으로 후에 객체 간의 관계 설정과 데이터 검색을 간편하게 수행할 수 있을 것 같다! @Test void saveCart(){ long productId = productDao.save(new ProductEntity(\"치킨\", \"testUrl\", 20000)); long memberId = memberDao.save(new MemberEntity(\"testEmail\", \"testPassword\")); h2CartDao.save(new CartEntity(productId, memberId)); } 그렇다면 반환 값으로 어떤 걸 주는 게 좋을까? id? 저장한 객체? 일반적으로는 객체의 고유 식별자인 id만 제공해서 관계 설정 및 업데이트를 위해 사용할 수 있을 것 같고 저장한 객체를 이용해서 추가적인 작업을 수행하는 경우에는 객체를 반환할 수 있을 것 같다. 도메인부터 설계? 데이터베이스부터 설계? 예전부터 프로젝트를 할 때 DB부터 설계를 하고 들어가서 당연히 DB부터 설계를 해야 되는 줄 알았다. 하지만, 이전 미션에서 데이터베이스 중심적으로 설계를 함으로 써 단점들을 체감을 했었기 때문에 최근에 이것에 대해 매우 고민이 많았다. DB에 굳이 저장할 필요 없는 필드들이 생성됨으로 써 코드를 짤 때 객체지향적이지 못한 코드들을 생성하게 되었다. 그래서 리뷰어님께 의견을 한번 구해봤다! 어떤 순서로 설계하더라도 결국 구현하다보면 요구사항이 변동/추가되는 일도 생기고 설계할 때는 미처 파악하지 못했던 엣지케이스들도 나오기 때문에 결국 설계를 조금 수정하는 일이 생기기 마련이더라구요 😂 프로젝트 규모에 따라, 요구사항의 복잡도에 따라 그때그때 달라지긴 하지만 보통은 화면을 보며 필요한 API 리스트업을 하고 도메인 설계에 들어가는 편입니다 설계에는 정답이 없기 때문에 무민이 가장 편하고 자신있는 방법으로 진행해도 무방하다고 생각해요 :) 어떻게 설계하든 정답은 없겠지만 나만의 기준이 필요할 것 같다. 우선 DB부터 설계하는 방식은 아무래도 데이터베이스 구조와 관계를 먼저 설계한 다음 거기에 맞춰 애플리케이션을 설계하는 것이므로 좀 더 성능 최적화에 맞춰져있지 않나 생각이 든다. 그래서 대규모 데이터베이스를 사용하는 시스템이나 성능이 중시되는 애플리케이션에 적합한 방식인 것 같다. 아직 현업을 경험해 보지 않아서 대규모니 성능이니 잘 모르겠지만 지금 우선해서 제일 중요하다고 생각하는 건 비즈니스 도메인과 요구사항이라고 생각하기 때문에 여기에 집중할 수 있는 도메인부터 설계하는게 현재로서는 최선이라고 생각한다. 그렇게 설계하고 나서 후에 성능이 개선할 필요가 있을 때 최적화를 하면 좋을 것 같다. 올바른 오류 메시지 오류 메시지는 문제가 발생할 때 개발자가 사용자와 상호 작용하는 주요 방법이라 할 수 있다. 올바르게 작성된 오류 메시지는 사용자에게 어떤 문제점이 있는지, 어떻게 해결해야 하는지 중요한 정보를 제공하지만, 잘못된 오류 메시지는 오히려 사용자에게 혼란을 줄 수 있다. 잘못된 오류 메시지는 다음과 같다. 실행 불가능 애매모호함 어떻게 해결해야 하는지 설명 없음 어떻게 오류 메시지를 작성하면 좋을까? 올바른 오류 메시지 작성을 위한 몇가지 방법들을 알아보자. 조용히 실패하지 마라 실패는 할 수 있다. 하지만 실패를 보고 하지 않는 것은 치명적이다. 왜 실패했는지 알리지 않으면 사용자들에게 올바른 정보를 전달할 수 없다. 사람들이 소프트웨어를 오용하는 걸 완전히 제거할 수는 없지만 최소화하려고 노력해야한다. 오류의 원인 파악 무엇이 잘못되었는지 사용자에게 구체적이고 정확하게 알려줘야 된다. 애매모호한 오류 메시지는 사용자에게 혼란을 줄 수 있다. ❗ Not recommended 잘못된 디렉토리입니다. 👍 Recommended 지정된 디렉토리가 존재하지만 쓸 수 없습니다. 이 디렉토리에 파일을 추가하려면 디렉토리가 쓰기 가능해야 합니다. [이 디렉토리를 쓰기 가능하게 만드는 방법에 대한 설명.] 사용자의 유효하지 않은 입력 사용자가 잘못된 입력을 할 경우 오류메시지는 잘못된 입력을 알려줘야 된다. ❗ Not recommended 우편번호가 잘못되었습니다. 👍 Recommended 미국 우편 번호는 5자리 또는 9자리로 구성되어야 합니다. 지정된 우편 번호(4872953)에는 7자리 숫자가 포함되어 있습니다. 요구 사항 및 제약 조건 지정 사용자가 시스템의 한계를 알고 있지 않기 때문에 요구 사항과 제약 조건을 이해하도록 구체적으로 알려줘야 된다. ❗ Not recommended 첨부파일의 크기가 너무 큽니다. 👍 Recommended 첨부파일의 총 크기(14MB)가 허용 한도(10MB)를 초과합니다. [가능한 솔루션에 대한 세부 정보.] 문제를 해결하는 방법 설명 실행 할 수 있는 오류 메시지를 만들어야 한다. 즉, 문제의 원인을 설명한 후 문제를 해결하는 방법을 설명해야한다. ❗ Not recommended 장치의 클라이언트 앱은 더 이상 지원되지 않습니다. 👍 Recommended 장치의 클라이언트 앱은 더 이상 지원되지 않습니다. 클라이언트 앱을 업데이트하려면 앱 업데이트 버튼을 클릭합니다. 간결하게 작성하라 불필요한 텍스트는 잘라내고 간결하고도 중요한 것을 작성해야한다. ❗ Not recommended 리소스를 찾을 수 없으며 구분할 수 없습니다. 선택한 항목이 클러스터에 없습니다. [클러스터에서 유효한 리소스를 찾는 방법에 대한 설명] 👍 Recommended 리소스가 클러스터에 없습니다. [클러스터에서 유효한 리소스를 찾는 방법에 대한 설명] 이중 부정을 피하라 이중 부정은 말의 뜻을 어렵게 만들기 때문에 피하는게 좋다. ❗ Not recommended 경로 이름 에 대한 범용 읽기 권한은 운영 체제가 액세스를 금지하는 것을 방지합니다. 👍 Recommended 경로 이름 에 대한 범용 읽기 권한이 있으면 누구나 이 파일을 읽을 수 있습니다. 모든 사람에게 액세스 권한을 부여하는 것은 보안 결함입니다. 독자를 제한하는 방법에 대한 자세한 내용은 하이퍼링크를 참조하십시오. 대상자를 고려하라 해당 오류 메시지를 읽는 사람은 개발자가 아닐 확률이 더 높다. 나에게 친숙한 용어가 대상 청중들은 익숙하지 않을 수 있기 때문에 대상자를 고려하자 ❗ Not recommended 서버 CPU 용량의 92%에서 실행 중이기 때문에 서버에서 클라이언트의 요청을 삭제했습니다. 5분 후에 다시 시도하십시오. 👍 Recommended 지금 너무 많은 사람들이 쇼핑하고 있어서 Google 시스템에서 구매를 완료할 수 없습니다. 5분 후에 다시 시도하십시오. 일관된 용어를 사용해라 하나의 오류메시지에서 무언가의 이름을 지정했으면 다른 모든 오류 메시지에도 일관되게 용어를 사용하자 ❗ Not recommended 127.0.0.1:56에서 클러스터에 연결할 수 없습니다. minikube가 실행 중인지 확인합니다. 👍 Recommended 127.0.0.1:56에서 minikube에 연결할 수 없습니다. minikube가 실행 중인지 확인합니다. 올바른 어조를 사용해라 오류 메시지의 어조는 사용자가 메시지를 해석하는 방식에 상당한 영향을 미치기 때문에 다음과 같은 올바른 어조를 사용하자. 1. 긍정적 사용자가 무엇을 잘못했는지 알려주는 대신, 바로잡는 방법을 알려주자 ❗ Not recommended 이름을 입력하지 않았습니다 👍 Recommended 이름을 입력하세요. 2. 오버해서 사과하지말자 “미안”, “제발”이라는 말은 피하고 문제와 솔루션을 명확하게 설명하는데 집중하자. ❗ Not recommended 죄송합니다. 서버 오류가 발생하여 일시적으로 스프레드시트를 로드할 수 없습니다. 불편을 끼쳐드려 죄송합니다. 잠시 기다린 후 다시 시도하십시오. 👍 Recommended Google 문서도구에서 일시적으로 스프레드시트를 열 수 없습니다. 그동안 문서 목록에서 스프레드시트를 마우스 오른쪽 버튼으로 클릭하여 다운로드해 보세요. 3. 유머를 피하자 오류 메시지에서 유머는 사용자를 화나게하거나 잘못 해석할 수 있도록 하여 원래 오류 메시지의 목적을 손상시킬 수 있다. ❗ Not recommended 서버가 실행 중인가요? 가서 잡아야 겠네요 :D. (Is the server running? Better go catch it :D.) 👍 Recommended 일시적으로 서버를 사용할 수 없습니다. 몇 분 후에 다시 시도하십시오. 4. 사용자를 비난하지 마라 비난보다는 무엇이 잘못되었는지에 대해 초점을 맞추자 ❗ Not recommended 오프라인 상태인 프린터를 지정했습니다. 👍 Recommended 지정된 프린터가 오프라인입니다. usingRecursiveComparison() 테스트코드를 작성할 때 해당 객체들의 동등성(내부 값)을 비교하는 경우가 많다. 그럴 때 마다 equals를 재정의 해줄 수도 있겠지만 usingRecursiveComparsion()을 사용하면 좀 더 편리하게 비교할 수 있다. 그리고 특정 값을 제외하고 비교할 수 있는 기능도 지원해준다. 기본적인 사용법은 아래와 같고 직접 Comparator를 정의하고 싶은 경우 usingComparator()를 사용하면 된다. @Test void testEqual() { assertThat(actualObject) .usingRecursiveComparison() // .ignoringFields(\"id\") // .ignoringActualNullFields() .isEqualTo(expectedObject); } 특정 필드값 제외 비교 - ignoringFields() null값 제외하고 비교 - ignoringActualNullFields() Argumentresolver 테스트 이번에 장바구니 미션에서 인증을 하기 위해 사용한 ArgumentResolver를 테스트 해보기위해선 resolveArgument의 파라미터인 MethodParameter, ModelAndViewContainer, NativeWebRequest, WebDataBinderFactory들을 모킹하거나 적절하게 넣어줘야 되는데 어찌 되었든 매우 테스트하기 어려웠다. 그래서 다음과 같이 통합 테스트로 인증을 넣은 경우와 안 넣은 경우를 추가로 테스트했었다. @Test void findCartItems() throws Exception { given(productService.findById(any())).willReturn(List.of( new CartItemResponse(1, new ProductEntity(1L, \"치킨\", \"chicken\", 10000)), new CartItemResponse(2, new ProductEntity(2L, \"피자\", \"pizza\", 20000)))); mockMvc.perform(get(\"/carts\") .header(AUTHORIZATION, BASIC_TYPE + Base64Coder.encodeString(EMAIL + DELIMITER + PASSWORD))) .andExpect(status().isOk()) .andExpect(jsonPath(\"$.size()\").value(2)); } @Test void noAuthentication() throws Exception { given(memberDao.findByMemberEntity(any())).willReturn( Optional.empty()); mockMvc.perform(get(\"/carts\") .header(AUTHORIZATION, BASIC_TYPE + Base64Coder.encodeString(EMAIL + DELIMITER + INVALID_PASSWORD))) .andExpect(status().isUnauthorized()); } 하지만 ArgumentResolver를 구성하는 핵심 로직만 따로 분리해서 테스트했다면 좀 더 쉽고 적절하게 테스트할 수 있었을 거 같다. 다음에는 테스트하기 어려운 경우 좀 더 작게 나눠볼 생각을 해야겠다! dao 단위 테스트를 순수하게? dao에서 어느 메서드를 테스트할 때 다른 메서드를 사용하게 되면 순수하게 그 메서드를 테스트하는 게 아니라고 생각했다. 그래서 하나의 메서드만 순수하게 테스트하기 위해 아래와 같이 jdbcTemplate으로 쿼리를 날렸다. @Test void save() { //when h2ProductDao.save(productEntity); //then final List&lt;ProductEntity&gt; productEntities = getProducts(); final ProductEntity actual = productEntities.get(0); //then assertThat(productEntities).hasSize(1); assertThat(actual) .usingRecursiveComparison() .ignoringFields(\"id\") .isEqualTo(productEntity); } private List&lt;ProductEntity&gt; getProducts() { final String sql = \"select * from product\"; final List&lt;ProductEntity&gt; productEntities = namedParameterjdbcTemplate.getJdbcOperations().query(sql, (resultSet, count) -&gt; new ProductEntity( resultSet.getLong(\"id\"), resultSet.getString(\"name\"), resultSet.getString(\"image_url\"), resultSet.getInt(\"price\") )); return productEntities; } 그런데 위 경우에는 간단해서 괜찮은데 나중에 저장해야 하는 연관 객체가 많아질 경우 쿼리도 무수하게 많아질 텐데 그 비용을 감수해서라도 순수하게 쿼리를 날리는 게 맞는 걸까라는 의문이 들었다. 그렇게 고민하던 중 문뜩 “다른 메서드를 사용하는 것을 생각하기 전에 우선 지금 @JdbcTest를 이용해 실제 데이터베이스에 접근해 테스트하는 게 단위 테스트가 맞나?”라는 생각이 들었다. 단위 테스트보단 통합 테스트에 가까운 거 같아서 다른 메서드를 통해 테스트해도 충분할 것 같다. @Test void save() { //when h2ProductDao.save(productEntity); //then final List&lt;ProductEntity&gt; productEntities = h2ProductDao.findAll(); final ProductEntity actual = productEntities.get(0); assertThat(productEntities).hasSize(1); assertThat(actual) .usingRecursiveComparison() .ignoringFields(\"id\") .isEqualTo(productEntity); } 만약에 단위 테스트로 진행하고 싶다면 jdbcTemplate을 mock 해서 안의 로직들이 제대로 실행된 지 verify를 통해 확인할 수 있을 것 같다. +리뷰어님께 받은 코드 리뷰에 대해 관심이 있으면 다음 PR들을 참고! 1단계 - 상품 관리 기능 2단계 - 장바구니 기능 참고: https://developers.google.com/tech-writing/error-messages#learning_objectives *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse spring cart",
    "url": "/woowacourse/2023-05-27-cart/"
  },{
    "title": "공통 인증 로직 어디서 처리 할 수 있을까?(feat. Interceptor, Filter, Resolver)",
    "text": "이번에 장바구니 미션에서 공통 인증 기능을 추가하기 위해 argumentResolver을 사용했다. 아래 코드와 같이 구현할 수도 있을 것이고 책임을 분리해서 인증 처리 관련 로직은 Interceptor에서 하고 argumentReoslver에서는 인증 정보만 가공해서 넘겨주도록 사용할 수도 있을 것이다. 또한, 필터를 사용해서 처리할 수도 있을 텐데 어떤 상황에서 어떤 걸 사용해야 될까 궁금해졌다. @Component public class LoginArgumentResolver implements HandlerMethodArgumentResolver { private static final String BASIC_TYPE = \"Basic\"; private static final String DELIMITER = \":\"; private final MemberDao memberDao; public LoginArgumentResolver(final MemberDao memberDao) { this.memberDao = memberDao; } @Override public boolean supportsParameter(final MethodParameter parameter) { return parameter.hasParameterAnnotation(Login.class); } @Override public Object resolveArgument(final MethodParameter parameter, final ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) { final String authorization = getAuthorization(webRequest); final String emailAndPassword = new String(Base64.getDecoder().decode(authorization)); final String[] splitEmailAndPassword = emailAndPassword.split(DELIMITER); final MemberEntity memberEntity = new MemberEntity(splitEmailAndPassword[0], splitEmailAndPassword[1]); final MemberEntity findEntity = memberDao.findByMemberEntity(memberEntity) .orElseThrow(() -&gt; new AuthenticationException(\"올바른 인증정보를 입력해주세요.\")); return findEntity.getId(); } private static String getAuthorization(NativeWebRequest webRequest) { final HttpServletRequest request = (HttpServletRequest) webRequest.getNativeRequest(); final String header = request.getHeader(HttpHeaders.AUTHORIZATION); if (header == null) { throw new AuthenticationException(\"Authorization header가 비어있습니다.\") } return header.substring(BASIC_TYPE.length()).trim(); } } 만약 이러한 중복되는 로직을 처리해 줄 수 있는 기능(Interceptor, Resolver, Filter)들을 사용하지 않으면 어떻게 될까? 인증이 필요한 곳마다 중복되는 코드가 들어가 매우 번거로울 것이다. 공통 인증 로직 어떤 걸 사용해볼 수 있을까? 크게 Filter, Interceptor, AOP 3가지가 있을 것 같다. AOP 같은 경우는 설명하려면 길어질 거 같아 나중에 따로 글을 포스팅하려고 한다. 우선 Filter와 Interceptor를 알아보기 전에 Spring의 요청 과정을 보자. 요청이 들어오면 Filter를 거쳐 DispatcherServlet에게 전달되고 DispatcherServlet은 HandlerMapping을 통해 요청을 처리할 Controller를 찾는다. 이때, 조건에 맞는(ex. url) Interceptor가 있는 경우 Interceptor가 실행된다. Filter 필터는 리소스에 대한 요청이나 리소스의 응답에 대해 부가 작업을 처리할 수 있는 기능을 제공할 수 있는 Object이다. J2EE 표준 스펙 기능으로 디스패처 서블릿에 도달하기 전에 처리하므로 스프링 범위 밖인 웹 컨테이너(ex.톰켓)에서 처리가 된다. public interface Filter { public default void init(FilterConfig filterConfig) throws ServletException {} public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException; public default void destroy() {} } init(): 웹 컨테이너가 호출하여 필터가 서비스 중임을 필터에게 알린다. 서블릿 컨테이너는 필터를 인스턴스화한 후 정확히 한 번만 init 메서드를 호출한다. 필터가 필터링 작업을 수행하도록 요청받기 전에 init 메서드가 성공적으로 완료되어야 한다. 필터 초기화 doFilter(): 필터의 doFilter 메서드는 리소스에 대한 클라이언트 요청으로 인해 요청/응답 쌍이 체인을 통과할 때마다 컨테이너에 의해 호출된다. 이 메서드에 전달된 FilterChain은 필터가 요청과 응답을 체인의 다음 엔티티로 전달할 수 있도록 한다. 여기에 공통 로직을 넣어서 원하는 작업을 처리할 수 있다. 전/후 처리 destroy(): 웹 컨테이너가 필터가 서비스 중단 중임을 알리기 위해 호출한다. 이 메서드는 필터의 doFilter 메서드 내의 모든 스레드가 종료되거나 시간 초과 기간이 지난 후에만 호출된다. 웹 컨테이너가 이 메서드를 호출한 후에는 필터의 이 인스턴스에서 doFilter 메서드를 다시 호출하지 않는다. 필터 종료 Filter 사용해보기 public class MoominFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { //... } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { //전처리 로직 chain.doFilter(request, response); //후처리 로직 } @Override public void destroy() { //... } } @Configuration public class WebMvcConfiguration implements WebMvcConfigurer { @Bean public FilterRegistrationBean filterRegistrationBean() { FilterRegistrationBean registrationBean = new FilterRegistrationBean(new MoominFilter(MoominFilter.class.getSimpleName())); registrationBean.addUrlPatterns(\"/*\"); registrationBean.setOrder(1); return registrationBean; } } 그래서 필터는 어디에 사용해볼 수 있을까? 필터는 스프링과 무관하게 주로 요청 자체에 대한 공통 처리를 담당한다. 주로 다음과 같이 사용될 수 있다고 한다. Authentication Filters(인증 필터) Logging and Auditing Filters (로깅 및 감사 필터) Image conversion Filters(이미지 변환 필터) Data compression Filters(데이터 압축 필터) Encryption Filters(암호화 필터) Tokenizing Filters(토큰화 필터) Filters that trigger resource access events(리소스 액세스 이벤트를 트리거하는 필터) XSL/T filters(XSL/T 필터) Mime-type chain Filter(마임형 체인 필터) +Spring MVC에서 분리하려는 모든 기능 +필터는 스프링 빈으로 등록이 가능할까? 옛날에는 필터가 스프링 빈으로 등록되지 못하였지만, DelegatingFilterProxy가 등장하면서 빈으로 등록할 수도 있게 되었고 다른 빈을 주입받을 수도 있게 되었다. 궁금하면 다음글을 한번 읽어보자 Interceptor 인터셉터를 사용하여 애플리케이션은 특정 handler들에 대해 기존 또는 커스텀 인터셉터를 등록하여 각 handler 구현체에 대해 수정할 필요 없이 공통 전처리 동작을 추가할 수 있다. 인터셉터는 스프링이 제공하는 기술으로 컨트롤러를 호출하기 전과 후로 요청과 응답에 공통 작업을 처리할 수 있다. 위의 Spring 요청 과정에서 Dispatcher Servlet은 핸들러 매핑을 통해 컨트롤러를 찾는다 했는데 그 결과값으로 실행 체인을 반환한다. 이 실행 체인에 인터셉터들이 등록되어 있다면 각 인터셉터를 거쳐 컨트롤러를 실행하고 없으면 바로 컨트롤러가 실행된다. public interface HandlerInterceptor { default boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return true; } default void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception { } default void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception { } } preHandle(): 핸들러 실행 전의 인터셉션 지점이다. HandlerMapping이 적절한 핸들러 객체를 결정한 후 HandlerAdapter가 핸들러를 호출하기 전에 호출된다. 컨트롤러 이전에 처리해야 되는 전처리 작업 3번째 파라미터 handler 파라미터는 컨트롤러 빈에 매핑되는 HandlerMethod라는 새로운 타입의 객체로써, @RequestMapping이 붙은 메소드의 정보를 추상화한 객체 preHandle의 반환 값이 true면 다음 단계로 진행, false면 이후 작업은 진행되지 않음. postHandle(): 핸들러가 성공적으로 실행된 후의 인터셉션 지점이다. HandlerAdapter가 실제로 핸들러를 호출한 후 DispatcherServlet이 뷰를 렌더링하기 전에 호출된다. 지정된 ModelAndView를 통해 뷰에 추가 모델 객체를 노출할 수 있다. 컨트롤러 이후 처리해야 되는 후처리 작업 컨트롤러에서 작업을 진행하다 중간에 예외가 발생하면 postHandle은 호출되지 않음. afterCompletion(): 요청 처리가 완료된 후, 즉 뷰를 렌더링한 후 호출된다. 핸들러 실행의 모든 결과에 대해 호출되므로 적절한 리소스 정리가 가능하다. afterCompletion은 컨트롤러 중간에 예외가 터지더라도 호출된다. Interceptor 사용해보기 public class MoominInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //전처리 로직 return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { //후처리 로직 } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { //afterCompletion } } @Configuration public class WebMvcConfiguration implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(new MoominInterceptor()) .addPathPatterns(\"/**\"); } } 그래서 인터셉터는 어디에 사용해볼 수 있을까? 사실 이 부분을 적다가 정말 시간이 많이 걸렸다. 필터와 인터셉터 둘 다 요청, 응답에 대한 전후 처리를 담당할 수 있는데 많은 글, 토론, 문서들에서 필터와 인터셉터의 용도 차이 관련 내용을 봤을 때 “어..? 필터에 적힌 내용은 인터셉터에서도 가능한 부분인데..? 어..? 인터셉터에 적힌 것도 필터에서 충분히 가능한 것들이잖아..?” 하고 무한 도르마무에 빠지게 되었다.. 일단 많은 사람들이 말하는 인터셉터의 용도는 인터셉터는 스프링 내부에서 작동하다 보니 필터보다 좀 더 정교한 작업들이 가능하다는 것이다. 그렇게 말하고 막상 밑에 나오는 예시들을 보면 위에 적힌 필터와 내용이 비슷하다. 봤던 예시들로는 보안 및 인증, 로깅, 정보 가공 등… 음..? 필터랑 뭐가 다름? 이러면 뭘 사용하라는 거지..? 취향에 따라 사용하라는 건가? 그래서 좀 더 기준을 잡는 데 도움이 될 수 있도록 filter나 interceptor에서만 가능한 것들을 알아보자. 공식문서에는? 우선 HandlerInterceptor 문서를 보면 다음과 같이 나와있다. HandlerInterceptor is basically similar to a Servlet Filter, but in contrast to the latter it just allows custom pre-processing with the option of prohibiting the execution of the handler itself, and custom post-processing. Filters are more powerful, for example they allow for exchanging the request and response objects that are handed down the chain. 핸들러 인터셉터는 기본적으로 서블릿 필터와 유사하지만, 필터와 달리 핸들러 자체의 실행을 금지하는 옵션이 있는 사용자 정의 사전 처리와 사용자 정의 사후 처리를 허용한다. 필터는 체인을 통해 전달되는 요청 및 응답 객체를 교환할 수 있는 등 더 강력하다고한다. Note that a filter gets configured in web.xml, a HandlerInterceptor in the application context. As a basic guideline, fine-grained handler-related preprocessing tasks are candidates for HandlerInterceptor implementations, especially factored-out common handler code and authorization checks. On the other hand, a Filter is well-suited for request content and view content handling, like multipart forms and GZIP compression. This typically shows when one needs to map the filter to certain content types (e.g. images), or to all requests. 필터는 애플리케이션 컨텍스트의 핸들러 인터셉터인 web.xml에서 구성된다는 점에 유의해야한다. 기본 지침으로, 세분화된 핸들러 관련 전처리 작업, 특히 팩터링된 공통 핸들러 코드와 권한 검사 등이 HandlerInterceptor 구현의 후보가 될 수 있다. 반면에 필터는 여러 부분으로 구성된 양식 및 GZIP 압축과 같은 요청 콘텐츠 및 보기 콘텐츠 처리에 적합하다. 이는 일반적으로 특정 콘텐츠 유형(예: 이미지) 또는 모든 요청에 필터를 매핑해야 하는 경우에 나타난다. 예외 처리 필터와 인터셉터는 실행 시점이 다르기 때문에 서로 예외 처리하는 부분에서 다르다. Filter는 스프링 밖의 서블릿 영역에서 관리되기 때문에 에외가 발생하면 Spring의 도움을 받을 수 없다. 만약 인증이 되지 않아 401을 던져줘야 하는 상황이 있다고 해보자. 필터에서 Exception이 던져졌다면 에러가 처리되지 않고 Servlet으로 다시 올라온다. 그러면 서버에 문제가 있다고 판단하여 500으로 응답해버린다. 해결하려면 아래와 같이 응답 객체에 예외 처리를 추가해 줘야 된다. @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { HttpServletResponse servletResponse = (HttpServletResponse) response; servletResponse.setStatus(HttpServletResponse.SC_UNAUTHORIZED); servletResponse.getWriter().print(\"INVALID AUTHENTICATION\"); } 하지만 Interceptor에서 예외가 발생하면? 인터셉터 같은 경우 스프링의 도움을 받아 @ExceptionHandler를 사용해서 전역적으로 예외를 처리해 줄 수 있다. 또한, 에러가 Servlet까지 올라오지 않아 다시 ErrorController를 호출하지 않는다. 예외 처리를 잘 모르면 다음글을 참고해 보자. 만약 전후 처리 로직에서 예외를 전역적으로 잡고 싶다면 인터셉터를 고려해 보자! @ExceptionHandler(AuthenticationException.class) public ResponseEntity&lt;String&gt; handleMyException(AuthenticationException e) { return ResponseEntity.status(HttpStatus.UNAUTHORIZED).body(e.getMessage()); } ServletRequest, ServletResponse 교체 가능 Filter에서는 ServletRequest와 ServletResponse를 교체할 수 있다. 내부 상태를 변경한다는 것이 아니다.(내부 상태는 Interceptor도 변경 가능, 하지만 객체 자체를 교체하지 못함) 다음과 같이 다른 객체로 교체할 수 있다는 것이다. public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) { //다음 필터를 호출하기 위해 필터체이닝을 호출 해주는데 이때 request, response를 교체해줄 수 있음 chain.doFilter(new MoominServletRequest(), new MoominServletResponse()); } 실제로 바꿔줄 일이 없어 보이지만 있다!! 예를 들어, HttpServletRequest의 body를 로깅할 때 바꿔줄 필요가 있을 것이다. HttpServletRequest는 본문을 읽기 위해 getInputStream()을 사용하는데 InputStream은 한 번 읽으면 다시 읽을 수 없다. 즉, body를 읽기 위해 Interceptor나 Filter에서 body를 읽게 되면 이후 Controller에서 Json 데이터를 바인딩 처리할 때 아래와 같은 에러를 만나게 된다. java.lang.IllegalStateException: getReader() has already been called for this request org.springframework.http.converter.HttpMessageNotReadableException: Could not read JSON: Stream closed; nested exception is java.io.IOException: Stream closed 그래서 body를 로깅하기 위해서는 다음과 같이 여러 번 읽을 수 있도록 하는 커스트마이징 된 request가 필요할 것이다. public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) { chain.doFilter(new CustomCachedServletRequest(request), response); } 좀 더 세분화된 제어 가능 Interceptor의 preHandle() 파라미터에서 handler라는 이름으로 HandlerMethod 객체가 들어온다고 했었다. 이 HandlerMethod를 통해 실제 핸들러에 접근할 수 있기 때문에 필터보다 더 세밀하게 제어할 수 있다. 즉, 요청이 실제로 수행하는 작업에 따라 수행하는 작업이 달라질 수 있다. (서블릿 필터는 모든 요청에 일반적으로 적용되어 각 요청 변수만 고려할 수 있음) 또한 Interceptor의 postHandle()에서 ModelAndView()가 들어온다고 했는데 이걸로 view를 렌더링 하기 전에 추가 작업을 해줄 수 있다. 예를 들어, 페이지가 권한에 따라 view를 다르게 노출해주는 부분이 있을 때 따로 처리를 해줄 수 있을 것이다. -&gt; 동작이 대상 핸들러에 따라 달라지거나 요청 처리와 뷰 렌더링 사이에 무언가를 수행하려는 경우 HandlerInterceptor를 사용하면 좀 더 세밀하게 제어 가능하다! 사전 차단 필터는 Servlet Container에서 실행되며, Servlet Container 레벨에서 HTTP 요청/응답을 처리하므로 Filter에서 불필요한 요청을 미리 걸러내면 스프링 컨테이너에 도달하기 전에 막을 수 있어 약간이라도 성능과 안정성을 높일 수 있을 것 같다. 보통 Spring Security를 이용해서 인증, 인가를 구현하는 걸 많이 봤는데 왜 Spring Security는 Filter를 사용하는 걸까? Spring Security’s web infrastructure is based entirely on standard servlet filters. It doesn’t use servlets or any other servlet-based frameworks (such as Spring MVC) internally, so it has no strong links to any particular web technology. It deals in HttpServletRequests and HttpServletResponses and doesn’t care whether the requests come from a browser, a web service client, an HttpInvoker or an AJAX application. Spring Security의 웹 인프라는 전적으로 표준 서블릿 필터를 기반으로 한다. 내부적으로 서블릿이나 기타 서블릿 기반 프레임워크(ex. Spring MVC)를 사용하지 않으므로 특정 웹 기술과의 강력한 연결고리가 없다. 이 필터는 HttpServletRequests와 HttpServletResponses를 처리하며 요청이 브라우저, 웹 서비스 클라이언트, HttpInvoker 또는 AJAX 애플리케이션에서 오는지 여부는 상관하지 않는다고 한다. 즉, 특정 서블릿에 관계없이 사용하기 위해서인거 같은데.. 이름부터 Spring이 들어가는데 Spring에 의존적인거 아닐까 생각이 들 수 있다. Spring 프레임워크를 기반으로 개발된 프레임워크이기 때문에 Spring이 이름이 들어가나 Spring Security는 Spring 프레임워크에 종속되는 부분을 최소화하고 서블릿 API에 종속되는 부분을 추상화하여 Spring 프레임워크와 종속적이지 않은 보안 프레임워크로 설계되어 있다고 한다. 또한, 다양한 환경에서 사용할수 있도록 유연성을 갖추고 있다. 여기에 대해 더 관심 있으면 FilterChainProxy, DelegatingFilterProxy에 대해 찾아보자 그럼 ArgumentResolver는 뭘까? ArgumentResolver 주어진 요청의 컨텍스트에서 메서드 매개변수를 인자 값으로 해석하기 위한 전략 인터페이스이다. 즉, 요청에 들어온 값으로부터 원하는 객체를 만들어 반환해 줄 수 있다. public interface HandlerMethodArgumentResolver { boolean supportsParameter(MethodParameter parameter); @Nullable Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception; } supportsParameter(): 주어진 메서드 매개변수가 이 리졸버에서 지원되는지 여부이다. 실행되길 원하는 파라미터 앞에 특정 애노테이션을 붙일 수 있다. 특정 애노테이션을 포함하고 있으면 true을 반환 resolveArgument(): 메서드 매개변수를 주어진 요청의 인자 값으로 해석한다. supportsParameter에서 true를 받은 경우, parameter를 원하는 형태로 반환하는 메서드 정리 공통 로직들을 작성할 때 Filter, Interceptor를 사용할 수 있을 것이다. 둘 다 전후 처리를 해줄 수 있어 어느 것을 사용하여도 상관없을 수 있다. 하지만 각자 할 수 있는 부분도 있기 때문에 이 차이점을 파악하고 사용하면 좀 더 능동적인 개발을 할 수 있지 않을까 한다. 이 글이 그 차이점을 파악하는 데 도움이 되었으면 좋겠다! 참고: https://docs.oracle.com/javaee/6/api/javax/servlet/Filter.html https://docs.spring.io/spring-framework/docs/3.0.x/javadoc-api/org/springframework/web/servlet/HandlerInterceptor.html https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/method/support/HandlerMethodArgumentResolver.html https://docs.spring.io/spring-security/site/docs/3.1.4.RELEASE/reference/security-filter-chain.html https://justforchangesake.wordpress.com/2014/05/07/spring-mvc-request-life-cycle/ https://mangkyu.tistory.com/173 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse spring interceptor filter argumentresolver",
    "url": "/woowacourse/2023-05-05-Filter-Interceptor/"
  },{
    "title": "우테코 - 웹 자동차 경주 회고",
    "text": "이번 웹 자동차 경주 미션은 Level 1 때 했던 콘솔 자동차 경주에 Spring과 DB를 적용해 웹 자동차 경주로 변환하는 미션이다. 처음 콘솔에서 웹으로 전환해 보는 과정이라 신기하면서도 재밌었다. 하지만 그 과정 속에서 헷갈리거나 혼란스러운 부분이 많았다. dao와 repository, entity의 정의? 등 헷갈리는 부분이 많아서 많은 고민을 해보며 성장했던 것 같다. 그리고 웹으로 전환하면서 Spring과 DB를 도입했는데 뭔가 레벨 1 때의 객체지향을 잊어버리고 데이터 중심적으로 짠 거 같아 아쉬움이 많이 남아 앞으로 이를 잘 생각해 보며 짜야겠다는 생각이 들었다. 데이터 중심 관점 설계 이번에 api의 결과 값으로 다음과 같은 Response를 응답해줘야 됐다. HTTP/1.1 200 Content-Type: application/json [ { \"winners\": \"브리\", \"racingCars\": [ { \"name\": \"브리\", \"position\": 6 }, { \"name\": \"토미\", \"position\": 4 }, { \"name\": \"브라운\", \"position\": 3 }, ] }, { \"winners\": \"브리,토미,브라운\", \"racingCars\": [ { \"name\": \"브리\", \"position\": 6 }, { \"name\": \"토미\", \"position\": 6 }, { \"name\": \"브라운\", \"position\": 6 }, ] } ] 그래서 처음에 다음과 같은 table을 만들게 되었는데.. 지금보니 참 무지성으로 만든거 같다 ㅋㅋㅋ CREATE TABLE PLAY_RESULT ( id BIGINT NOT NULL AUTO_INCREMENT, winners VARCHAR(50) NOT NULL, trial_count INT NOT NULL, created_at DATETIME NOT NULL default current_timestamp, PRIMARY KEY (id) ); CREATE TABLE PLAYER ( id BIGINT NOT NULL AUTO_INCREMENT, play_result_id INT NOT NULL, name VARCHAR(5) NOT NULL, position INT NOT NULL, PRIMARY KEY (id), FOREIGN KEY (play_result_id) REFERENCES PLAY_RESULT (id) ); 위처럼 설계하여 제출하였더니, 리뷰어님께서 다음과 같이 피드백을 주셨다. 이번에는 PlayerResult를 제거하고, 도메인 내에서 raw한 DB값을 통해 결과값을 계산해보게 도메인을 구성해보셨으면 좋겠어서, 한번더 요청드립니다 :) 생각해 보면 PlayerResult는 결과물인 데이터에 초점이 맞춰져 있어 데이터베이스가 view에 의존적이고 재사용하지 못해 유연성이 떨어진다. 그리고 PLAYER 만으로 충분히 만들 수 있기 때문에 불필요한 테이블이라고 생각해 볼 수 있었다. 그래서 이번엔 PLAY_RESULT에 사용하지 않는 필드는 지워 각 게임에 대한 식별자인 id만 주고 PLAYER에서 승자를 판단할 수 있게 winner 필드를 하나 추가해 주었다. 지금 생각해 보니 PLAY_RESULT 테이블을 지우지 않은 건, 각 게임에 대한 플레이어들을 PLAY_RESULT_ID로 쉽게 찾기 위해 남겨뒀지 않나 생각이 든다. CREATE TABLE PLAY_RESULT ( id BIGINT NOT NULL AUTO_INCREMENT, PRIMARY KEY (id) ); CREATE TABLE PLAYER ( id BIGINT NOT NULL AUTO_INCREMENT, play_result_id BIGINT NOT NULL, winner BOOL NOT NULL, name VARCHAR(5) NOT NULL, position INT NOT NULL, PRIMARY KEY (id), FOREIGN KEY (play_result_id) REFERENCES PLAY_RESULT (id) ); 역시 무지몽매한 나는 리뷰어님의 의도를 한 번에 이해하지 못해서 또 한 번 피드백으로 뚜드려 맞았다. PlayResult는 어떤 역할을 하는걸까요? 단순 데이터 저장의 역할을 하는것으로 보이는데요. 이경우, 해당 테이블은 객체지향의 관점에서 나온걸까요, 아니면 데이터 중심의 관점에서 나온걸까요? 객체들의 협력이 이루어지는 애플리케이션을 작성하기 위해서는, 각 객체들의 역할과 책임, 그리고 각 객체들의 협력을 집중하는 것이 우선시 되야한다 생각하는데요. 현재 PlayResult의 경우, 하나의 자료구조로 밖에 보이지 않아요. 그 이유로는 속성이 auto-increse되는 id뿐이다. 해당 속성은 자바 객체의 기본 식별자인 주솟값과 별반 차이가 없어 보여요. 식별의 책임을 DB에서 수행하게 위임한거라 판단됩니다. 즉 이경우 result의 식별자를 저장하기 위한 하나의 bundle로 보이며, 이는 PLAYER 테이블의 속성으로 가져가도 좋아보여요. (테이블의 이름도 GAME등으로 변경되도 문제없어보여요) 만약 조금더 객체지향의 관점에서 시작했다면 어땠을까요?! Result 보단은 각각의 PLayer가 책임을 수행하기 위해 필요한 속성이 무엇인지 고려한다. Player의 결과를 Player가 알고있는가? 혹은 알고있어야 하는가? 결과를 책임져줄 친구는 없을까? 여기서는 어떤 객체가 필요할까? 등이 되지 않았을까 생각합니다 :) 각 게임에 대한 플레이어들을 PLAY_RESULT 테이블 없이도 충분히 가능하고, PLAYER에 winner 필드가 없이도 충분히 구현 단계에서 객체지향적으로 구현 가능하다. CREATE TABLE PLAYER ( id BIGINT NOT NULL AUTO_INCREMENT, game BIGINT NOT NULL, name VARCHAR(5) NOT NULL, position INT NOT NULL, PRIMARY KEY (id) ); 위와 같이 테이블을 설계 후 구현 단계에서 결과를 책임져줄 객체를 생성해 주어 game마다 player들의 position을 비교해서 승자를 판별하여 winners와 players들을 만들어 응답해 주면 된다. LEVEL2가 되면서 Spring과 DB를 도입을 해 공부에 대한 우선순위가 이쪽으로 많이 기울어졌다. 그래서 점점 LEVEL1 때 공부한 객체지향을 조금씩 놓치고 있던 것 같다. 하지만 결국 스프링은 결국 개발자가 더 객체지향적으로 짤 수 있게 도와주는 프레임워크이다. 본질인 객체지향을 놓치지 말고 함께 균형을 맞춰가며 LEVEL2 미션을 진행해나가야겠다. +) 이미 미션이 Merge 되어서 더 pr을 보낼 수 없었지만 리뷰어님의 의도가 내가 결론 내린 부분과 맞는지 궁금했기 때문에 개인적으로 DM을 남겼다! 주말인데도 성심껏 답변해 주시는 리뷰어님.. 감사합니다 🙇🙇🙇 예시를 들으니 더욱더 와닿는 거 같다. DAO vs Repository 둘 다 데이터베이스나 영속성 접근을 캡슐화하는 역할을 수행하는 데 구체적으로 둘이 어떤 게 다르고 어떻게 사용할 수 있을까? DAO(Data Access Object) DAO는 데이터에 접근하기 위한 객체로 데이터베이스 혹은 다른 영속성(Persistence)에 대한 접근을 추상화해주는 역할을 수행한다. 쉽게 말하면 응용 계층에서 어떤 영속성 저장 방식을 사용할 것인지 알 필요 없게 하는 역할을 한다고 생각하면 된다. public class CarService { private final PlayerDao playerDao; ... } public interface PlayerDao { void insertAll(List&lt;Player&gt; players); List&lt;Player&gt; findAllPlayer(long playResultId); } //InMemory를 이용하여 Player를 저장하는 dao public class PlayerInMemoryDao implements PlayerDao { ... } //JDBC를 이용하여 Player를 저장하는 dao @Repository public class PlayerJdbcDao implements PlayerDao { ... } Repository Repository는 도메인 영역에서 도메인을 다루기 위한 저장소의 역할로 추상화되어 있다. 예를 들어, 하나의 주문을 저장할 때 주문뿐 아니라 주문 상품들도 같이 저장되어야 하고 조회, 삭제시에도 같이 조회, 삭제되어야 한다고 하자. @Service @Transactional public class OrderService { private final OrderDao orderDao; private final OrderItemDao orderItemDao; public OrderService(final OrderDao orderDao, final OrderItemDao orderItemDao) { this.orderDao = orderDao; this.orderItemDao = orderItemDao; } //주문 저장 시 주문 상품들도 같이 저장 public void order() { orderDao.save(); orderItemDao.save(); } //주문 조회 시 주문 상품들도 같이 조회 public Order findOrder() { Order order = orderDao.find(); List&lt;OrderItem&gt; orderItems = orderItemDao.findAllByOrder(); order.setOrderItems(orderItems); return order; } // 주문 삭제 시 연관된 주문들 또한 같이 삭제 public void deleteOrder() { orderItemDao.delete(); orderDao.delete(); } } 여기서 주문과 함께 저장, 조회, 삭제되어야 될 것들이 늘어나면 Service 코드도 계속해서 변경되어야 한다. 이는 Domain Model에 대한 캡슐화가 잘되지 않았기 때문에 발생한 일이다. 주문을 저장, 조회, 삭제할 때 주문 항목들도 같이 저장, 조회, 삭제되게 하는 주문이라는 도메인 엔티티의 진입점 역할을 하는 OrderRepository를 사용하면 OrderService의 복잡도를 줄일 수 있다. @Service @Transactional public class OrderService { private final OrderRepository orderRepository; public PlaceOrderService(final OrderRepository orderRepository) { this.orderRepository = orderRepository; } public void order() { orderRepository.save(order); //Order와 OrderItem을 함께 저장 } public Order findOrder() { return orderRepository.findById(); // OrderItem이 세팅된 Order조회 } public void deleteOrder() { orderRepository.deleteById(); //Order와 관련된 OrderItem을 함께 삭제 } } 즉, Repository는 연관이 있고 함께 동작이 수행해야 되는 여러 도메인 모델 사이에서 진입점을 제공해 해당 도메인을 다루는 저장소의 역할이라 할 수 있다. 하지만 이번 웹 자동차 경주에서는 이렇게 연관된 도메인을 다룰 일이 없어서 repository는 따로 생성하지 않고 dao만 구현해서 진행하였다! DTO와 Domain 간의 변환 작업은 어디서 수행? 우선 Repository Layer는 영속성을 관리하는 계층으로 Client에서 사용할 DTO를 Repository에서 변환하는 건 맞지 않다고 생각했다. 그럼 Controller는 어떨까? Controller에서 생성하게 되면 다음과 같은 단점이 있을 것 같다. Domain이 Controller에 결합됨 각 Domain을 가져오기 위한 service의 수가 많아질 수 있음 각 Domain들을 가져와서 가공할 일이 있을 경우 비즈니스 로직이 Controller에 들어갈 수 있음 그래서 결국 Service에서 DTO로 변환하는 역할을 주었는데 그렇다고 Service에 있는 게 완벽하냐고 하면 또 그렇지 않다. Service 단계에서 각 상황에 맞는 dto를 알고 있어야 되니 presentation 영역을 침범(즉, view에 의존) Service 레이어에 DTO가 들어오지 않아야, 여러 종류의 컨트롤러에서 해당 서비스 사용 가능 결국 또 진리의 trade off이다.. 상황에 맞게 써야 하는데 그 상황은 많이 겪어보지 않았으니 앞으로 미션들에 이것저것 적용해 보면서 겪어봐야 될 것 같다. DB Entity vs Domain Entity? 예전에 JPA를 이용해서 프로젝트를 할 때 하나의 Entity 객체가 DB관점에서의 Entity 역할도 수행하고 Domain 관점으로의 Entity도 수행했기 때문에 이번에 다음과 같이 두 개의 객체로 나눠져서 진행하니 약간 혼란스러웠다. //JPA @Entity public class Car { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private final Name name; private final Position position; ... } //이번 미션 public class Car { private final Name name; private final Position position; ... } public class CarEntity { private long id; private final String name; private final int position; ... } 그래서 리뷰어님께 질문드렸었는데 질문할 당시에는 용어가 헷갈렸어서 Entity vs Domain Model로 질문을 해버렸다.. 하지만 덕분에 각 용어에 대해 자세히 알게 되어서 오히려 좋아 👍👍 도메인 해결하고자 하는 문제 영역 도메인 모델(Domain Model) 도메인(문제 영역)을 개념적으로 표현한 것 UML, Diagram, graph, formula 등등 개념 영역 도메인 모델 패턴(Domain Model Pattern) 도메인(문제 영역)을 객체 지향 기법으로 구현하는 패턴 도메인 계층의 객체 모덺 구현 영역 엔티티(Entity) 도메인 모델을 구현하기 위한 도구 구현 영역 무민이 docs에 적어주신 요구사항은 도메인을 모델링하기 위한 첫 발걸음이에요. 즉 요구사항에서는, 핵심 구성요소, 규칙, 기능들이 포함되어 있는데, 이것 자체가 도메인 모델이죠. 요구사항을 개념적으로 도출했으니, 이를 구현해야겠죠? 이는 객체지향으로도, 절차지향으로도, 함수형으로도 작성될 수 있어요. 저희는 java를 쓰고 있으니, 자연스럽게 도메인 모델 패턴을 사용하여, 도메인 모델 패턴을 사용하여, 객체 모델로 구현을 할 거고, 이때 필요한 객체들이 entity나 값 객체가 되는것들이에요. https://martinfowler.com/bliki/EvansClassification.html 위의 마틴 파울러의 글에서 마지막 문단을 읽어보면 이 용어(Entity)는 다른 개념과 심하게 뒤섞인다. 엔티티는 종종 데이터베이스 테이블 또는 데이터 베이스 테이블에 해당하는 객체를 나타내는데 사용되기 때문에 이 용어를 사용하는 경우 어떤 의미에 따라 사용하고 있는지 명확히 해야 된다고 한다. 즉, Entity의 이러한 혼란은 어떠한 맥락에 따라 말하는지 달라지기 때문에 당연한 것이다. API Error 처리 콘솔에서 하던 것처럼 에러를 throw 하면 스프링 부트가 기본 오류 처리로 500 Internal Server Error를 던져준다. 그래서 에러마다 다른 메시지, 상태 코드(400)를 던져주기 위해 @ExceptionHandler, @ControllerAdvice를 적용해 보고 어떤 원리로 작동하는지 궁금해서 정리해 봤다. Exception Handler는 어떤 원리로 작동하는가 개행 관련 설정 마지막 줄 개행 누락 한번 씩 파일마다 마지막 줄에 개행이 누락된 것이 자꾸 보인다..ㅠ 방지하기 위해 Intellij 설정을 해보자 Preferences(설정) -&gt; Editor(에디터) -&gt; General(일반)에서 제일 아래 On Save(저장 시) 부분에서 Ensure every saved file ends with a line break(모든 저장된 파일이 줄 바꿈으로 끝나도록 함) 부분을 체크하면 된다. 코드 스타일 설정 Preferences(설정) -&gt; Editor(에디터) -&gt; Code Style(코드 스타일)에서 Schema(구성표)를 원하는 포맷 스타일로 설정해서 Command + Option + I로 전체 코드를 통일성 있게 가져갈 수 있다. 많은 사람들이 쓰는 Google Style을 적용해보자 https://github.com/google/styleguide에서 intellij-java-google-style.xml을 다운로드 후 Schema(구성표) 옆에 있는 톱니바퀴를 누른 후 Import Sheme(구성표 가져오기) -&gt; Intellij IDEA code style XML를 선택하고 다운받은 xml 선택 +리뷰어님께 받은 코드 리뷰에 대해 관심이 있으면 다음 PR들을 참고! 1단계 - 웹 자동차 경주 2단계 - 웹 자동차 경주 참고: 말랑갓 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse spring pair-programming",
    "url": "/woowacourse/2023-04-29-Web-Racing-Car/"
  },{
    "title": "DI란 무엇이고 사용하면 뭐가 좋을까?",
    "text": "DI는 Depedency Injection의 줄임말로 의존관계 주입이라고 한다. 의존관계(Dependency) 우선 DI를 설명하기 전에, 의존관계(Dependency)가 뭘까? 다음과 같은 코드를 A가 B에 대한 의존관계를 가지고 있다고 할 수 있다. class A { private final B b; public A() { b = new b(); } } 이렇게 의존관계를 가지고 있을 시, B가 변하게 되면 그 영향이 A에 미칠 수 있다. 즉, B에 대한 결합도가 높아지게 되는데 결합도가 높아지면 어떻게 될까? 결합도가 높을수록 하나의 모듈이 다른 모듈에 종속적이게 되어 유지 보수와 변경이 어려워진다. 또한, 확장성과 재사용성이 감소된다. 추상화를 이용하여 관계를 조금 느슨하게 하여 결합도를 낮출 수 있다. class A { private final BInterface bInterface; public A() { bInteface = new BInterfaceImpl1(); //bInteface = new BInterfaceImpl2(); //bInteface = new BInterfaceImpl3(); } public void print() { System.out.println(bInterface.print()); } } interface BInterface { print(); } class BInterfaceImpl1 implements BInterface { @Override public String print() { return \"B1\"; } } ... 하지만 아직까지도 추상화된 Interface인 BInterface뿐 아니라 구체 클래스인 BInterfaceImpl1(BInterface2, BIntreface3)도 의존하고 있다. Dependency Injection(DI) 토비의 스프링에서는 다음의 세가지 조건을 충족하는 작업을 의존관계 주입이라고 한다 클래스 모델이나 코드에는 런타임 시점의 의존관계가 드러나지 않는다. 그러기 위해서는 인터페이스만 의존하고 있어야 한다. 런타임 시점의 의존관계는 컨테이너나 팩토리 같은 제3의 존재가 결정한다. 의존관계는 사용할 오브젝트에 대한 레퍼런스를 외부에서 제공(주입)해줌으로써 만들어진다. 이처럼 그 의존관계를 외부에서 결정하고 주입하는 것을 의존관계 주입이라고 한다. 즉, A 안의 BInterface를 내부적으로 어떤 값을 가질지 정하는 것이 아니라 런타임 시점의 의존관계를 외부에서 결정하고 주입해주는 것이다. class A { private final BInterface bInterface; public A(BInterface bInterface) { this.bInterface = bInterface; } public void print() { System.out.println(bInterface.print()); } } A a = new A(new BInterfaceImpl1()); a.print(); //B1 위처럼 외부에서 DI를 주입함으로 써 DIP까지 만족하게 되었다. 하지만 추상화와 DI를 이용하여 결합도를 낮추고 DIP까지 만족하게 되었지만 아직 OCP를 만족하지 못하였다. DIP: 프로그래머는 추상화(인터페이스)에 의존해야지 구현체(클래스)에 의존하면 안된다. OCP: 기존의 코드를 변경하지 않으면서 기능을 확장할 수 있어야 함. //A a1 = new A(new BInterfaceImpl1()); A a2 = new A(new BInterfaceImpl2()); //BInterfaecImpl2로 변경 위처럼 직접 DI를 주입했을 때는 기능을 확장하려면 기존의 코드를 수정해야 한다. 하지만 스프링을 이용하여 DI를 주입하게 되면 이러한 문제점을 해결할 수 있다. 애플리케이션의 전체 동작 방식을 구성(config) 하기 위해 구현 객체를 생성하고, 연결하는 책임을 가지는 별도의 설정 클래스 생성(혹은 각 객체에@Component 사용할 수도 있음) @Configuration public class AppConfig{ @Bean public BInterface bInterface() { return new BInterfaceImpl1()); } } 위와 같은 설정 클래스에 @Configuration을 붙이고 주입하려고 하는 의존관계를 @Bean을 붙여 빈 등록을 한다. AnnotationConfigApplicationContext ac = new AnnotationConfigApplicationContext(AppConfig.class); A a = ac.getBean(A.class); //b = BInterfaceImpl1 a.print() //B1 이렇게 하면 BInterfaceImpl1이 주입된 A 객체를 사용할 수 있다. BInterfaceImpl1이 아닌 BInterfaceImpl2로 변경하고 싶다면 AppConfig의 다음 부분만 수정하면 된다. @Configuration public class AppConfig{ @Bean public BInterface bInterface() { return new BInterfaceImpl2()); //이 부분만 수정 } } AnnotationConfigApplicationContext ac = new AnnotationConfigApplicationContext(AppConfig.class); A a = ac.getBean(A.class); //b = BInterfaceImpl2 a.print(); //B2 기존의 코드는 변경되지 않았지만, 내부의 의존성은 BInterfaceImpl2로 변경된 걸 확인할 수 있다. 이처럼 스프링을 사용하여 DI를 주입하면 기존의 코드를 변경하지 않고도 기능 확장이 가능하다. DI의 장점 결합도의 감소: DI를 사용하면 객체 간의 결합도가 낮아지므로 개발, 유지보수성이 높아진다. 테스트 용이성: DI를 사용하면 의존성을 주입하기 때문에 객체의 동작을 검증하기 위해 테스트 객체를 주입하여 테스트 용이성을 높일 수 있다. 재사용성: 객체 간의 결합도를 낮추기 때문에 객체의 재사용성을 높인다. 가독성: DI는 객체 간 의존성을 명시하고 객체 생성 및 의존성 주입을 한 곳에서 관리하기 때문에 코드의 가독성을 높일 수 있다. 아래는 예전에 스프링의 강력한 무기 DI에 대해 작성한 글이므로 참고 Spring의 강력한 무기 참고: 의존관계 주입(Dependency Injection) 쉽게 이해하기 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse spring DI",
    "url": "/woowacourse/2023-04-23-DI/"
  },{
    "title": "Exception Handler는 어떤 원리로 작동하는가",
    "text": "잘못 입력했을 경우마다 다른 에러 메시지, 상태 코드(400, 404.. 등)를 던져주고 싶어서 @ExceptionHandler, @ControllerAdvice를 적용해 봤다. @RestControllerAdvice public class CarControllerAdvice { @ExceptionHandler public ResponseEntity&lt;ErrorResult&gt; handleException(RacingCarException racingCarException) { return ErrorResult.toResponseEntity(String.valueOf(racingCarException.getStatus().value()), racingCarException.getMessage()); } } 다음과 같이 각 에러마다 다른 메시지, 상태 코드를 던져주는 걸 볼 수 있다. 이게 어떻게 가능한 걸까? 스프링 부트 기본 오류 처리 스프링 부트는 기본 오류 처리를 제공한다. BasicErrorController 코드를 한번 보자. ... public class BasicErrorController extends AbstractErrorController { @RequestMapping(produces = MediaType.TEXT_HTML_VALUE) public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) { HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections .unmodifiableMap(getErrorAttributes(request, getErrorAttributeOptions(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView != null) ? modelAndView : new ModelAndView(\"error\", model); } @RequestMapping public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) { HttpStatus status = getStatus(request); if (status == HttpStatus.NO_CONTENT) { return new ResponseEntity&lt;&gt;(status); } Map&lt;String, Object&gt; body = getErrorAttributes(request, getErrorAttributeOptions(request, MediaType.ALL)); return new ResponseEntity&lt;&gt;(body, status); } ... } 에러가 터졌을 때 클라이언트 요청의 Accept 헤더 값이 text/html인 경우 errorHtml()을 호출해서 error view를 제공하고, 그 외 경우 error()를 호출해 ResponseEntity()로 HTTP Body에 에러를 JSON 형태로 반환한다. 에러를 따로 잡지 않았을 경우 다음과 같은 기본 500 에러가 터지게 된다. 이걸 발생하는 예외에 따라 다른 상태 코드나 메시지로 처리하려면 어떻게 해야 될까? HandlerExceptionResolver HandlerExceptionResolver는 매핑 또는 실행 중에 throw된 예외를 해결할 수 있는 인터페이스이다. HandlerExceptionResolver가 적용되기 전을 한번 보자. 에러가 터지면 정상적으로 처리되지 않고 중간에 끊겨서 WAS로 에러가 전달되고 그 후 후속 조치(띄울 에러 페이지를 다시 요청한다거나)가 일어난다. HandlerExceptionResolver가 적용되고 나면 에러가 터져도 ExceptionResolver가 예외를 해결하고 WAS로 정상적으로 응답을 전달한다. 우선 가장 원초적인 방법으로 직접 HandlerExceptionResolver를 만들어서 사용해 보자. public interface HandlerExceptionResolver { ModelAndView resolveException( HttpServletRequest request, HttpServletResponse reponse, Object handler, Exception ex); } public class CustomExceptionResolver implements HandlerExceptionResolver { @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse reponse, Object handler, Exception ex) { try { if (ex instanceof IllegalArgumentException) { response.sendError(HttpServletResponse.SC_BAD_REQUEST, ex.getMessage()); return new ModelAndView(); } } catch (IOException e) { log.error(\"resolver ex\", e); } return null; } } public class WebConfig implements WebMvcConfigurer { ... @Override public void extendHandlerExceptionResolvers(List&lt;HandlerExceptionResolver&gt; resolvers) { resolvers.add(new CustomExceptionResolver()); } ... } IllegalArgumentException이 발생하면 response.sendError(400)을 호출해서 HTTP 상태 코드를 400으로 지정하고 빈 ModelAndView를 반환한다. 이렇게 ModelAndView를 반환하여 Exception을 정상 흐름으로 변경하게 하는 것이다. HandlerExceptionResolver의 반환 값에 따라 DispatcherServlet의 동작 방식은 각자 다르다. 빈 ModelAndView()가 반환되면 뷰를 렌더링 하지 않고, 정상 흐름으로 서블릿이 리턴된다. ModelAndView 안에 View나 Model 같은 정보가 들어있는 상태로 반환하게 되면 뷰를 렌더링 한다. null이 반환될 시, 다음 HandlerExceptionResolver를 찾아서 실행하고 맞는 resolver가 없을 시 기존에 발생한 예외를 서블릿 밖으로 던진다. HandlerExceptionResolver를 다음과 같이 활용해볼 수 있다. 예외 상태 코드 반환 서블릿에서 상태 코드에 따른 오류를 처리하도록 위임 이후 WAS는 서블릿 오류 페이지를 찾아서 호출 그래서 이 경우에는 2번(첫 호출, error 후 재호출)의 컨트롤러 호출 과정이 발생하게 됨. ex) response.sendError(xxx); 뷰 템플릿 처리 ModelAndView에 View나 Model 같은 정보를 넣어 예외에 따른 새로운 오류 화면 제공 ex) new ModelAndView(“error/500”); API 응답 처리 HTTP 응답 바디에 직접 데이터를 넣어 JSON으로 응답하면 API 응답 처리 가능 ex) response.getWriter().println(“Error!!”); 그런데 각 에러마다 이렇게 직접 HandlerExceptionResolver를 구현해 주려고 하니 상당한 비용이 든다. 그래서 친절한 스프링님께서 HandlerExceptionResolver를 제공해 주는데 어떤 게 있는지 알아보자. DefaultHandlerExceptionResolver 앞으로 설명할 3개의 HandlerExceptionResolver 중 가장 우선순위가 낮다. DefaultHandlerExceptionResolver는 표준 Spring MVC 예외들을 해결하고 해당 HTTP 상태 코드로 변환하는 기본 구현체이다. 예를 들어, 파라미터 바인딩 시점에 타입이 맞지 않으면 내부에서 TypeMismatchException이 발생하고 결과적으로 500 오류가 발생해야 된다. 하지만 파라미터 바인딩은 대부분 클라이언트가 잘못 요청해서 발생하는 문제기 때문에 HTTP에서 400을 사용하도록 되어있다. 그래서 DefaultHandlerExceptionResolver는 이 오류를 500이 아니라 400으로 변경해서 상황에 맞게 처리해 준다. 내부 코드를 살펴보면 다음과 같은 표준 예외들을 처리해 주는 걸 볼 수 있다. @Override @Nullable protected ModelAndView doResolveException( HttpServletRequest request, HttpServletResponse response, @Nullable Object handler, Exception ex) { try { if (ex instanceof HttpRequestMethodNotSupportedException) { return handleHttpRequestMethodNotSupported( (HttpRequestMethodNotSupportedException) ex, request, response, handler); } else if (ex instanceof HttpMediaTypeNotSupportedException) { return handleHttpMediaTypeNotSupported( (HttpMediaTypeNotSupportedException) ex, request, response, handler); } else if (ex instanceof HttpMediaTypeNotAcceptableException) { return handleHttpMediaTypeNotAcceptable( (HttpMediaTypeNotAcceptableException) ex, request, response, handler); } else if (ex instanceof MissingPathVariableException) { return handleMissingPathVariable( (MissingPathVariableException) ex, request, response, handler); } else if (ex instanceof MissingServletRequestParameterException) { return handleMissingServletRequestParameter( (MissingServletRequestParameterException) ex, request, response, handler); } else if (ex instanceof ServletRequestBindingException) { return handleServletRequestBindingException( (ServletRequestBindingException) ex, request, response, handler); } else if (ex instanceof ConversionNotSupportedException) { return handleConversionNotSupported( (ConversionNotSupportedException) ex, request, response, handler); } else if (ex instanceof TypeMismatchException) { return handleTypeMismatch( (TypeMismatchException) ex, request, response, handler); } else if (ex instanceof HttpMessageNotReadableException) { return handleHttpMessageNotReadable( (HttpMessageNotReadableException) ex, request, response, handler); } else if (ex instanceof HttpMessageNotWritableException) { return handleHttpMessageNotWritable( (HttpMessageNotWritableException) ex, request, response, handler); } else if (ex instanceof MethodArgumentNotValidException) { return handleMethodArgumentNotValidException( (MethodArgumentNotValidException) ex, request, response, handler); } else if (ex instanceof MissingServletRequestPartException) { return handleMissingServletRequestPartException( (MissingServletRequestPartException) ex, request, response, handler); } else if (ex instanceof BindException) { return handleBindException((BindException) ex, request, response, handler); } else if (ex instanceof NoHandlerFoundException) { return handleNoHandlerFoundException( (NoHandlerFoundException) ex, request, response, handler); } else if (ex instanceof AsyncRequestTimeoutException) { return handleAsyncRequestTimeoutException( (AsyncRequestTimeoutException) ex, request, response, handler); } } catch (Exception handlerEx) { if (logger.isWarnEnabled()) { logger.warn(\"Failure while trying to resolve exception [\" + ex.getClass().getName() + \"]\", handlerEx); } } return null; } 그중 TypeMismatchException를 처리해 주는 코드를 보면 response.sendError()를 통해서 문제를 해결한다. 그래서 WAS에서 다시 오류 페이지(/error)를 내부 요청한다. protected ModelAndView handleTypeMismatch(TypeMismatchException ex, HttpServletRequest request, HttpServletResponse response, @Nullable Object handler) throws IOException { response.sendError(HttpServletResponse.SC_BAD_REQUEST); return new ModelAndView(); } 아래와 같은 매핑에서 문자를 입력해서 요청을 하면 TypeMismatchException이 발생하고 주석과 같은 오류가 발생한다. @GetMapping(\"/default-handler\") public String defaultHandlerException(@RequestParam Integer data) { return \"test\"; } // http://localhost:8080/default-handler?data=test 요청시 // // { // \"timestamp\": \"2023-04-18T13:51:14.707+00:00\", // \"status\": 400, // \"error\": \"Bad Request\", // \"message\": \"Failed to convert value of type 'java.lang.String' to required type 'java.lang.Integer'; nested exception is java.lang.NumberFormatException: For input string: \\\"test\\\"\", // \"path\": \"/default-handler\" // } response에 message가 안 뜨면 application.properties에 다음과 같은 옵션을 추가하자 server.error.include-message=always ResponseStatusExceptionResolver 세 개의 HandlerExceptionResolver 중 두 번째의 우선순위를 가진다. @ResponseStatus 애노테이션을 사용하여 예외를 HTTP 응답 코드에 매핑할 수 있게 처리해 준다. 또한, 5.0 부턴 ResponseStatusException을 지원하여 예외를 처리할 수 있다. 다음과 같이 @ResponseStatus 애노테이션을 적용하면 HTTP 상태 코드를 변경해 주고 메시지도 담을 수 있다. @ResponseStatus(code = HttpStatus.BAD_REQUEST, reason = \"잘못된 요청 오류\") public class BadRequestException extends RuntimeException { } @GetMapping(\"/response-status\") public String responseStatus() { throw new BadRequestException(); } // http://localhost:8080/response-status 요청시 // // { // \"timestamp\": \"2023-04-18T14:11:22.601+00:00\", // \"status\": 400, // \"error\": \"Bad Request\", // \"message\": \"잘못된 요청 오류\", // \"path\": \"/response-status\" // } ResponseStatusExceptionResolver의 처리 코드를 살펴보면 다음과 같이 sendError를 실행하는 것을 볼 수 있다. 그래서 이것도 마찬가지로 WAS에서 다시 오류 페이지(/error)를 요청한다. protected ModelAndView applyStatusAndReason(int statusCode, @Nullable String reason, HttpServletResponse response) throws IOException { if (!StringUtils.hasLength(reason)) { response.sendError(statusCode); } else { String resolvedReason = (this.messageSource != null ? this.messageSource.getMessage(reason, null, reason, LocaleContextHolder.getLocale()) : reason); response.sendError(statusCode, resolvedReason); } } return new ModelAndView(); @ResponseStatus는 개발자가 직접 변경할 수 없는 예외에는 적용할 수 없다.(ex. IllegalArgumentException()) 또한. 애노테이션을 사용하기 때문에 조건에 따라 동적으로 변경하는 것도 어려운데 그때는 ResponseStatusException을 사용하면 된다. @GetMapping(\"/response-status-exception\") public String responseStatusException() { throw new ResponseStatusException(HttpStatus.NOT_FOUND, \"잘못된 요청입니다.\", new IllegalArgumentException()); } // http://localhost:8080/response-status-exception 요청시 // // { // \"timestamp\": \"2023-04-18T14:23:25.185+00:00\", // \"status\": 404, // \"error\": \"Not Found\", // \"message\": \"잘못된 요청입니다.\", // \"path\": \"/response-status-exception\" // } ExceptionHandlerExceptionResolver ExceptionHandlerExceptionResolver는 @ExceptionHandler라는 애노테이션을 통해 예외를 해결한다. ExceptionResolver 중에 우선순위가 가장 높고 실무에서도 API 예외 처리는 대부분 이 기능을 사용한다. 사용 방법은 @ExceptionHandler 애노테이션을 선언하고, 처리하고 싶은 예외를 지정해 주면 된다. 지정한 예외 또는 그 예외의 자식 클래스는 모두 잡아준다. public class ErrorResult { private String code; private String message; } @ExceptionHandler(IllegalArgumentException.class) public ErrorResult handleIllegal(IllegalArgumentException e) { return new ErrorResult(\"400\", e.getMessage()); } 예외 처리 과정을 보면 IllegalArgumentException이 발생하게 되면 ExceptionResolver 중 가장 우선순위가 높은 ExceptionHandlerExceptionResolver가 실행 ExceptionHandlerExceptionResolver는 해당 컨트롤러에 IllegalArgumentException을 처리할 수 있는 @ExceptionHandler가 있는지 확인 handleIllegal을 발견하고 실행 @ControllerAdvice 엄청나게 많은 @ExceptionHandler가 Controller에 섞여 있으면 가독성이 좋지 않다. 이를 @ControllerAdvice를 사용해 분리해 볼 수 있다. @RestControllerAdvice public class ControllerAdvice { @ExceptionHandler public ResponseEntity&lt;ErrorResult&gt; ex1(Exception1 e) { ... } @ExceptionHandler public ResponseEntity&lt;ErrorResult&gt; ex2(Exception2 e) { ... } @ExceptionHandler public ResponseEntity&lt;ErrorResult&gt; ex3(Exception3 e) { ... } } @ControllerAdvice는 대상을 지정하지 않으면 모든 컨트롤러에 적용되므로 특정한 컨트롤러에만 적용하고 싶으면 대상 컨트롤러를 지정하자. 지정하는 방법은 다음 스프링 공식 문서에 잘 나와있으니 읽어 보자. 참고: HandlerExceptionResolver DefaultHandlerExceptionResolver ResponseStatusExceptionResolver ExceptionHandlerExceptionResolver Spring 인프런 - Spring MVC1 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse spring exception-handler resolver",
    "url": "/woowacourse/2023-04-19-Exception-Handler/"
  },{
    "title": "객체를 요청하거나 반환하면 어떻게 적절하게 처리되는걸까?",
    "text": "이번에 콘솔 자동차 경주를 웹 자동차 경주로 바꾸면서 다음과 같은 코드가 있었는데 어떻게 json 입력이 GameInfo 파라미터에 매핑되고, WinnerCarDto가 json으로 출력되는지 궁금해졌다. 난 아무것도 한 게 없는데.. 어떻게 자동으로 변환되는 것일까? 스프링은 신인가..? @RestController public class CarController { private final CarService carService; public CarController(CarService carService) { this.carService = carService; } @PostMapping(\"/plays\") public WinnerCarDto playGame(@RequestBody GameInfo gameInfo) { final WinnerCarDto winnerCarDto = carService.playGame(gameInfo); return winnerCarDto; } } //request //{names: \"a,b,c\", count: \"10\"} //response //{\"winners\":[\"a\",\"c\"],\"racingCars\":[{\"name\":\"a\",\"position\":6},{\"name\":\"b\",\"position\":5},{\"name\":\"c\",\"position\":6}]} public class GameInfo { private final String names; private final String count; public GameInfo(final String names, final String count) { this.names = names; this.count = count; } public String getNames() { return names; } public String getCount() { return count; } } HttpMessageConverter JSON 데이터를 HTTP 메시지 바디에서 직접 읽거나 쓰는경우 적절한 HttpMessageConverter가 알맞게 변환시켜준다. HttpMessageConverter는 HTTP 요청 및 응답으로 변환하기 위한 전략 인터페이스이다. 다음의 경우에 HTTP 메시지 컨버터를 적용한다. HTTP 요청: @RequestBody, HttpEntity(RequestEntity) HTTP 응답: @ResponseBody, HttpEntity(ResponseEntity) HttpMessageConverter 인터페이스를 살펴보자. public interface HttpMessageConverter&lt;T&gt; { boolean canRead(Class&lt;?&gt; clazz, @Nullable MediaType mediaType); boolean canWrite(Class&lt;?&gt; clazz, @Nullable MediaType mediaType); List&lt;MediaType&gt; getSupportedMediaTypes(); default List&lt;MediaType&gt; getSupportedMediaTypes(Class&lt;?&gt; clazz) { return (canRead(clazz, null) || canWrite(clazz, null) ? getSupportedMediaTypes() : Collections.emptyList()); } T read(Class&lt;? extends T&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException; void write(T t, @Nullable MediaType contentType, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException; canRead(), canWrite(): 메시지 컨버터가 해당 클래스, 미디어 타입을 지원하는지 체크한다. read(), write(): 메시지 컨버터를 통해서 메시지를 읽고 쓰는 기능 WebMvcConfigurationSupport 클래스의 addDefaultHttpMessageConverters() 메서드를 살펴보면 다양한 메시지 컨버터를 볼 수 있는데 다음과 같은 우선 순위를 가지고 있다. 만약 각 메시지 컨버터에 만족하지 않으면 다음으로 우선순위가 넘어간다. ByteArrayHttpMessageConverter StringHttpMessageConverter ResourceHttpMessageConverter ResourceRegionHttpMessageConverter SourceHttpMessageConverter Jaxb2RootElementHttpMessageConverter KotlinSerializationJsonHttpMessageConverter MappingJackson2HttpMessageConverter GsonHttpMessageConverter … (너무 많아서 생략) 이 중 대표적인 세가지 메시지 컨버터만 살펴보도록 하자 ByteArrayHttpMessageConverter: byte[] 데이터 처리 만족 조건 클래스 타입: byte[] 미디어 타입: */* 요청 example: @RequestBody byte[] byteRequest 응답 example: @ResponseBody return byte[], 쓰기 미디어타입 application/octet-stream StringHttpMessageConverter: String 문자로 데이터 처리 만족 조건 클래스 타입: String 미디어 타입: */* 요청 example: @RequestBody String stringRequest 응답 example: @ResponseBody return “test”, 쓰기 미디어타입 text/plain MappingJackson2HttpMessageConverter: application/json 데이터 처리 만족 조건 클래스 타입: 객체 또는 HashMap 미디어 타입: application/json 요청 example: @RequestBody JacksonData data 응답 example: @ResponseBody return jacksonData, 쓰기 미디어타입 application/json 관련 다음과 같은 요청이 오면 어떤 HttpMessageConverter가 작동할까? content-type: application/json @RequestMapping void test(@RequestBody String test) { ... } HTTP 요청이 와서, 컨트롤러에서 @RequestBody 파라미터 사용 올바른 메시지 컨버터를 찾기위해 컨버터를 돌면서 canRead() 호출 대상 클래스 타입을 지원하는가 String HTTP 요청의 Content-Type 미디어 타입을 지원하는가 application/json StringHttpMessageConverter 조건을 만족해 read()를 호출하여 객체 생성 후 반환 content-type: application/json @RequestMapping void test(@RequestBody TestData test) { ... } HTTP 요청이 와서, 컨트롤러에서 @RequestBody 파라미터 사용 올바른 메시지 컨버터를 찾기위해 컨버터를 돌면서 canRead() 호출 대상 클래스 타입을 지원하는가 TestData 객체 HTTP 요청의 Content-Type 미디어 타입을 지원하는가 application/json MappingJackson2HttpMessageConverter 조건을 만족해 read()를 호출하여 객체 생성 후 반환 다음과 같은 응답을 하면 어떤 HttpMessageConverter가 작동할까? @ResponseBody @RequestMapping TestData test(...) { return new TestData(test); } 컨트롤러에서 @ResponseBody로 값을 반환 올바른 메시지 컨버터를 찾기위해 컨버터를 돌면서 canWrite() 호출 대상 클래스 타입을 지원하는가 TestData 객체 HTTP 요청의 Accept 미디어 타입을 지원하는가.(produces) application/json MappingJackson2HttpMessageConverter 조건을 만족해 write() 를 호출하여 HTTP 응답 메시지 바디에 데이터를 생성한다. @ResponseBody @RequestMapping String test(...) { return \"test\"; } 컨트롤러에서 @ResponseBody로 값을 반환 올바른 메시지 컨버터를 찾기위해 컨버터를 돌면서 canWrite() 호출 대상 클래스 타입을 지원하는가 String HTTP 요청의 Accept 미디어 타입을 지원하는가.(produces) text/plain StringHttpMessageConverter 조건을 만족해 write() 를 호출하여 HTTP 응답 메시지 바디에 데이터를 생성한다. 응답의 경우 produces로 미디어 타입을 정해주지 않으면 어떻게 될까 궁금해서 돌려봤더니 각 클래스 타입을 만족하는 컨버터의 미디어 타입으로 반환되었습니다! ex) String -&gt; text/plain ex) 객체 -&gt; application/json ex) byte[] -&gt; application/octet-stream 그렇다면 @RequestBody의 객체와 요청 받은 데이터가 다르면 어떻게 될까? @RestController public class CarController { private final CarService carService; public CarController(CarService carService) { this.carService = carService; } @PostMapping(\"/plays\") public WinnerCarDto playGame(@RequestBody GameInfo gameInfo) { final WinnerCarDto winnerCarDto = carService.playGame(gameInfo); return winnerCarDto; } } //request //{names: \"a,b,c\", count: \"10\", test: \"100\"} //request //{names: \"a,b,c\"} 첫번째 request를 넣게 되면 GameInfo에 names, count밖에 없으므로 test는 누락되게 된다. 두번째 request를 넣게 되면 GameInfo에 names만 채워지고 count는 null로 들어 오게 된다. 참고: docs - HttpMessageConverter docs - WebMvcConfigurationSupport 인프런 - Spring MVC1 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse spring message-converter",
    "url": "/woowacourse/2023-04-13-Message-Converter/"
  },{
    "title": "우테코 - 체스 미션 회고",
    "text": "얼마 전, Level 1 마지막 미션인 체스 미션을 마무리하였다. 하지만 뭔가 시원하지 않고 오히려 고민만 많아져서 글을 작성하는 지금도 찝찝함이 남아있다. 지금까지 배운 것들에 대해 아직 제대로 정리도 되지 않았고 또한, 여러 가지를 맛보다 보니 더 깊게 공부해야 될 필요성을 느끼게 되어 걱정이 많아진 것이다. 그래서 Level 2에서는 Level 2 미션과 동시에 Level 1 때 덜 한 것을 보충하려 한다. 목차 mermaid 사용해서 도메인 다이어그램 나타내기 명령어 좀 더 깔끔하게 처리하기 Null 객체 패턴 Stream이 꼭 필요한 것일까? 상속의 단점을 직접 경험 오브젝트 스터디 mermaid 사용해서 도메인 다이어그램 나타내기 저번 블랙잭 회고할 때 Over README를 작성하다 보니 시간이 너무 지체되어 이번에는 간단히 mermaid라는 툴로 도메인 다이어그램을 그리고 그에 맞는 도메인 기능들을 작성하자 마음먹었었는데 계획했던 대로 잘 됐던 것 같아 뿌듯하다. 하지만 설계할 때 역시 누구한테 어떤 책임을 부여할지는 아직도 어려운 숙제인 것 같다. 우아한 객체지향 세미나에서 조영호 님께서 이런 말씀을 하셨다. “항상 코드를 짜면 저는 의존성(dependency)을 종이에 그려본다. 의존성을 그렸을 때 뭔가 이상한 게 있으면 코드는 정말 이상한 부분들이 많다. 코드는 최종 결과물은 예쁠 수 있지만 과정은 지저분하다. 초반에는 저도 코드를 절차적으로 짤 때가 많다. 왜냐면 생각이 나지 않는다. 이걸 어떻게 쪼갤지도 모르겠고 어디에 넣어야 될지도 모르겠음. 그래서 일단 코드를 짜고 의존성을 보면서 개선하다 보면 자기가 원하는 구조로 가는 경우가 많다.” 경험이 많으신 조영호 님도 한 번에 설계하는 것이 아니라 점차 개선하는데, 병아리인 내가 어떻게 한 번에 설계를 완성시키려 했나 생각이 든다. 앞으로는 의존성을 그려보고 점차 개선해 봐야겠다. 명령어 좀 더 깔끔하게 처리하기 이번 체스 미션에서는 start, end, status, move 등 다양한 명령어에 따른 행동이 실행된다. 그에 따라 어떠한 메커니즘을 사용하지 않으면 Controller에 if 분기문이 그만큼 많이 들어가게 되어 가독성이 좋지 않을 거라 생각되어서 커맨드 패턴과 비슷한 구조를 도입하게 되었다. 커맨드 패턴: 커맨드 패턴이란 요청을 객체의 형태로 캡슐화하여 사용자가 보낸 요청을 나중에 이용할 수 있도록 매서드 이름, 매개변수 등 요청에 필요한 정보를 저장 또는 로깅, 취소할 수 있게 하는 패턴 체스 미션 1, 2단계에서는 다음과 같이 BiConsumer 함수형 인터페이스를 Command 객체 안에 넣어놓고 controller에서 command.execute()만 실행하면 각 명령어가 실행되도록 만들어놨다. package chess.controller; import chess.domain.game.ChessGame; import chess.domain.position.File; import chess.domain.position.Position; import chess.domain.position.Rank; import java.util.Arrays; import java.util.function.BiConsumer; public enum Command { START(\"start\", (chessGame, ignored) -&gt; chessGame.start()), END(\"end\", (chessGame, ignored) -&gt; chessGame.end()), MOVE(\"move\", moveOrNot()); public static final int SOURCE_INDEX = 1; public static final int TARGET_INDEX = 2; public static final int FILE_INDEX = 0; public static final int RANK_INDEX = 1; private final String name; private final BiConsumer&lt;ChessGame, String[]&gt; consumer; Command(final String name, final BiConsumer&lt;ChessGame, String[]&gt; consumer) { this.name = name; this.consumer = consumer; } private static BiConsumer&lt;ChessGame, String[]&gt; moveOrNot() { return (chessGame, splitCommand) -&gt; { final String[] source = splitPosition(splitCommand, SOURCE_INDEX); final String[] target = splitPosition(splitCommand, TARGET_INDEX); final Position sourcePosition = Position.of(File.getFile(source[FILE_INDEX]), Rank.getRank(Integer.parseInt(source[RANK_INDEX]))); final Position targetPosition = Position.of(File.getFile(target[FILE_INDEX]), Rank.getRank(Integer.parseInt(target[RANK_INDEX]))); chessGame.moveOrNot(sourcePosition, targetPosition); }; } private static String[] splitPosition(final String[] splitCommand, final int index) { return splitCommand[index].split(\"\"); } public static Command findByString(final String name) { return Arrays.stream(values()) .filter(command -&gt; command.name.equals(name)) .findFirst() .orElseThrow(() -&gt; new IllegalArgumentException(\"잘못된 커맨드입니다.\")); } public void execute(final ChessGame chessGame, final String[] splitCommand) { this.consumer.accept(chessGame, splitCommand); } } 하지만 3, 4단계에서 db가 도입되고 난 후 명령어를 실행하면 영속성까지 건드릴 일이 생겼다. 예를 들어, moveOrNot() 명령어를 실행하고 나면 db의 체스 위치를 update시켜줘야 한다. 그래서 BiConsumer 부분을 Controller로 빼기로 했고 EnumMap을 사용하여 기존의 Command를 각 Biconsumer에 매핑 시켜줬다. package chess.controller; import chess.domain.game.ChessGame; import chess.domain.piece.Side; import chess.domain.position.File; import chess.domain.position.Position; import chess.domain.position.Rank; import chess.service.ChessGameService; import chess.view.InputView; import chess.view.OutputView; import java.sql.SQLException; import java.util.EnumMap; import java.util.Map; import java.util.function.BiConsumer; public class ChessController { public static final int COMMAND_INDEX = 0; public static final int SOURCE_INDEX = 1; public static final int TARGET_INDEX = 2; public static final int FILE_INDEX = 0; public static final int RANK_INDEX = 1; private final Map&lt;Command, BiConsumer&lt;ChessGame, String[]&gt;&gt; commands = new EnumMap&lt;&gt;(Command.class); private final ChessGameService chessGameService; public ChessController(ChessGameService chessGameService) { putCommands(); this.chessGameService = chessGameService; } private void putCommands() { commands.put(Command.START, (chessGame, ignored) -&gt; start(chessGame)); commands.put(Command.END, (chessGame, ignored) -&gt; end(chessGame)); commands.put(Command.STATUS, (chessGame, ignored) -&gt; status(chessGame)); commands.put(Command.MOVE, this::moveOrNot); } private void start(ChessGame chessGame) { chessGame.start(); } private void end(ChessGame chessGame) { chessGame.end(); } private void status(ChessGame chessGame) { final Double whiteScore = chessGame.calculateScore(Side.WHITE); final Double blackScore = chessGame.calculateScore(Side.BLACK); OutputView.printScore(whiteScore, blackScore); OutputView.printWinner(Side.calculateWinner(whiteScore, blackScore)); } private void moveOrNot(ChessGame chessGame, String[] splitCommand) { final String[] source = splitPosition(splitCommand, SOURCE_INDEX); final String[] target = splitPosition(splitCommand, TARGET_INDEX); final Position sourcePosition = Position.of(File.getFile(source[FILE_INDEX]), Rank.getRank(parseRank(source))); final Position targetPosition = Position.of(File.getFile(target[FILE_INDEX]), Rank.getRank(parseRank(target))); chessGame.moveOrNot(sourcePosition, targetPosition); chessGameService.update(chessGame, sourcePosition, targetPosition); } private int parseRank(final String[] source) { try { return Integer.parseInt(source[RANK_INDEX]); } catch (NumberFormatException e) { throw new IllegalArgumentException(\"올바른 위치를 입력해주세요.\"); } } private String[] splitPosition(final String[] splitCommand, final int index) { return splitCommand[index].split(\"\"); } public void run() throws SQLException { ChessGame chessGame = initChessGame(inputInitCommand()); while (chessGame.isRunnable()) { printChessBoard(chessGame); executeCommand(chessGame); } processIfClear(chessGame); } private InitCommand inputInitCommand() { try { final String command = InputView.readInitCommand(); return InitCommand.findByString(command); } catch (IllegalArgumentException e) { OutputView.printErrorMessage(e); return inputInitCommand(); } } private ChessGame initChessGame(InitCommand command) throws SQLException { ChessGame chessGame = findChessGameIfContinue(command); if (chessGame == null) { OutputView.printNewGameMessage(); chessGameService.delete(); chessGame = chessGameService.save(); } return chessGame; } private ChessGame findChessGameIfContinue(final InitCommand command) throws SQLException { ChessGame chessGame = null; if (command.isContinue()) { chessGame = chessGameService.findChessGame(); printContinueMessage(chessGame); } return chessGame; } private void printContinueMessage(final ChessGame chessGame) { if (chessGame == null) { OutputView.printNonContinueMessage(); return; } OutputView.printContinueMessage(); } private void printChessBoard(final ChessGame chessGame) { if (chessGame.isStart()) { OutputView.printBoard(chessGame.getBoard()); } } private void executeCommand(ChessGame chessGame) { try { final String[] splitCommand = InputView.readCommand().split(\" \"); final Command command = Command.findByString(splitCommand[COMMAND_INDEX]); commands.get(command).accept(chessGame, splitCommand); } catch (IllegalArgumentException e) { OutputView.printErrorMessage(e); executeCommand(chessGame); } } private void processIfClear(final ChessGame chessGame) { if (chessGame.isClear()) { final Side winner = chessGame.calculateWinner(); chessGameService.delete(); OutputView.printKingDie(winner); OutputView.printScore(chessGame.calculateScore(Side.WHITE), chessGame.calculateScore(Side.BLACK)); OutputView.printWinner(winner); } } } Null 객체 패턴 이번에 체스보드를 보면 Map&lt;Position, Piece&gt;로 객체를 관리하였다. 그런데 생각해 보면 체스보드에 기물이 없는 빈칸도 있을 것인데 이를 어떻게 처리해야 될까 고민하다 빈 기물 객체를 넣기로 하였다. Null 객체 패턴이 있는 걸 몰랐었는데 코드를 구현하다 보니깐 Null 객체 패턴과 비슷하게 되어 뭔가 신기했다. package chess.domain.piece; import chess.domain.board.Board; import chess.domain.position.Path; import chess.domain.position.Position; import java.util.List; public class Empty extends Piece { public Empty(final Type type, final Side side) { super(type, side, List.of()); } @Override protected void validate(final Type type, final Side side) { validateType(type); validateSide(side); } @Override public Path findMovablePositions(final Position source, final Board board) { throw new UnsupportedOperationException(\"지원하지 않는 메서드입니다.\"); } } Null 객체 패턴이란 뭘까? 우선, null 검사 코드를 사용할 때 단점은 개발자가 null 검사 코드를 누락하기 쉬워 NullPointerException(NPE)을 발생시킬 가능성을 높인다. 그래서 null 객체 패턴을 사용함으로 써 null을 대신할 객체를 리턴하여 null 검사 코드를 없앨 수 있도록 한다. 이렇게 되면 향후 코드 수정을 보다 쉽게 할 수 있다는 장점이 있다. Stream이 꼭 필요한 것일까? 이번에 점수를 더할 때 for를 쓸지 stream을 써서 계산을 할지 고민했다. 현재 for문 같은 경우 그냥 한바퀴만 돌면 되지만 stream으로 하게되면 기본 점수로 더할 때 한바퀴, 그리고 폰이 2개이상 있는 곳마다 점수를 마이너스 해주기 위해 또 한바퀴가 돌게된다. 즉, stream을 사용함으로써 불필요하게 한바퀴를 더 도는거 같아서 적용을 결국 안했다. 코드로 둘 다 비교를 해보자 //for문 public double calculateScore(Side side) { double sum = 0; for (int file = LOWER_BOUNDARY; file &lt;= UPPER_BOUNDARY; file++) { sum += addScoreByRank(file, side); } return sum; } private double addScoreByRank(final int file, Side side) { double sum = 0; int pawnCount = 0; for (int rank = LOWER_BOUNDARY; rank &lt;= UPPER_BOUNDARY; rank++) { final Piece piece = getPiece(Position.of(File.getFile(file), Rank.getRank(rank))); sum += addScoreIfRightColor(piece, side); pawnCount += addPawnCount(piece, side); } sum = minusIfPawnOver(sum, pawnCount); return sum; } private double minusIfPawnOver(double sum, final int pawnCount) { if (pawnCount &gt;= OVER_COUNT) { sum -= pawnCount * (Type.PAWN.getValue() / DIVIDE_VALUE); } return sum; } private int addPawnCount(final Piece piece, Side side) { if (side.equals(piece.getSide()) &amp;&amp; piece.isPawn()) { return 1; } return 0; } private double addScoreIfRightColor(final Piece piece, Side side) { if (side.equals(piece.getSide())) { return piece.getScore(); } return 0; } //stream문 final double score = board.values().stream() .filter(piece -&gt; piece.getSide() == side) .mapToDouble(Piece::getScore) .sum(); final Map&lt;Integer, Long&gt; countByFile = board.entrySet().stream() .filter(entry -&gt; entry.getValue().getSide() == side) .filter(entry -&gt; entry.getValue().isPawn()) .collect(Collectors.groupingBy(entry -&gt; entry.getKey().getFileIndex(), Collectors.counting())); final double minusScore = countByFile.values().stream() .filter(count -&gt; count &gt;= OVER_COUNT) .mapToDouble(score -&gt; score / DIVIDE_VALUE) .sum(); final double finalScore = score - minusScore; stream문이 불필요하게 한바퀴 더 돌고는 있지만 확실히 stream문이 좀 더 간결하고 깔끔한거 같긴하다. 예전에 알고리즘 풀때 비용을 좀 더 중시했던 경향이 아직 남아있어 이번에도 for을 택하긴 했다. 하지만, 최근에 협력을 위한 코드를 좀 더 생각하고 있어서 앞으로는 stream을 택할거 같은데 이는 상황에 맞게 잘 사용하면 좋을 거 같다. for vs stream? 상속의 단점을 직접 경험 이전 단계 미션에서 상속보단 조합을 사용하자고 한 적이 있는데 그땐 이론적으로 공부한 것이라 직접 와닿지 않았었는데 이번에 체스 미션을 구현하면서 그 단점을 제대로 느껴본 것 같다. 이번 체스 미션에서는 Piece 기물을 상속받는 클래스가 무려 7개나 된다. 그래서 Piece를 수정하게 되면 아래 모든 클래스들을 수정해야 된다. 처음 설계에서 너무 많은 시간을 쏟게 되면 나중에 구현할 시간이 부족하게 되어 기한 내에 시간을 완료하지 못할 수 있기 때문에 적당한 설계를 하고 구현하면서 맞춰가자고 페어들과 항상 얘기했다. 그래서 코드를 구현하는 중간중간에 설계를 수정할 일이 잦았고, 또한 더 좋은 설계가 보이면 그걸로 수정할 일도 잦았다. 그렇게 수정하는 건 좋은데, 이게 Piece 부분을 수정할 때가 문제였다. Piece 부분을 수정했더니 오는 무수히 많은 빨간색들의 요청… Piece 부분들의 자식 클래스들이 문제였다. 7개가 관련해서 묶여있다 보니깐 전부 고치는데 상당히 많은 시간이 걸린다.(+테스트 코드도 고쳐야 됨..) 이 짓을 Piece와 관련된 수정을 할 때마다 해야 되니깐 이제 나중에는 Piece 부분에서 페어가 “어?”만 해도 ptsd가 왔다. “어” 금지.. 이렇게 직접 상속의 단점을 겪어보니깐 왜 “상속을 위한 설계와 문서가 없다면 상속하지 마라”라는지 직접 피부로 잘 느끼게 되었다. 앞으로는 특별한 이유가 없다면 상속보다는 조합을 좀 더 적극 사용할거 같다. 오브젝트 스터디 레벨 1 동안 오브젝트 스터디를 했었는데 현재 10장까지 읽고 토론했다.(방학 때 마무리했었어야 했는데ㅠㅠ) 책을 읽는 습관과 말주변이 없는 나에게는 매우 좋은 기회였다고 생각한다. 레벨 2 때 빨리 남은 5장을 마무리하고 관련해서 글을 정리해 보면 좋을 것 같고 레벨 2 때도 다시 책 한 권 정해서 스터디를 진행해 보려 한다. 💪 +조영호 님의 오브젝트가 지금까지 읽은 책 중 TOP3 안에 들 정도로 너무 맛있었기 때문에 강추. +리뷰어님께 받은 코드 리뷰에 대해 관심이 있으면 다음 PR들을 참고! 이번에는 체스 미션할 때 힘들었던 거 같아서 리뷰어님께 많이 못 여쭤본게 아쉽네요.. 1, 2단계 - 체스 3, 4단계 - 체스 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse chess pair-programming tdd",
    "url": "/woowacourse/2023-04-10-Chess/"
  },{
    "title": "Stream을 좀 더 잘 활용 해보자",
    "text": "Stream이 뭔지, 어떻게 사용하는지는 기본적으로 다들 알 것이다. 혹시 Stream에 대해 처음 들어봤다면 Stream API을 보고 오자. 하지만 이를 어떻게 더 잘 활용해 볼 수 있을지 한번 알아보자 peek 주로 디버깅을 지원하기 위해 존재하며, 특정 지점을 거치는 데이터들을 보기 위해 사용할 수 있다. 파라미터로 Consumer 함수형 인터페이스를 받는다. Stream.of(\"one\", \"two\", \"three\", \"four\") .filter(e -&gt; e.length() &gt; 3) .peek(e -&gt; System.out.println(\"Filtered value: \" + e)) .map(String::toUpperCase) .peek(e -&gt; System.out.println(\"Mapped value: \" + e)) .collect(Collectors.toList()); // Filtered value: three // Mapped value: THREE // Filtered value: four // Mapped value: FOUR Stream &lt;-&gt; 기본형 Stream Stream을 기본형 Stream으로 바꾸거나 기본형 Stream을 Stream으로 바꿔야 될 경우가 있다. 이를 위해, mapToInt(), mapToLong(), mapToDouble()을 지원하고 있으며 해당 메서드로 IntStream, LongStream, DoubleStream으로 변경할 수 있다. 역으로 mapToObj를 이용하여 Stream으로 다시 바꿀 수도 있다. // 기본형 Stream(IntStream) -&gt; Stream(Stream&lt;Integer&gt;) IntStream.range(1, 10) .mapToObj(v -&gt; v + 1) // Stream(Stream&lt;Integer&gt;) -&gt; 기본형 Stream(IntStream) Stream.of(1, 2, 3, 4) .mapToInt(v -&gt; v + 2) 그렇다면 언제 사용할 수 있을까..? 예를 들어, 점수의 합을 구한다고 가정해보자 int score = student.stream() .map(Student:getScore) .reduce(0, Integer::sum); 위와 같이 하게 되면 합계를 계산하기 전에 Integer를 기본형으로 언박싱해야 되기 때문에 추가 비용이 들게 된다. 그래서 효율적으로 처리할 수 있도록 기본형 특화 스트림(primitive stream specialization)이 제공되기 때문에 기본형 Stream으로 변경 후 계산할 수 있다. int score = student.stream() .mapToInt(Student:getScore) .sum(); Collect를 좀 더 활용해보자 Collectors.joining() Stream 값을 delimiter, prefix, suffix를 이용하여 간단하게 하나의 String 값으로 만들어 줄 수 있다. String join = List.of(\"a\", \"b\", \"c\").stream() .collect(Collectors.joining(\", \", \"(\", \")\")); //(a, b, c) Collectors.groupingBy() 제공한 Type T에 따라 요소를 그룹화하고 Map으로 결과를 반환한다. 파라미터로 Function 함수형 인터페이스를 받는다. List&lt;User&gt; users = List.of(new User(1, \"a\"), new User(2, \"b\"), new User(2, \"c\"), new User(1, \"d\")); Map&lt;Integer, List&lt;User&gt;&gt; collect = users.stream() .collect(Collectors.groupingBy(User::getAge)); //{1=[User{age=1, name='a'}, User{age=1, name='d'}], // 2=[User{age=2, name='b'}, User{age=2, name='c'}]} Collectors.partitioningBy() 입력받은 요소들을 Predicate에 따라 분류하고 Boolean을 키값으로 Map을 반환한다. 파라미터로 Prediate 함수형 인터페이스를 받는다. List&lt;User&gt; users = List.of(new User(1, \"a\"), new User(2, \"b\"), new User(2, \"c\"), new User(1, \"d\")); Map&lt;Boolean, List&lt;User&gt;&gt; collect = users.stream() .collect(Collectors.partitioningBy(user -&gt; user.getAge() == 1)); //{false=[User{age=2, name='b'}, User{age=2, name='c'}], // true=[User{age=1, name='a'}, User{age=1, name='d'}]} flatMap flatMap은 스트림의 요소에 일대다 변환을 적용한 다음 결과 요소를 새 스트림으로 평면화한다. 즉, 2중 배열 혹은 2중 리스트인 경우 구조를 허물고 이를 1차원으로 반환해 준다. 파라미터로 Function 함수형 인터페이스를 받고 있다. List&lt;List&lt;String&gt;&gt; lists = List.of(List.of(\"a, b, c\"), List.of(\"d, e, f\")); List&lt;String&gt; flat = lists.stream() .flatMap(Collection::stream) .collect(Collectors.toList()); //[a, b, c, d, e, f] reduce 해당 스트림의 요소들을 연관 누적 함수(reduce 괄호 안 BinaryOperator)를 이용하여 연산하고 Optional 값을 반환하게 된다. BinaryOperator는 BiFunction을 상속받았기 때문에 2개의 T 타입 파라미터를 넘기고 T 타입을 반환하는 함수이다. List&lt;Integer&gt; list = List.of(1, 2, 3, 4); Optional&lt;Integer&gt; reduce = list.stream() .reduce(Integer::sum); System.out.println(reduce.get()); //10 (1 + 2 + 3 + 4) BinaryOperator 앞에 계산을 처리하기 위한 초깃값을 설정해 줄 수 있다. List&lt;Integer&gt; list = List.of(1, 2, 3, 4); Integer reduce = list.stream() .reduce(100, Integer::sum); System.out.println(reduce); //110 (100(초기값) + 1 + 2 + 3 + 4) 실행 순서 과연 Stream API의 연산들의 실행 순서도 성능과 관련이 있을까? 관련이 있다. 어떻게 동작되는지 모른 채로 사용하게 되면 비효율적으로 사용할 수 있다. 실행 순서에 따라 어떻게 되는지 한번 살펴보자 우선 Stream이 어떤 구조로 돌아가는지 한번 확인해 보자. 과연 아래와 같이 코드를 작성하면 어떻게 출력이 될까? 1번? 2번? List&lt;Integer&gt; list = List.of(1, 2, 3, 4); list.stream() .filter(v -&gt; { System.out.println(\"first: \" + v); return v &gt;= 1; }) .forEach(v -&gt; System.out.println(\"second: \" + v)); //1번 //first:1 //first:2 //first:3 //.. //2번 //first:1 //second:1 //first:2 //second:2 //.. //정답: 2번 first: 1 second: 1 first: 2 second: 2 first: 3 second: 3 first: 4 second: 4 정답은 2번이다. 각 연산을 한 번에 다하고 그다음으로 넘어가는 게 아니라 하나씩 수직적으로 돌게 된다. 왜 이런 구조로 돌아가는 걸까? 결론부터 말하자면 수직적으로 실행되는 것이 더 효율적이기 때문이다. List&lt;Integer&gt; list = List.of(1, 2, 3, 4); list.stream() .filter(v -&gt; { System.out.println(\"first: \" + v); return v &gt;= 1; }) .anyMatch(v -&gt; { System.out.println(\"second \" + v); return v == 1; }); //실행 결과 //first: 1 //second 1 위 코드가 만약 수평적으로 돌아가게 된다면 filter에서 4번, anyMatch에서 1번으로 총 5번의 연산이 발생할 것이다. 하지만 수직적으로 돌아가기 때문에 실제로는 filter 1번, anyMatch 1번으로 총 2번의 연산이 발생한다. 그렇다면 실행 순서에 따른 연산 개선을 한번 해보고 Stream을 여기서 마무리해 보자. (여기선 간단하게 예로 filter 두 개를 써서 약간 어색할 수 있지만 넘어가 주세요… ㅋㅋㅋ) List&lt;Integer&gt; list = List.of(1, 2, 3, 4, 5, 6); list.stream() .filter(v -&gt; { System.out.println(\"first: \" + v); return v &gt;= 1; }) .filter(v -&gt; { System.out.println(\"second: \" + v); return v == 4; }) .forEach(v -&gt; System.out.println(\"third: \" + v)); //실행 결과 // first: 1 // second: 1 // first: 2 // second: 2 // first: 3 // second: 3 // first: 4 // second: 4 // third: 4 // first: 5 // second: 5 // first: 6 // second: 6 위와 같이 v &gt;=1 필터가 먼저 온 경우에는 총 연산이 13번 이루어졌다. 하지만 다음과 같이 v == 4 필터가 먼저 오면 어떻게 될까? List&lt;Integer&gt; list = List.of(1, 2, 3, 4, 5, 6); list.stream() .filter(v -&gt; { System.out.println(\"first: \" + v); return v == 4; }) .filter(v -&gt; { System.out.println(\"second: \" + v); return v &gt;= 1; }) .forEach(v -&gt; System.out.println(\"third: \" + v)); //실행 결과 // first: 1 // first: 2 // first: 3 // first: 4 // second: 4 // third: 4 // first: 5 // first: 6 두 번째 연산을 앞으로 옮김으로써 총 연산이 8번 이루어졌다. 물론 현재는 5번밖에 차이가 안 나지만, 나중에 대량의 데이터를 연산할 때는 그 차이가 어마어마할 것이다. 이렇게 실행 순서에 따라 같은 입력과 결과에 대해 더 적게 연산을 처리할 수 있으므로 Stream을 사용할 때는 주의해서 사용하자. 참고: https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java stream etc",
    "url": "/etc/java/2023-03-26-Advanced-Stream/"
  },{
    "title": "우테코 - 블랙잭 미션 회고",
    "text": "이번 세 번째 미션은 블랙잭 미션! 세 번째 미션부터 어렵다고 소문을 들었기 때문에 시작하기 전에 겁을 먹고 있었다(사실 설렜을지도..?). 카드에 어떤 카드가 있는지도 정확하게 몰랐고 심지어 미션을 구현하기 전까지 블랙잭의 룰도 몰랐다. 하지만 역시 인간은 적응의 동물? 하기 전까지는 겁을 먹고 있었지만 막상 다가오니 어떻게든 했던 것 같다 ㅋㅋ 하지만 시간 분배도 잘못했던 거 같고 아직 tdd에 익숙하지 않아 오래 걸렸기 때문에 위험할 뻔했던 기억이 난다. 이번 회고를 통해 블랙잭 미션을 다시 되돌아보자 Over README 처음 페어랑 만난 날 거의 모든 시간을 기능 목록 작성에만 쏟았던 기억이 난다.. 그렇게 다음과 같은 README가 만들어졌지만 다시 해보라 하면 못할 거 같다.ㅋㅋㅋ 지금 생각해 보면 너무 상세하게 적어서 시간이 많이 소모되었던 거 같은데 어차피 세부 구현 사항은 추후에 바뀌므로 그렇게 공들일 필요가 없었을 거 같다. 다음에는 간단히 도메인 다이어그램 정도 그리고 도메인별 기능 정도만 작성해 주면 좋을 거 같다! 다이어그램 작성을 도와주는 mermaid라는 툴을 알게 되었는데 매우 유용해 보여 다음 체스 미션에 적용해 볼 예정이다. 카드 상속 이번에 우리는 각 카드를 구현할 때 abstract class인 Card를 생성하고 이 Card를 상속받는 AceCard, CourtCard, StandardCard를 생성했다. 앞으로 각자의 카드마다 추가적인 기능이 추가될 경우를 위해 이렇게 구조를 잡았는데 생각해 보니 지금 구조에서는 카드마다 특별한 행위는 없기 때문에 분리할 필요가 없어 보인다. 이에 리뷰어님께서도 지금 구조에서는 딱히 분리할 필요가 없어 보이고 필요한 시점에 분리해 보는 건 어떨까라는 답변을 주셨다. 그래서 상속에 대해 좀 더 깊게 알아보고 싶었고 다음과 같이 상속과 조합에 대해 정리해 보았다. 많은 사람들이 중복을 제거하기 위한 관점으로 상속을 사용하곤 한다. 물론 나도 그랬고, 하지만 중복을 제거하기 위한 방법이 상속만 가능하냐? 아니다. 조합을 이용해서도 가능하다. 중복을 제거하기 위해 사용한다는 관점보다는 각 객체들 간의 관계에 대해 생각해 보면 좋을 것 같다는 리뷰어님의 말씀을 듣고 둘에 대한 차이가 궁금해졌다. Efftective Java item 18에서 “상속보다는 조합을 사용해라”라는 말이 나온다. 왜 그럴까? 상속을 적절히 사용하면 그 장점들은 강력하지만 잘못 사용하면 두 가지 관점에서 설계에 안 좋은 영향을 미친다. 첫 번째는 캡슐화를 위반하여 하위 클래스가 상위 클래스에 강하게 결합하게 되고 변화에 유연하게 대처하기 어려워진다. public class Moomin { protected List&lt;Integer&gt; integers; ... } public class Momin extends Moomin{ publi Momin(List&lt;Integer&gt; integers) { super(integers); } ... } 예를 들어 무민 클래스의 인스턴수 변수가 List&lt; Integer &gt;에서 int[]로 변경되면 무민을 상속받고 있는 하위 클래스들은 모두 수정을 해야 된다. 두 번째로는 설계가 유연하지 않다. 상속은 부모 클래스와 자식 클래스 사이의 관계를 컴파일 시점에 결정하기 때문에 실행 시점에 객체의 종류를 변경하는 것이 불가능하다. 그렇다면 조합을 사용하게 되면 어떻게 될까? 조합은 기존의 클래스가 새로운 클래스의 구성요소(인스턴스)로 쓰이는 걸 말한다. 새로운 클래스는 기존 클래스의 메서드를 호출하여 사용할 수 있다. public class Momin { private Moomin moomin; ... } 위와 같이 사용하게 되면 메서드를 호출하여 동작하기 때문에 캡슐화를 깨뜨리지 않는다. 그리고 기존 클래스가 변화되더라도 그 영향은 적어 비교적 안전하다. 조합 방식 같은 경우 Moomin의 인스턴스 변수 타입이 변경되더라도 메서드로 호출하기 때문에 영향을 받지 않게 된다. 그렇기 때문에 웬만하면 유연하지 않은 상속보단 조합을 사용하도록 하자. 그러면 상속은 언제 사용해야 될까? 상속이 적절하게 사용되려면 다음과 같은 조건을 만족해야 한다. 부모와 자식 클래스가 is-a 관계인 경우 자식 클래스는 부모 클래스라고 할 수 있을 때 ex) cat은 Animal이다. 행동 호환성이 만족하는 경우 행동 호환성: 클라이언트 입장에서 부모 클래스와 자식 클래스의 차이를 몰라야 된다. 부모 클래스를 새로운 자식 클래스로 대체하더라도 시스템이 문제없이 동작할 것이라는 것을 보장 //펭귄과 새의 관계가 있을 때 일반적으로 새는 날 수 있지만 펭귄은 날 수 없다. //하지만 다음과 같이 구현하게 되면 행동 호환성에 오류가 발생하게 된다. //(클라이언트는 모든 새는 날 수 있다고 알지만 사실 펭귄은 못 날기 때문) public class Bird { publi void fly(){ ... } } //따라서 다음과 같이 클라이언트의 예상에 맞게 분리하게 되면 만족할 수 있다. public class Bird { } public classs FlyginBird extends Bird { publi void fly() { ... } } public class Penguin extends Bird { } 하지만 저 조건들을 만족하더라도 캡슐화를 깨뜨리기 때문에 상황에 맞게 쓰자! 그리고 is-a와 has-a 차이점 글도 한번 읽어보면 좋을 것 같다. is-a: A는 B이다, 한 클래스가 다른 클래스의 서브 클래스 밀접하게 결합되므로 클래스 계층구조에서 좀 더 안정적인 기반을 마련 has-a: ~에 속한다(belong), 한 오브젝트가 다른 오브젝트에 속한다. 느슨하게 결합되므로 변경이 발생하더라도 구성 요소를 쉽게 변경할 수 있다(유연성 제공) 캐싱 캐싱이라는 걸 알고 있었지만 지금까지 적용해 본 적은 없었는데 이번 블랙잭 미션에서 적용해 보게 되었다. 이번 블랙잭에서는 게임을 플레이하기 위한 Deck(카드 52장)이 존재한다. 지금 구조에서는 Deck을 생성할 때마다 매번 52장의 카드들이 생성되고 있기 때문에 미리 생성 해놓고 재사용함으로 써 자원 낭비를 줄여볼 수 있다! public class Deck { private static final List&lt;Card&gt; CACHE; static { final Pattern[] values = Pattern.values(); List&lt;Card&gt; cards = new ArrayList&lt;&gt;(); for (Pattern pattern : values) { addAllCardByPattern(cards, pattern); } CACHE = cards; } } 좋은 설계를 위한 책임, 역할, 협력 최근에 미션들을 진행하다 각 객체의 적절한 역할들에 대해 고민해 볼 생각이 많았고 그때 읽고 있던 오브젝트에 관련 좋은 내용이 있기에 혼자 보기 아까워 한번 정리해봤다. 오브젝트 이 놈… 진짜 맛있다. 꼭 읽어보길 강추! 좋은 설계를 위한 책임, 역할, 협력 그 외 피드백 테스트 코드 더 명확히 하기 @Test void testDealerBurstResult() { //given Participant dealer = new Dealer(); dealer.hit(createStandCard(Pattern.CLUB, \"10\")); dealer.hit(createStandCard(Pattern.CLUB, \"4\")); dealer.hit(createStandCard(Pattern.CLUB, \"9\")); Players players = createPlayers(); //when List&lt;Result&gt; result = referee.judgeResult(dealer, players); //then Assertions.assertThat(result) .isEqualTo(List.of(Result.WIN, Result.WIN, Result.WIN, Result.LOSE)); } 현재 위의 테스트는 딜러가 버스트가 된 경우 Result가 어떻게 되는지 테스트해 보려고 하는데 각 플레이어의 상태가 어떤지 드러나지 않기 때문에 무엇을 테스트하려는지 애매모호하다. 다음과 같이 given 부분을 좀 더 명확하게 바꿔주어 가독성을 높여줄 수 있다. 마지막 줄 개행 다음과 같은 피드백이 왔다. 마지막 줄에 개행이 없으면 No newline at end of life라는 경고문이 뜬다. 이것은 POSIX의 명세로 줄 바꿈이 하나의 행을 정의하여 파일 끝에 newline 문자가 없으면 끝나지 않은 행으로 여긴다. 특히, 마지막에 개행이 없다면 파일 간 차이를 알기 어렵고 줄 바꿈이 없으면 파일을 올바르게 처리하지 못하는 프로그램도 있다. 그래서 github에서 사전에 방지하기 위해 경고를 띄워준다! 비슷한 메서드도 모두 테스트? 애송이: 클래스가 분리되어 있다 보니깐 player가 bet을 하는 과정에서 Player -&gt; Players -&gt; BlackjackGame -&gt; BlackjackController로 계속해서 bet을 호출하는 과정이 일어나는데 여기서 모든 bet 메소드를 테스트 할 필요가 있는가 고민이 듭니다. 애송이: 이렇게 호출만 하는과정에서는 오히려 테스트를 모두 하게되면 제일 아래 하나를 고치면 모두 고쳐야되는 비용이 추가 되기 때문에 하지 않아도 된다고 생각하는데 또링은 어떻게 생각하는지 궁금합니다!! 리뷰어: 좋은 고민이네요. 이번 미션은 아래와 같은 요구사항이 있었는데요, 요구사항을 지키기 위해서는 모두 작성했어야 하지 않을까요~? 모든 기능을 TDD로 구현해 단위 테스트가 존재해야 한다. 단, UI(System.out, System.in) 로직은 제외 리뷰어: TDD외 관점에서도 궁금하실 것 같아 추가로 의견 남겨드리자면, 저는 모두 테스트하는게 맞다고 생각해요. 제일 아래 하나를 고치면 모두 고쳐야되는 비용이 추가 된다고 하셨는데요, 아무래도 테스트의 목적이 프로그램이 잘 돌아가는지 검증하는 것이기 때문에, 하나의 메서드가 변경되면 관련 로직에서도 테스트를 변경해주어야 하는것이 당연하다고 생각해요. (메서드의 구현 방법이 바껴서 테스트가 깨지는 것과는 다른 이야기입니다! 요런 경우는 생기지 않도록 최대한 지양해야겠죠.) 디미터 법칙은 어디까지..? public List&lt;Integer&gt; getAmounts() { return players.stream() .map(player -&gt; player.getAmount().getValue()) .collect(Collectors.toList()); } 애송이: 코드를 짜다보니깐 계속해서 디미터 법칙이 거슬리게 되는거 같은데.. 디미터 법칙을 위반하게 되면 캡슐화를 위반하게 되고 객체를 객체스럽게 사용하지 못하기 때문에 좋지 않은것은 납득이 되었습니다! 애송이: 하지만 실제 사용이 필요한 출력부분이나 또는 이번에 수익을 계산하기 위해 배팅금액을 사용하기 위해선 내부까지 접근을 해야되기 때문에 위반하게 되었는데 이러한 때는 예외적으로 허용할 수 있는 부분일까요..? 뭔가 기준이 정립되지 않고 계속 혼돈이 와 또링의 의견이 궁금합니다! 리뷰어: 예외상황에 대한 기준은 무민이 미션을 수행해나가면서 세워보시면 좋겠습니다. 다만, 개인적으로는 이런 경우도 Players는 Player의 Amount가 Value로 구성되어있다는 사실은 몰라도 되기 때문에 getAmountValue()정도로 감쌀 수 있겠네요. (+ 사실 저는 List 를 넘겨줄 것 같긴 합니다. 금액에 대해 원시값으로 돌아다니는 것은 금액을 Amount라는 객체로 감싼 이점을 포기하는 것과 같아서요.) +리뷰어님께 받은 코드 리뷰에 대해 관심이 있으면 다음 PR들을 참고! 1단계 - 블랙잭 게임 실행 2단계 - 블랙잭(베팅) *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse blackjack pair-programming tdd",
    "url": "/woowacourse/2023-03-18-Blackjack/"
  },{
    "title": "좋은 설계를 위한 역할, 책임, 협력",
    "text": "“아 설계를 좀 더 제대로 할걸…” 어디선가 많이 들어본 말이다. 구현하기 전 설계가 중요하다는 말을 많이 들어봤을 것이다. 하지만 어떻게 좋은 설계를 할 수 있을까..? 이번에 역할, 책임, 협력을 통해 한번 알아보자. 애플리케이션의 기능을 구현하기 위해 어떤 협력이 필요하고 그 협력을 위해 어떤 책임과, 역할이 필요한지 충분히 생각하지 않고 바로 구현에 들어가 버리면 변경하기 어렵고 유연하지 못한 구조의 코드를 만들 수 있다. 실제로 우테코 미션을 진행할 때 제대로 설계를 해놓지 않고 들어가 나중에 엄청난 대공사를 거쳤던 경험을 한 적이 있었다… 작은 규모의 미션을 할 때도 이 정도였는데 엄청 큰 서비스였다면 매우 끔찍했을 것이다. 우선 협력과 책임, 역할의 의미는 다음과 같다. 협력: 객체들이 애플리케이션의 기능을 구현하기 위해 수행하는 상호작용 책임: 객체가 협력에 참여하기 위해 수행하는 로직 역할: 객체의 협력안에서 수행하는 책임들이 모여 객체가 수행하는 역할 협력(Collaboration) 객체는 독립적인 존재가 아니라 애플리케이션이라는 큰 목표를 만들기 위해 다른 객체와 협력하는 사회적인 존재이다. 두 객체 사이의 협력은 하나의 객체가 다른 객체에게 메시지 전송을 통해 자신의 요청을 전송한다. 그리고 메시지를 수신한 객체는 메서드를 실행해 요청에 응답한다. 외부의 객체는 오직 메시지만 전송할 수 있으며 메시지를 어떻게 처리할지는 수신한 객체가 직접 결정한다. 이것은 객체가 능동적으로 일을 처리할 수 있는 자율적 존재라는 걸 알 수 있다. 자율적인 객체는 자신에게 할당된 책임을 수행하던 중 외부의 도움이 필요한 경우 적절한 객체에게 메시지를 전송해 협력을 요청한다. 예매 요금을 계산하기 위한 Screening(상영)과 Movie(영화)의 협력 관계를 보면 Screening은 Movie에 calculateMovie 메시지를 전송해 요금 계산을 요청할 수 있다. 협력이 설계를 위한 문맥(Context)을 결정 객체가 가질 수 있는 상태와 행동을 어떻게 결정할 수 있을까? 지금까지 객체를 설계할 때 어떤 상태와 행동을 할당했다면 왜 그렇게 했는지 생각해 보자. 객체의 행동을 결정하는 것은 객체가 참여하고 있는 협력이다. 협력이 바뀌면 객체가 제공해야 하는 행동도 바뀌어야 된다. 객체의 행동을 결정하는 것이 협력이라면 상태를 결정하는 것은 행동이다. 객체의 상태는 그 객체가 행동을 수행하는 데 필요한 정보이다. 객체는 자신의 상태를 스스로 결정하고 결정하는 자율적인 존재이기 때문에 객체가 수행하는 행동에 필요한 상태도 함께 가지고 있어야 한다. public class Movie { private Money fee; private DiscountPolicy discountPolicy; public Money calculateMovieFee(Screening screening) { return fee.minus(discountPolicy.calculateDiscountAmount(screening)); } } 기본요금인 fee와 할인 정책인 discountPolicy를 상태로 포함하는 이유는 요금 계산이라는 행동을 수행하는 데 이 정보들이 필요하기 때문! 그래서 결국 협력에 따라 행동과 상태가 모두 결정되므로 협력은 객체를 설계하는 문맥을 제공한다 할 수 있다. 책임(Responsibility) 객체를 설계하기 위해 필요한 문맥인 협력이 갖춰진 후 다음으로는 협력에 필요한 행동을 수행할 수 있는 적절한 객체를 찾는 것이다. 책임은 객체에 의해 정의되는 응집도 있는 행위의 집합으로, ‘무엇을 알고 있는가’와 ‘무엇을 할 수 있는가’로 구성된다. 하는 것(doing) 객체를 생성하거나 계산을 수행하는 등의 스스로 하는 것 다른 객체의 행동을 시작시키는 것 다른 객체의 활동을 제어하고 조절하는 것 아는 것(knowing) 사적인 정보에 관해 아는 것 관련된 객체에 관해 아는 것 자신이 유도하거나 계산할 수 있는 것에 관해 아는 것 예를 들어 Screening과 Movie의 책임은 무엇일까? Screening의 책임은 영화를 예매하는 것이고 Movie는 예매 가격을 계산할 책임을 진다. 이것은 하는 것과 관련된 책임이다. 그리고 Screening은 자신이 상영할 영화를 알고 있어야 되고 Movie는 가격과 어떤 할인 정책이 적용됐는지도 알고 있어야 한다. 이것은 아는 것과 관련된 책임이라 할 수 있다. 객체지향 설계에서 가장 중요한 것은 책임으로 객체에게 얼마나 적절한 책임을 할당하느냐가 설계의 전체적인 품질을 결정한다. 책임 할당 과정 영화 예매하는 기능의 책임을 할당하는 과정을 살펴보자. 객체가 책임을 수행하게 하는 방법은 메시지를 전송하는 것이므로 예매하라라는 이름의 메시지로 협력을 시작할 수 있겠다. 이제 메시지를 처리할 적절한 객체를 선택해야 되는데 영화 예매와 관련된 정보를 가장 많이 알고 있는 객체에게 책임을 할당하는 것이 좋다. 영화를 예매하기 위해서는 상영 시간과 기본요금을 알아야 하기 때문에 이 정보를 소유하고 있거나 해당 정보의 소유자를 가장 잘 알고 있는 객체는 Screening이라 할 수 있다. 하지만 영화를 예매하기 위해서는 예매 가격을 알아야 하는데 Screening은 가격을 계산하는데 필요한 정보를 충분히 알고 있지 않기 때문에 외부 객체의 도움이 필요하다. 그래서 새로운 메시지가 필요하다. 이제 가격을 계산하라라는 새로운 메시지를 처리할 적절한 객체를 선택해야 되는데 가격을 계산하기 위해서는 가격과 할인 정책이 필요하다. 이것을 가장 잘 알고 있는 객체는 Movie이므로 Movie에게 책임을 할당하자. 여기서 끝이 아니다.. 가격을 계산하기 위해서는 할인 요금이 필요하지만 Movie는 할인 요금을 혼자 계산할 수 없기 때문에 또한 외부에 도움을 요청해야 된다. 즉, 할인 요금을 계산하라라는 새로운 메시지가 또 필요하다. 이렇게 객체지향 설계는 메시지를 찾고, 적절한 객체를 선택하는 반복적인 과정이 필요하다. 모든 책임 할당 과정이 이렇게 단순하게 되지는 않는다. 응집도와 결합도 관점에서 정보 전문가가 아닌 다른 객체에게 책임을 할당하는 것이 더 좋을 때도 있다. 하지만 기본적으로는 정보 전문가에게 책임을 할당함으로써 자율적인 객체를 만들 가능성이 높다. 위와 같은 방법으로 설계하는 방법을 책임 주도 설계(Responsibility-Driven Design, RDD)라고 부른다. 구현이 아닌 책임에 집중하는 것으로 유연하고 견고한 객체지향 시스템을 만들 수 있다. 이제 책임을 할당할 때 고려해야 하는 두 가지 요소를 알아보자. 1. 메시지가 객체를 결정 앞의 과정을 생각해 보면 필요한 메시지를 먼저 식별하고 메시지를 처리할 객체를 나중에 선택했다. 다시 말해 메시지가 객체를 선택했다고 할 수 있는데 그렇게 함으로써 다음과 같은 장점이 있다. 객체가 최소한의 인터페이스를 가질 수 있게 된다. 필요한 메시지가 식별될 때까지 객체의 퍼블릭 인터페이스에 어떤 것도 추가하지 않기 때문에 최소한의 퍼블릭 인터페이스를 가질 수 있다. 객체는 충분히 추상적인 인터페이스를 가질 수 있게 된다. 객체의 인터페이스는 무엇(what)을 하는지는 표현해야 하지만 어떻게(how) 수행하는지를 노출해서는 안된다. 메시지는 외부의 객체가 요청하는 무언가를 의미하기 때문에 메시지를 먼저 식별하면 무엇을 수행할지에 초점을 맞추는 인터페이스를 얻을 수 있다. 2. 행동이 상태를 결정 객체의 행동은 객체가 협력에 참여할 수 있는 유일한 방법으로 객체가 협력에 적합한지를 결정하는 것은 그 객체의 상태가 아니라 행동이다. 얼마나 적절한 객체를 만들었느냐는 얼마나 적절한 책임을 할당했는냐에 달려있고, 책임이 얼마나 적절한지는 협력에 얼마나 적절한지에 달려있다고 볼 수 있다. 초보자들이 가장 쉽게 하는 실수가 객체의 행동이 아니라 상태에 초점을 맞춰 객체에 필요한 상태가 무엇인지 결정하고, 그 후에 상태에 필요한 행동을 결정한다. 이런 방식은 객체의 내부 구현이 객체의 퍼블릭 인터페이스에 노출되도록 만들기 때문에 캡슐화를 저해한다. 이렇게 되면 객체의 내부 구현을 변경하면 퍼블릭 인터페이스도 변경되고, 결국 객체에 의존하는 클라이언트로 변경이 전파된다. 역할(Role) 객체가 어떤 특정한 협력 안에서 수행하는 책임의 집합을 역할이라고 하는데 실제로 협력을 모델링 할 때는 특정한 객체가 아니라 역할에게 책임을 할당한다고 생각하는 게 좋다. 예를 들어, 앞에서 예매하라라는 메시지를 처리하기 위해 Screening을 선택했는데 이 과정에서는 사실 두 가지 단계가 합쳐진 것이다. 첫 번째는 영화를 예매할 수 있는 적절한 역할이 무엇인가를 찾고, 그 뒤에 두 번째로 역할을 수행할 객체로 Screening 인스턴스를 선택하는 것이다. 그리고 가격을 계산하라라는 메시지를 처리할 때도 역할에 관해 먼저 고민하고 역할을 수행할 객체로 Movie를 선택한 것이다. 위와 같이 설계하는 이유는 뭘까? 괜히 역할이라는 개념을 만들어 더 번거롭게 하는 건 아닐까..? 유연하고 재사용 가능한 협력 역할이 중요한 이유는 역할을 통해 유연하고 재사용 가능한 협력을 이용할 수 있다. 만약 역할이라는 개념을 제외하고 객체에게 책임을 할당한다면 어떻게 될지 한번 살펴보자. Movie가 가격을 계산하기 위해서는 할인 요금이 필요한데 혼자 할 수 없기 때문에 외부의 객체에 도움을 요청해야 된다. 하지만 현재 할인 정책에는 금액 할인 정책과 비율 할인 정책이라는 두 가지 종류의 할인 정책이 존재한다. 그래서 두 가지 종류의 객체가 할인 요금을 계산하라라는 메시지에 응답할 수 있어야 되기 때문에 협력을 개별적으로 생성해 중복적인 코드가 발생하게 된다. 이러한 문제를 해결하기 위해서는 객체가 아닌 책임에 초점을 맞춰서 생각해야 한다. AmountDiscountPolicy, PercentDiscountPolicy 모두 할인 요금 계산이라는 동일한 책임을 수행하기 때문에, 객체라는 존재를 지우고 할인 요금을 계산하라라는 메시지에 응답할 수 있는 대표자를 만들어 놓고 두 할인 정책을 갈아끼우면 두 협력을 하나로 통합할 수 있다. 여기서 역할이 두 종류의 구체적인 객체를 포괄하는 추상화라는 점을 보자. 그래서 구체적인 객체들을 포괄할 수 있는 추상적인 이름 DiscountPolicy를 부여했다. 여기서 가격을 할인하지 않는 NonDiscountPolicy를 추가하더라도 새로운 협력을 생성하지 않고 그냥 위에서 갈아끼우기만 하면 된다. 이제 왜 역할이 책임들의 집합인지 알겠는가? 이렇게 책임과 역할을 중심으로 협력을 바라보면 변경과 확장이 용이한 설계로 나아갈 수 있다. 이론과 실전 대부분의 경우 어떤 것이 역할이고 어떤 것이 객체인지 또렷하게 드러나지 않아 명확한 기준을 세우기 어려울 것이다. 그래서 설계를 처음 할 때는 역할과 객체를 구분하는 것은 신경 쓰지 말고 적절한 책임과 협력을 탐색하는 것에 더 집중하자. 애매하다면 단순하게 객체로 시작해 반복적으로 책임과 협력을 정제해가면서 필요한 순간에 객체로부터 역할을 분리해 내면 된다. 만약 다양한 객체들이 협력에 참여한다는 것이 확실하다면 역할로 시작하면 되겠지만 모든 것이 확실하지 않아 결정하기 어려운 상황이라면 구체적인 객체로 시작하자. 다양한 시나리오를 탐색하고 유사한 협력을 단순화하고 합치다 보면 자연스럽게 역할이 드러날 것이다. 참고:오브젝트 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse OOP role responsibility collaboration object",
    "url": "/woowacourse/2023-03-12-RRC/"
  },{
    "title": "우테코 - 사다리 타기 미션 회고",
    "text": "두 번째 미션인 사다리 타기가 시작되었다~ 이번 미션을 통해 어떤 점을 느꼈고 어떤 걸 배운지 회고 해보자! 사다리 타기 미션 시작 브라운 당첨(?) 축하합니다… TDD 이번 미션부터는 TDD를 적용하며 페어와 함께 미션을 진행해야 한다. TDD를 직접 적용하는 건 처음이라 많이 서툴렀던 것 같다. 코드를 작성하다가 “아 맞다 TDD..” 만 몇 번 외쳤는지 ㅋㅋㅋ 아직 TDD에 완벽하게 적응한 건 아니지만 확실히 직접 경험해 보니깐 어떤 느낌인지는 알 것 같다. 직접 하면서 느낀 장점으로는 먼저 테스트 코드를 기반으로 프로덕션 코드를 작성하기 때문에 절대 테스트 코드를 작성하지 못할 일은 없을 거 같고 빠른 피드백을 기반으로 개발할 수 있다는 게 매우 큰 장점으로 느껴졌던 것 같다. TDD를 하다보니 다음과 같은 의문점이 들었다. fail -&gt; success -&gt; refactor 단계로 사이클을 돌아가면서 개발을 진행하는데 이 refactor 단계에서 어느정도까지 refactor를 해야 적당한지? 이 내용은 테코브러리에도 있는 켄트백의 TDD 책에도 나오는데, 리팩토링에 대한 부분은 TDD에 대한 익숙함과 도메인 지식과 관련되어있습니다. TDD에 익숙하거나 사다리게임에 대한 도메인 지식이 풍부한 상태라면 리팩토링 단계에서 생략할 수 있는 부분이 많아질 거에요. 이 부분은 생략하는것도 좋은 방법입니다. 사소한 변경사항이 생기셨을때도 fail을 보는 단계부터 시작하셨나요? 아마 무민도 은연중에 많은 부분을 건너뛰셨을거에요 😂 테스트코드와 기능 구현을 번갈아 가면서 진행하기 때문에 깃커밋 메시지를 test, feat 으로 번갈아가며 썼는데 이것두 맞는지 아니면 feat 한개로도 충분한지? test, feat으로 구분해주셔서 보는 입장에선 보기 편했고 TDD로 잘 작성해주셨다는것도 알 수 있었습니다! 커밋 메시지는 오답만 있지 정답은 없는 부분이라서 나만의 생각을 정해 나가는것도 좋을 것 같아요. 무민은 번갈아가면서 쓰시는게 어떠셨나요? 역시 자기만의 기준이 제일 중요한 거 같다. 나의 기준이 잘 세워질 수 있도록 계속해서 적용해 보자! step2(사다리 게임 실행) 시작 이번 미션에서 목표가 피드백을 최대한 많이 받는거라 이번 step2에서 빨리 내고 피드백 cycle을 최대한 많이 돌리려고 생각했다. 근데 지금 생각해보니깐 빨리만 짜고 이상하게(머리에 과부하가 와서 사실 뭘 더 고쳐서 내야 될지 못 찾음..) 낸거 같아 고생하신 리뷰어님께 죄송한 마음이 든다. 하지만 덕분에 엄청나게 많은 피드백으로 앞으로 코드를 구현할 때 어떻게 짜야될지 무엇을 고민해봐야 될지 많은 깨달음을 얻은 정말 귀중한 시간이였다! 감사합니다 터틀🐢.. 각 객체와 Controller 역할 망각 리뷰어님께 온 피드백 중 상당 수가 다음과 같았다. 컨트롤러가 Model과 View의 상호작용 뿐만 아니라 게임 진행까지 진행하고 있는것같아요. 무민이 의도한 Controller의 역할은 무엇인가요? 플레이어 이름을 입력받은 다음 객체를 만들었다면 이후엔 객체간 협력을 통해 로직이 진행하도록 하면 어떨까요? 만든 객체가 아니라 입력받은 names를 통해 진행하는 로직이 많아보여요 Position 클래스를 만들었다면 값을 꺼내서 계산하는 것이 아니라 Position 클래스와 상호작용을 통해 계산해보는건 어떨까요? 플레이어의 이름이 all과 같지 않아야 한다는 비즈니스 로직이 아닐까요? 적절한 책임을 갖는 객체로 이동시켜보면 어떨까요? 위 리뷰에서 적절한 객체로 이동한다면 이 부분도 Controller가 아니라 해당 객체에서 처리할 수 있지 않을까요? 진짜 부끄럽지만 사실대로 말하면 지금까지 코드를 짜는 동안 각 객체의 역할이나 Controller의 역할에 대해 제대로 생각해 보지 않고 짠 거 같다. 그냥 기능만 잘 돌아가게 만들고 그 위치는 아무 곳이나 넣었다 해도 과장이 아닌 것 같다.. 이 사실을 막상 마주치니깐 약간 망치로 머리를 맞은 것 같았다. 그동안 나.. 뭐하고 있었지? 맨날 객체지향, 객체지향하고 있었는데 사실 겉멋 객체지향이었던 건가..? 사실 얼마 전 객체지향의 사실과 오해에서도 이 부분과 관련해서 읽었었는데 그래도 깨닫지 못했었고 이번 피드백과 많은 다른 사람들의 코드 리뷰나 블로그 회고 등을 통해 느끼게 되었다. 앞으로 각 객체에 어떠한 적절한 책임이 주어져야 될지 고민해 보고 이들 간의 협력을 잘 이용해 보려 한다. 메서드 추출시 static 인텔리 제이 단축키를 이용해 메서드 추출을 할 때 static과 관련된 코드가 포함되어 있으면 그 추출한 메서드에 static이 붙게 되는데 이걸 까먹고 지우지 않아 종종 지적받았던 것 같다. 흠.. 추출할 때 default로 static이 붙지 않게 해주는 설정을 찾아보았지만 아직 못 찾았다ㅠ 혹시 아는 분 있으면 댓글로 좀…! 🙇 메서드 위치 코드 작성시에 메서드 위치 관련해서도 얘기가 나왔는데 다른사람이 코드를 봤을 때에도 빠르게 파악하기 위해 다음과 같은 순서로 진행되면 좋을 것 같다! public 주석 클래스 정적 변수 : public -&gt; protected -&gt; private 인스턴스 변수 : public -&gt; protected -&gt; private 생성자 정적 메소드 : static 메소드 (main 메소드가 있다면 static 메소드 이전에 작성) 메소드 : 접근자 기준으로 작성하지 않고, 기능 및 역할별로 분류하여 기능을 구현하는 그룹별로 작성이 이루어지도록 해야한다. (public 사이에 private 메소드가 존재할 수 있다) 스탠다드 메소드 : toString, equals, hashcode 와 같은 메소드 getter, setter 메소드 : 클래스의 밑 부분에 위치 코드 예시는 다음 링크를 참고 package-private 테스트 코드에서 hasMessage를 검증할 때 다음과 같이 적용하고 있었는데 리뷰어님이 객체의 상수를 package-private로 변경해 참조하면 어떨까 추천해 주셨다. assertThatThrownBy(() -&gt; new Ladder(height, 4, new RandomPointGenerator())) .isInstanceOf(IllegalArgumentException.class) .hasMessage(\"[ERROR] 사다리 높이는 1이상이어야 합니다.\"); public class Ladder { //package-private 적용 static final String INVALID_HEIGHT_ERROR = \"[ERROR] 사다리 높이는 1이상이어야 합니다.\"; ... } //test 코드 assertThatThrownBy(() -&gt; new Ladder(height, 4, new RandomPointGenerator())) .isInstanceOf(IllegalArgumentException.class) .hasMessage(Ladder.INVALID_HEIGHT_ERROR); package-private를 사용함으로 상수를 재활용해 볼 수 있고 접근도 public이 아닌 같은 패키지안으로 제한해 볼 수 있기 때문에 좋은 방법인 것 같다! Stream API Stream과 관련해서도 다음과 같은 피드백들이 왔다..ㅠㅠ stream API에 익숙하시지 않다면 이 부분을 stream API를 활용해 변경해보면 어떨까요? 이 results 컬렉션에 add해야되어서 아래 메서드에 계속 인자를 넘겨주게 되네요. 로직 중간에 다른 리스트가 인자로 넘어가거나 순서가 변경된다면 기대하는 동작이 이루어지지 않을 수도 있을것같아요. 리스트를 생성하고 add 하는 것이 아니라 새로운 ArrayList를 반환하도록 변경해보는건 어떨까요? 이 부분도 stream API를 활용하면 조금 더 깔끔해질 수 있지만 익숙하지 않다면 굳이 stream을 사용하지 않으셔도 됩니다! 그동안 Stream을 어느 정도 사용할 줄 안다고 생각해왔었는데 알고 보니 빙산의 일각이었고 ㅋㅋ 그마저 제대로 사용하지 않고 있었다. 이 계기로 현재 보던 다른 책들을 제치고 모던 자바 인 액션으로 자바의 보충 시간을 갖기로 했다! 일주일 간 흡수 지난주 코드 리뷰를 기다리는 동안 어떻게 하면 더 성장할 수 있을지, 내 부족함을 채울 수 있을지에 대해 고민했고 다른 사람들의 코드 리뷰와 블로그 회고를 보며 다른 사람들의 내공을 흡수하려고 노력했다. 대략 한 10명의 코드 리뷰를 봤던 거 같고 블로깅은 한 50개 이상 정도 보지 않았을까 예상된다. ㅋㅋㅋㅋ 꾸글스(꾸준히 글 쓰는 스터디)에도 가입했는데 다른 크루들의 글을 수십 개 읽으면서 정말 멋진 생각, 고민들을 하는 사람들이 많다는 걸 알게 되었고 앞으로의 방향성이나 어떤 고민(설계, 기술적 적용 등)을 할 수 있을지 생각해 보는 뜻깊은 기간이었다~! 지금 보니깐 약간 주접을 떤 거 같은데… 저 때 당시에는 보고 나서 “와 우리 꾸글스 크루들 진짜 멋있다.. 더 열심히 해서 어울리는 크루가 되야겠다” 라는 생각이 들어서 썼다.. ㅋㅋㅋ 지금 보니 약간 부끄럽다;; 앞으로 커비에 빙의해서 다 흡수하고 다닐 예정이다. 앞으로 적용해볼 것 다른 분들의 코드 리뷰나 블로그를 보면서 엄청나게 많은 내공들을 흡수한 것 같다! 하지만 직접 써보지 않으면 의미가 없기 때문에 이제 적용해 볼 차례. 앞으로 적용할 생각에 벌써부터 설레는 것 같다. 앞으로 적용해볼 것 기능 목록을 작성할 때 도메인 별 기능 정리 Functional Interface(재입력시) Equals and hashcode 방어적 복사 정적 팩토리 메서드 +리뷰어님께 받은 코드 리뷰에 대해 관심이 있으면 다음 PR들을 참고! 1단계 - 사다리 생성 2단계 - 사다리 게임 실행 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse ladder pair-programming tdd",
    "url": "/woowacourse/2023-03-01-Ladder/"
  },{
    "title": "함수형 인터페이스(Functional Interface)에 대해 알아보자",
    "text": "Functional Interface…? 인터페이스란 용어도 낯선데 함수형까지 붙으니 더 낯설게 느껴진다.. 이놈에 대해 자세히 알아보자~ 함수형 인터페이스(Functional Interface)란? 함수형 인터페이스는 다음과 같이 1개의 추상 메소드를 갖는 인터페이스를 말한다. @FunctionalInterface interface MoominInterface&lt;T&gt; { T printCustom(); } 하지만 다음과 같이 default method나 static method는 여러 개 가질 수 있기 때문에 실제로 사용하면 다음과 같이 사용해 볼 수도 있다. @FunctionalInterface interface MoominInterface&lt;T&gt; { T printCustom(); default void printDefault() { System.out.println(\"Default\"); } static void printStatic() { System.out.println(\"Static\"); } } MoominInterface&lt;String&gt; moominInterface = () -&gt; \"Custom\"; String custom = moominInterface.printCustom(); System.out.println(custom); moominInterface.printDefault(); MoominInterface.printStatic(); // 실행 결과 // Custom // Default // Static 이제 우리는 람다식으로 순수 함수를 선언할 수 있게 되었는데 Java에서는 이 순수 함수와 일반 함수를 구분하여 사용하기 위해 함수형 인터페이스가 등장하게 되었다. 함수형 인터페이스를 사용하는 이유는 람다식이 함수형 인터페이스를 반환하기 때문! 순수 함수란 다음과 같은 부수 효과(Side Effect)를 제거해 함수의 실행이 외부에 영향을 끼치지 않는 함수를 뜻한다. 변수의 값이 변경됨 객체의 필드 값을 설정함 예외나 오류가 발생하며 실행이 중단됨 첫 번째와 같이 람다가 있기 전에는 익명 클래스를 구현해 메서드를 사용하였다. 하지만 함수형 인터페이스의 등장으로 두 번째처럼 람다식을 이용해 구현할 수 있게 되어 코드가 매우 간결해졌다. @FunctionalInterface public interface MoominFunction { int min(int first, int second); } // 익명 클래스를 구현하여 메서드 사용 MoominFunction moominFunction = new MoominFunction() { @Override public int min(int first, int second) { return Math.min(first, second); } }; System.out.println(moominFunction.min(1, 2)); // 람다 사용 MoominFunction lambdaFunction = (int first, int second) -&gt; Math.min(first, second); System.out.println(lambdaFunction.min(1, 2)); 그리고 interface 위마다 @FunctionalInterface 애노테이션이 달려있는 걸 볼 수 있는데 이 친구는 해당 인터페이스가 함수형 인터페이스 조건(추상 메서드 1개)에 맞는지 검사를 해주는 역할을 하고있다. 여러 개의 추상 메서드를 선언하면 다음과 같은 컴파일 에러가 발생한다. Java에서 제공하는 함수형 인터페이스 함수형 인터페이스 안에 선언된 메소드에 종속되는 람다식 밖에 구현할 수 없기 때문에 매개변수의 타입과 개수, 반환 값의 유무 등을 가진 메소드를 하나의 함수형 인터페이스로 구현할 수 없고, 상황에 따라 함수형 인터페이스를 만들어 줘야 된다. 하지만 자바에서는 기본적으로 많이 사용되는 함수형 인터페이스를 제공해 주기 때문에 직접 함수형 인터페이스를 만드는 경우는 거의 없다. Predicate&lt; T &gt; Supplier&lt; T &gt; Consumer&lt; T &gt; Function&lt;T, R&gt; Runnable Comparator&lt; T &gt; 더 많은 함수형 인터페이스를 보고 싶다면 다음 링크를 참고 1. Predicate&lt; T &gt; @FunctionalInterface public interface Predicate&lt;T&gt; { boolean test(T t); default Predicate&lt;T&gt; and(Predicate&lt;? super T&gt; other) { Objects.requireNonNull(other); return (t) -&gt; test(t) &amp;&amp; other.test(t); } default Predicate&lt;T&gt; negate() { return (t) -&gt; !test(t); } default Predicate&lt;T&gt; or(Predicate&lt;? super T&gt; other) { Objects.requireNonNull(other); return (t) -&gt; test(t) || other.test(t); } static &lt;T&gt; Predicate&lt;T&gt; isEqual(Object targetRef) { return (null == targetRef) ? Objects::isNull : object -&gt; targetRef.equals(object); } @SuppressWarnings(\"unchecked\") static &lt;T&gt; Predicate&lt;T&gt; not(Predicate&lt;? super T&gt; target) { Objects.requireNonNull(target); return (Predicate&lt;T&gt;)target.negate(); } } Predicate는 인자 하나를 넘겨받어 처리한 후 boolean 타입을 리턴한다. test() 메소드를 사용해서 비교 결과를 리턴 받을 수 있고 and(), or(), negate()를 사용하여 복수의 조건을 추가할 수 있다. //예시 Predicate&lt;String&gt; predicateContains = (s) -&gt; s.contains(\"min\"); Predicate&lt;String&gt; predicateStartsWith = (s) -&gt; s.startsWith(\"oom\"); String moomin = \"moomin\"; boolean containsResult = predicateContains.test(moomin); boolean startsWithResult = predicateStartsWith.test(moomin); System.out.println(containsResult); //true System.out.println(startsWithResult); //false boolean orResult = predicateContains.or(predicateStartsWith).test(moomin); System.out.println(orResult); //true 2. Supplier&lt; T &gt; @FunctionalInterface public interface Supplier&lt;T&gt; { T get(); } Supplier는 아무런 인자를 넘겨받지 않고 T 타입의 객체를 리턴한다. get() 메소드를 사용해 값을 얻을 수 있다. //예시 int value = 5; Supplier&lt;Integer&gt; supplier = () -&gt; value * 10; System.out.println(supplier.get()); //50 3. Consumer&lt; T &gt; @FunctionalInterface public interface Consumer&lt;T&gt; { void accept(T t); default Consumer&lt;T&gt; andThen(Consumer&lt;? super T&gt; after) { Objects.requireNonNull(after); return (T t) -&gt; { accept(t); after.accept(t); }; } } Consumer는 인자 하나를 넘겨받고 아무것도 리턴하지 않는다. 인자를 받아 accept()를 사용할 수 있고, 여러 개의 Consumer를 연결하여 수행할 수 있는 andThen() 메소드도 있다. //예시 Consumer&lt;Integer&gt; plus = (value) -&gt; System.out.println(value + 5); Consumer&lt;Integer&gt; minus = (value) -&gt; System.out.println(value - 5); plus.accept(10); //15 plus.andThen(minus).accept(10); //15 \\n 5 4. Function&lt;T, R&gt; @FunctionalInterface public interface Function&lt;T, R&gt; { R apply(T t); default &lt;V&gt; Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) { Objects.requireNonNull(before); return (V v) -&gt; apply(before.apply(v)); } default &lt;V&gt; Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) { Objects.requireNonNull(after); return (T t) -&gt; after.apply(apply(t)); } static &lt;T&gt; Function&lt;T, T&gt; identity() { return t -&gt; t; } } Function은 T 타입 인자를 넘겨받어 R 타입을 리턴한다. 파라미터를 받아 로직을 수행한 후 리턴할 때 사용할 수 있다. Consumer와 똑같이 andThen() 메소드를 제공하며 andThen()과 반대 방향으로 순서가 진행되는 compose()도 추가로 제공하고 있다. identity는 파라미터를 그대로 반환하는 static 메서드이다. //예시 Function&lt;String, Integer&gt; toIntFunction = (string) -&gt; Integer.valueOf(string); Function&lt;Integer, String&gt; toStringFunction = (value) -&gt; String.valueOf(value); Integer apply = toIntFunction.apply(\"5\"); System.out.println(apply); //5 (int) String andThen = toIntFunction.andThen(toStringFunction).apply(\"5\"); System.out.println(andThen); //5 (string) 진행 순서: toIntFunction -&gt; toStringFunction Integer compose = toIntFunction.compose(toStringFunction).apply(5); System.out.println(compose); //5 (int) 진행 순서: toStringFunction -&gt; toIntFunction Function&lt;Integer, Integer&gt; identityFunction = Function.identity(); Integer identity = identityFunction.apply(5); System.out.println(identity); //5 (int) 5. Runnable @FunctionalInterface public interface Runnable { public abstract void run(); } Runnable은 아무런 인자도 받지 않고 리턴도 하지 않는다. Runnable 이름에 맞게 실행만 할 수 있다. //예시 Runnable runnable = () -&gt; { for (int i = 0; i &lt; 100; i++) { System.out.println(i); } }; Thread thread = new Thread(runnable); thread.start(); 6. Comparator&lt; T &gt; @FunctionalInterface public interface Comparator&lt;T&gt; { int compare(T o1, T o2); ... } Comparator는 T 타입 인자를 두 개 넘겨받아 int 타입을 리턴한다. //예시 Comparator&lt;Integer&gt; comparator = (a, b) -&gt; Math.max(a, b); int max = comparator.compare(1, 2); System.out.println(max); //2 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java Functional-Interface lambda etc",
    "url": "/etc/java/2023-02-26-Functional-Interface/"
  },{
    "title": "디자인 패턴 - 전략 패턴, 템플릿 메서드 패턴, 상태 패턴",
    "text": "디자인 패턴에 대해 관심을 가진 이유 프리코스부터 시작해서 현재 우테코 미션을 진행하다 보니 어느 순간 계속해서 같은 형태의 코드만 양산하고 있는 듯한 느낌이 들었다. 기능을 정의하고 mvc 패턴에 따라 클린 코드, 객체지향 법칙을 적용하며 코드를 작성한다.. 라 매번 이렇게 같은 방식에 약간 재미도 없어지고 뭔가 어떻게 하면 더 성장할 수 있을까 고민에 빠지게 되었다. 매일 같은 메뉴만 먹는다고 생각해보자.. 삶이 재미가 있을까? 물론 저걸 완벽하게 다 할 수 있다는 건 아니다ㅋㅋㅋ 하지만 가끔은 다른 반찬도 먹고 싶은 법.. 그래서 어떤 게 좋을까 생각하다 책을 보며 이론을 채우기로 했다. 그중 디자인 패턴에 대해 공부해 보기로 했고 가능하면 적용해 보려고 한다. 자동차 경주 미션 때 전략 패턴을 적용해 봤는데 굉장히 인상 깊었던 기억이 있어서 그런가.. 디자인 패턴에 관심이 갔다! 근데 담당 코치님인 구구가 디자인 패턴을 싫어한다고 들었던 거 같은데.. 왜 그런지 조만간 물어봐야겠다. 디자인 패턴이란? 디자인 패턴에 대해 처음으로 시작하는 글이니 만큼 간단하게 디자인 패턴의 정의에 대해 알아보고 패턴으로 넘어가자. 객체지향 설계를 하다 보면, 이전과 비슷한 상황에서 사용했던 설계를 재사용하는 경우가 종종 발생한다. 이런 설계는 특정 상황에 맞는 해결책을 빠르게 찾을 수 있도록 도와주는데, 이렇게 반복적으로 사용되는 설계는 일정 패턴을 가지며 이를 디자인 패턴이라 한다. 이런 패턴을 잘 습득하면 상황에 맞는 올바른 설계를 더 빠르게 적용할 수도 있고, 각 패턴의 장단점을 통해 설계를 선택하는 데 도움을 얻을 수도 있다. 전략(Strategy) 패턴 전략 패턴은 객체가 할 수 있는 행위 각각에 대해 전략 클래스를 생성하고 이를 정의하는 공통의 인터페이스를 정의해 행위를 동적으로 바꾸고 싶은 경우 직접 행위를 수정하지 않고 전략을 바꿔끼어주어 행위를 유연하게 확장할 수 있는 방법이다. 우리는 말보다 코드가 더 친숙하니 코드로 한번 봐보자! 다음과 같은 할인 정책을 적용하고 있는 매장이 있다고 해보자 public class Calculator { public int calculate(boolean firstGuest, List&lt;Item&gt; item) { int sum = 0; for (Item item : items) { if (firstGuest){ sum += (int) (item.getPrice() * 0.9); // 첫 손님 10% 할인 }else if (!item.isFresh()){ sum += (int) (item.getPrice() * 0.8); //덜 신선한 경우 20% 할인 }else{ sum += item.getPrice(); } } } } 위와 같은 코드의 경우 다음과 같은 문제점이 있다. 모든 할인 정책들이 하나의 코드에 있어, 정책이 추가될 때마다 코드 분석이 힘들어진다. 가격 정책이 추가될 때마다 if 블록이 추가되기 때문에 유지 보수가 더 힘들어진다. 그럼 어떻게 하면 좋을까?? 이때 전략 패턴을 적용해 볼 수 있다. 각각의 가격 할인 정책을 전략으로 두고 별도 객체로 분리하는 것이다. 아래와 같이 분리하면 Calculator라는 콘텍스트에서는 전략을 직접 선택하지 않고 클라이언트에서 DI(의존 주입)를 이용해 전략을 전달해 줄 수 있다. 그러면 다음과 같이 코드를 작성할 수 있을 것이다. public class Calculator { private DiscountStrategy discountStrategy; public Calculatr(DiscountStrategy discountStrategy) { this.discountStrategy = discountStrategy; } public int calculate(List&lt;Item&gt; item) { int sum = 0; for (Item item : items) { sum += discountStrategy.getDiscountPrice(item); } return sum; } } 이렇게 되면 이제 새로운 할인 정책이 추가되더라도 Calculator 클래스의 코드는 변경되지 않고 단지 새로운 전략 클래스만 추가 되어 바꿔 끼어질 수 있다. 즉, 개방 폐쇄 원칙인 OCP를 만족하게 된 것이다! 객체지향 프로그래밍 5가지 원칙인 SOLID에 대해 자세히 알아보고 싶으면 다음 링크를 참고해보자 예를 들어가며 SOLID에 대해 알아보자(1) 예를 들어가며 SOLID에 대해 알아보자(2) 템플릿 메서드(Template Method) 패턴 프로그램을 구현하다 보면 완전히 동일한 절차를 가진 코드를 작성하게 될 때가 있다. 예를 들어 다음과 같이 사용자 정보를 가져오는 부분의 구현만 다를 뿐 인증을 처리하는 과정은 완전히 동일할 수 있다. public class DbAuthenticator { public Auth authenticate(String id, String pw) { //사용자 정보로 인증 확인 User user = userDao.seletById(id); boolean auth = user.equalPassword(pw); //인증 실패시 익셉션 발생 if (!auth){ throw createException() } //인증 성공시, 인증 정보 제공 return new Auth(id, user.getName()); } } private AuthException createException() { return new AuthException(); } public class LdapAuthenticator { public Auth authenticate(String id, String pw) { //사용자 정보로 인증 확인 boolean lauth = IdapClient.authenticate(id, pw); //인증 실패시 익셉션 발생 if (!auth){ throw createException() } //인증 성공시, 인증 정보 제공 LdapContext ctx = IdapClient.find(id); return new Auth(id, ctx.getAttribute(\"name\")); } } private AuthException createException() { return new AuthException(); } DB나 LDAP가 아닌 다른 인증 서버를 두더라도 위의 과정을 유사하게 거칠 것이다. 이렇게 실행 과정/단계는 동일한테 각 단계 중 일부의 구현이 다른 경우 사용할 수 있는 패턴이 템플릿 메서드 패턴이다. 템플릿 메서드 패턴은 다음과 같이 두 가지로 구성될 수 있다. 실행 과정을 구현한 상위 클래스 실행 과정의 일부 단계를 구현한 하위 클래스 상위 클래스는 실행 과정을 구현한 메서드를 제공하고 이 메서드는 구현하는데 필요한 각 단계를 정의하며 이 중 일부 단계는 추상 메서드를 호출하는 방식으로 구현된다. 예를 들어 다음과 같이 작성될 수 있다. public abstract Authenticator { //템플릿 메서드 public Auth authenticate(String id, String pw) { if (!doAuthenticate(id, pw)) { throw createException(); } return createAuth(id); } protected abstract boolean doAuthenticate(String id, String pw); private RuntimeException createException() { throw new AuthException(); } protected abstract Auth createAuth(String id); } 두 클래스에서 차이가 나는 부분인 인증 여부 확인(doAuthenticate), 객체 생성 단계(createAuth)는 추상 메서드로 분리하였다. authenticate() 메서드는 모든 하위 타입에 동일하게 적용되는 실행 과정을 제공하기 때문에, 이 메서드를 템플릿 메서드라 한다. 위처럼 Authenticator 클래스를 생성한 후 이제 하위 클래스에서 상속받아 추상 메서드 부분만 알맞게 재정의 해주면 된다. public class LdapAuthenticator extends Authenticator { @Override protected boolean doAuthenticate(String id, String pw) { return IdapClient.authenticate(id, pw); } @Override protected Auth createAuth(String id) { LdapContext ctx = IdapClient.find(id); return new Auth(id, ctx.getAttribute(\"name\")); } } 이제 새로운 인증 서버가 추가되더라도 다른 부분만 추가적으로 구현해 주면 되니 중복 코드를 제거할 수 있다. 이렇게 템플릿 메서드 패턴을 사용함으로 써 코드 중복 문제를 제거하면서 동시에 코드를 재사용할 수 있게 되었다! 상태(State) 패턴 상태에 따라 다르게 동작하는 자판기를 구현한다고 생각해 보자 public class VendingMachine { public static enum State { NOCOIN, SELECTABLE } private State state = State.NOCOIN; public void insertCoin(int coin) { switch(state) { case NOCOIN: increaseCoin(coin); state = State.SELECTABLE; break; case SELECTABLE: increaseCoin(coin); } } public void select(int productId) { switch(state) { case NOCOIN: //아무 행동 X break; case SELECTABLE: provideProduct(productId); decreaseCoin(); if (hasNocoin()){ state = State.NOCOIN; } } } ... } 위와 같은 경우 새로운 상태가 추가될 때 마다 insertCoin() 메서드와 select() 메서드에 조건문이 추가된다. 그렇게 되면 후에 유지 보수가 매우 어렵게 될 것이다.(매번 조건문을 어렵게 찾고 수정해야 함) 그러면 어떻게 해야 될까..? 위의 코드를 보면 각 상태에 따라 다르게 기능들이 동작하는 걸 볼 수 있다. 이렇게 기능이 상태에 따라 다르게 동작해야 할 때 사용할 수 있는 패턴이 상태 패턴이다. 상태 패턴에서는 아래와 같이 상태를 별도로 분리하고 각 상태별로 맞는 하위 타입을 구현한다. 근데 이렇게 보다 보니깐 전략 패턴과 상태 패턴이 서로 헷갈렸다. 둘이 공통의 인터페이스로 분리한 후 각각의 상황에 맞게 기능을 구현하는 거까지 비슷하다 보니.. 전략 패턴은 한 번 인스턴스를 생성하고 나면, 상태가 거의 바뀌지 않는 경우에 사용하고 상태 패턴은 한 번 인스턴스를 생성하고 난 뒤, 상태를 바꾸는 경우가 잦은 경우에 사용할 수 있다! 분리하고 나면 이제 VendingMachine 코드는 아래와 같이 되고 public class VendingMachine { private State state; public VendingMachine() { state = new NoCoinState(); } public void insertCoin(int coin) { state.increaseCoin(coin, this); // 상태 객체에 위임 } public void select(int productId) { state.select(productId, this); // 상태 객체에 위임 } public void changeState(State newState) { this.state = newState; } ... } 각각의 상태를 구현한 클래스들은 아래와 같이 된다. public class NoCoinState implements State { @Override public void increaseCoin(int coin, VendingMachine vm) { vm.increaseCoin(coin); vm.changeState(new SelectableState()); } @Override public void select(int productId, VendingMachine vm) { SoundUtil.beep(); } } public class SelectableState implements State { @Override public void increaseCoin(int coin, VendingMachine vm) { vm.increaseCoin(coin); } @Override public void select(int productId, VendingMachine vm) { vm.provideProduct(productId); vm.decreaseCoin(); if (vm.hasNoCoin()) { vm.changeState(new NoCoinState()); } } } 이렇게 상태 패턴을 적용하게 되면 새로운 상태가 추가되더라도 콘텍스트 코드(Vending Machine)가 받는 영향은 최소화되어 유지 보수에 유리하다. 새로운 상태가 추가되더라도 insertCoin() 메서드와 select() 메서드 코드는 그대로 유지되고 구현 코드가 각 상태 별로 구분되기 때문에 상태 별 동작을 수정하기 쉽다. 상태 변경은 누가하는게 좋을까? 그런데 여기서 한 가지 더 고민해 볼 게 있다. 상태 변경을 누가 하느냐에 관한 것이다. 콘텍스트(VendingMachine)가 될 수도 있고 위에서 한 것처럼 상태 객체가 할 수도 있다. 콘텍스트에서 상태를 변경할 경우 코드는 아래처럼 된다. 콘텍스트에서 변경할 경우 콘텍스트 코드가 약간 복잡해질 수 있다. public class VendingMachine { private State state; public VendingMachine() { state = new NoCoinState(); } public void insertCoin(int coin) { state.increaseCoin(coin, this); if (hasCoin()) { changeState(new SelectableState()); //콘텍스트에서 상태 변경 } } public void select(int productId) { state.select(productId, this); if (state.isSelectable() &amp;&amp; hasNoCoin()) { changeState(new NoCoinState()); //콘텍스트에서 상태 변경 } } public void changeState(State newState) { this.state = newState; } private boolean hasCoin() { ... } private boolean hasNoCoin() { return !hasCoin(); } ... } public class SelectableState implements State { // 콘텍스트가 상태를 변경하므로, 상태 객체는 자신이 할 작업만 처리 @Override public void select(int productId, VendingMachine vm) { vm.provideProduct(productId); vm.decreaseCoin(); } } 그럼 어떤 방식이 더 좋을까..? 상태 변경을 누가 할지는 주어진 상황에 맞게 선택해야 된다. 위와 같이 콘텍스트에서 상태를 변경하는 방식은 상태 개수가 적고 상태 변경 규칙이 거의 바뀌지 않는 경우에 유리하다. 왜냐하면 상태 종류가 지속적으로 변경되거나 상태 변경 규칙이 자주 바뀌면 그만큼 콘텍스트 상태 변경 처리 코드가 복잡해질 가능성이 높기 때문에 유연성이 떨어질 수 있다. 반면에 상태 객체에서 상태를 변경하는 경우, 콘텍스트에 영향을 주지 않으면서 상태를 추가하거나 상태 변경 규칙을 바꿀 수 있게 된다. 하지만, 상태 변경 규칙이 여러 클래스에 분산되어 있기 때문에, 클래스가 많아질수록 상태 변경 규칙을 파악하기 어려울 수 있다. 또한, 한 상태 클래스에서 다른 상태 클래스에 대한 의존도 발생하게 된다. 참고: 개발자가 반드시 정복해야 할 객체지향과 디자인 패턴 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "design-pattern strategy template-method state etc design pattern",
    "url": "/etc/design%20pattern/2023-02-22-design-pattern(1)/"
  },{
    "title": "우테코 - 자동차 경주 미션 회고",
    "text": "그렇게 기다리고 기다리던 우테코 생활이 2월 7일부터 시작되었다..!! 첫 번째 미션인 자동차 경주가 꽤 지난 지금 회고하는 이유는 그동안 엄청나게 많이 들어오는 새로운 정보(규칙, 일정, 공지 등) 처리에 뇌 정지가 왔고 ㅋㅋㅋㅋ 거기다가 동시에 미션 + 연극 + 새로운 크루들과 친해지기까지 해야 되었기 때문에 엄청나게 바쁜 나날들이었다…(나중에 시간 되면 일상 블로깅도..!) 그리고 제일 치명적이었던 건 너무 급하게 집을 옮기고(2월 6일에 집을 옮김..) 위에 과정들을 진행하며 몸을 거침없이 소비했는데 내 몸이 받쳐주지 않아 몸 컨디션이 많이 악화됐었다. 어제 병원까지 갔다 와서 약 먹으며 한 이틀째 죽을 먹으니깐 이제서야 조금 회복되는 거 같기도 하다. 다음 주부터는 풀 컨디션으로 돌아가서 다시 열심히 달려야지.. 다들 몸 잘 챙기면서 코딩합시다!! 일단 tmi는 여기까지 하고 이제 본 주제인 미션에 대해 얘기해 보자. 자동차 경주 미션 시작 페어 프로그래밍 첫 번째 미션은 자동차 경주 미션이었다. 프리코스 미션과 비슷한데 다른 점이라면은 페어 프로그래밍으로 페어와 함께 미션을 진행해야 된다! 페어 프로그래밍이 어떤 건지 모르겠으면 다음글 참고. 페어 프로그래밍은 처음이었는데 정말 쉽지 않았던 것 같다. 계속해서 서로 대화를 주고받으며 더 나은 코드를 짜기 위해 집중력을 발휘하기 때문에 그만큼 피로감도 두 배, 세배로 상승했다. 그리고 옆에서 누가 나를 감시하는 것 같아 코드를 작성할 때 더욱 긴장이 되었던 것 같다 ㅋㅋㅋ 그래도 정기적인 휴식으로 잘 커버하면서 진행했던 기억이 난다! 하지만 그만큼 얻어 가는 것도 많고 더 좋은 코드를 짤 수 있어 좋은 방법 같다는 생각이 든다. 내가 진행자(드라이버)가 되어 코드를 작성할 때도 페어에게 왜 이렇게 작성했는지, 이런 방향은 어떤지 계속 설명하고 의논하며 코드를 작성해 더욱 근거 있고 좋은 코드를 작성할 수 있게 되는 거 같아서 더욱 마음에 들었다. 전략 패턴을 이용해 랜덤 값 테스트 public void move() { if (isMovable()){ position++; } } private boolean isMovable() { return ((int) (Math.random() * 10)) &gt;= 4; } 4 이상의 값이 나와야 전진이라는 요구사항이 있었다. 그렇다면 이 랜덤 값을 생성하는 로직을 포함하고 있는 move 메소드는 그때그때마다 결괏값이 다를 텐데 어떻게 테스트해야 될까?? 랜덤 값을 생성하는 클래스와 사용하려는 클래스의 강한 결합을 약한 결합으로 낮추기 위한 분리가 필요하다. 이를 위해 전략 패턴(Strategy Pattern)을 사용하였다. 전략 패턴은 객체가 할 수 있는 행위 각각에 대해 전략 클래스를 생성하고 이를 정의하는 공통의 인터페이스를 정의해 행위를 동적으로 바꾸고 싶은 경우 직접 행위를 수정하지 않고 전략을 바꿔끼어주어 행위를 유연하게 확장할 수 있는 방법이다. 다음과 같이 RacingNumberGenerator 인터페이스를 생성한 후 구현체인 전략들을 주입해 주면 테스트해 볼 수 있다. public interface RacingNumberGenerator { int generate(); } //실제 사용 랜덤값(0 ~ 9) 생성 generator public class RacingRandomNumberGenerator implements RacingNumberGenerator{ private static final int MIN_VALUE = 0; private static final int MAX_VALUE = 9; @Override public int generate() { return (int) (Math.random() * (MAX_VALUE - MIN_VALUE + 1)); } } //이동가능한 값을 생성하는 generator (Test Stub) public class StubMovableRacingNumberGenerator implements RacingNumberGenerator { private static final int MOVABLE_VALUE = 4; @Override public int generate() { return MOVABLE_VALUE; } } ////이동 불가능한 값을 생성하는 generator (Test Stub) public class StubUnmovableRacingNumberGenerator implements RacingNumberGenerator { private static final int UNMOVABLE_VALUE = 3; @Override public int generate() { return UNMOVABLE_VALUE; } } public void move(RacingNumberGenerator generator) { if (isMovable(generator)){ position++; } } private boolean isMovable(RacingNumberGenerator generator) { return generator.generate() &gt;= 4; } 위에 Stub에 대해서 궁금증이 생겼다면 Test Double을 읽어보자. 테스트하려는 객체와 연관된 객체를 사용하기가 어렵고 모호할 때 대신해 줄 수 있는 객체를 테스트 더블이라 한다. Stub은 Test Double의 한 종류로 링크에 자세하게 설명되어 있다! 좋은 방법, 글 추천해 준 페어(지토), 리뷰어(다니)에게 감사 인사를… 🙇🙇🙇 +추가적으로 다양한 테스트 종류에 대해 궁금하다면 다음 글을 추천드립니다! 제 리뷰어가 쓰신 글인 건 비밀ㅋㅋ 단위 테스트 vs 통합 테스트 vs 인수 테스트 원시 값 포장 이번에 구현할 때 객체지향 생활 체조 원칙을 참고했었는데 모든 원시 값과 문자열을 포장하라는 규칙을 참조해 Car 객체에서 이름과 위치를 나타내는 속성을 다음과 같이 원시 값으로 포장했었다. public class Car { private final Name name; private final Position position; ... } 원시 값을 포장함으로 써 다음과 같은 장점들을 느껴볼 수 있었다. 각각의 데이터에 대한 정보를 숨김 한 가지 책임만 잘 줄 수 있고 사용하는 곳 가장 가까운 곳에 코드를 둘 수 있어 유지 보수하기 좋음 코드 리뷰 코드 리뷰나 질문을 하나하나 다 적기는 너무 많아 아래 PR들을 참고하면 좋을 거 같고 공유하면 좋을 것 같은 것들을 적어보겠다. 1단계 - 자동차 경주 구현 2단계 - 자동차 경주 리팩터링 과연 CarService가 필요할까? 음… 이전까지 계속 사이드 프로젝트를 하고 있었기 때문에 이번 미션에도 자연스럽게 Controller, Service를 무지성으로 생성했던 것 같은데 이번 미션에서는 domain에서만 기능을 부여해서 만들어도 충분히 커버가 가능했기 때문에 Service 영역이 필요 없었다. 이걸 시작으로 오버 프로그래밍에 대해서 조금 생각해 봤다. 현재 상황에 맞게 하느냐… 아니면 후에 확장성까지 생각하여 추가적으로 구현하느냐(예를 들어 Service, DTO 같은).. 어느 것이 확실하게 정답이라고 할 수 없다. 현재 주어진 상황에 충실하게 짜볼 수도 있는 것이고 아니면 미리 후에 확장성까지 고려할 수 있도록 짜보며 공부해 볼 수도 있을 것이다. 현재 내가 내린 결정은 현재 상황에 맞게 하되 여유가 있으면 추가적으로 적용해 보자였다! 현재 나에게 주어진 상황에 맞게 하지도 못하는데 오버 프로그래밍을 하는 것은 말 그대로 오버라 할 수 있다ㅋㅋㅋ 그래서 우선 기본에 충실한 후 이 기본기가 갖춰지면 조금씩 오버 프로그래밍을 적용해 보려고 한다. 공백 처리 기존에 나는 입력에 공백이 있는지 확인하기 위해 trim으로 공백을 제거한 뒤 length를 확인하여 처리했었는데 String의 isBlank()로 바로 처리가 가능하다..ㅜㅠ 아직 자바의 기본 API에 대해 더 공부가 필요함을 느꼈다. 보는김에 isNull() vs isEmpty() vs isBlank()의 차이를 한번 알아보자. isNull(): 문자열이 null이면 true를 반환 isEmpty(): 문자열의 길이가 0이면 true를 반환 isBlank(): 문자열이 비어있거나 공백만 포함하고 있으면 true 반환 String isNull() isEmpty() isBlank() ”” false true true ” “ false false true null true NPE NPE 리뷰어님의 질문에 대한 나의 답변 Cars 일급 컬렉션을 만들었을 때 어떤 장점이 있었나요? 관련 상태나 행위를 한곳에서 관리할 수 있습니다. 컬렉션의 불변성을 보장해 줄 수 있습니다. isSameAs()와 isEqualTo()는 어떤 차이가 있을까요? isSameAs는 주소를 비교하는 메소드이고 isEqualTo는 값 자체를 비교하는 메소드입니다. isSameAs 같은 경우 원시형 타입은 값 비교를 하고, 객체는 주소를 비교합니다. isEqualTo는 값으로 비교하지만 객체를 비교하게 되는 경우 참조를 비교합니다. 자동차 경주(첫번째 미션) 미션을 하고 난 뒤 느낀 점 자동차 미션을 제출한 날 참 많은 감정이 느껴졌다. 당연한 거지만.. 많은 부족함을 느꼈고 페어에게도 도움을 많이 주고 싶었는데 그러지 못한 것 같아 아쉬웠다. 다음 페어에게는 더 많은 걸 알려줄 수 있는 좋은 페어가 될 수 있도록 더 열심히 해야겠다는 생각이 들었다. 그리고 앞으로 어떻게 더 좋은 코드를 짤 수 있을지, 다른 사람들은 어떻게 짰는지 보며 많은 고민을 해봐야겠다. 그리고 뭔가 진 거 같은 느낌이 들어 계속해서 분함이 느껴졌다…ㅋㅋㅋ 이 분함이 나를 더 성장시켜주기를… 우테코에 오고 나서 좋은 습관이 하나 생겼다. 뭐든 그냥 하는 게 아니라 왜라는 걸 붙이는 습관..(좋은 거 맞..나?) 우테코가 물고기를 잡는 법을 알려주는 것이 아닌 물고기를 잡는 환경을 만들어 주다 보니 자연스럽게 이런 습관이 생기게 되었는데 끊임없이 고민하고 생각하게 해주어 엄청난 도움이 되고 있다. 하지만, 이 모든 게 가능하려면 건강(체력)이 제일 중요한 것 같다. 그래서 앞으로 어떻게 루틴을 잡을지, 체력을 유지할 수 있을지도 고민해 봐야겠다. 이 글을 보는 분들도 아프지 않게 꼭 몸 잘 챙기면서 코딩해 보자~ 그럼 첫 번째 회고는 여기서 끝! *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse racing-car pair-programming",
    "url": "/woowacourse/2023-02-18-Racing-Car/"
  },{
    "title": "페어 프로그래밍 준비하기",
    "text": "곧 있으면 우테코가 시작된다!(두근두근..) 하지만 위와 같이 우테코에서 미션을 진행할 때 서로 페어 프로그래밍을 통해 미션을 해결하고 성장한다고 적혀있다. 페어 프로그래밍을 하기 앞서 페어 프로그래밍이 뭔지, 어떻게 진행하는 건지, 또 어떻게 하면 더 효율적으로 할 수 있을지 알아보자! 페어 프로그래밍(Pair Programming)이란? 페어 프로그래밍은 애자일 개발 방법론 중 한개로 하나의 컴퓨터에서 두 사람의 프로그래머가 작업하는 방법이다. 코드를 작성하는 사람이 진행자가 되고 다른 한 사람이 관찰자가 되어 코드리뷰를 하며 프로그래밍을 하고 역활은 중간중간 계속 바꾸며 진행한다. 어떻게 진행해볼 수 있을까? 다음 영상을 참고해서 상세한 예를 들며 설명하겠다! 조금이지만 직접 다른 분과 함께 진행하는 과정을 볼 수 있으니 한번 봐보자. (TMI: 우테코 프론트엔드 코치님이신 준님 영상이다! 😆 ㅋㅋㅋ 볼 때마다 밝은 에너지가 나에게도 전파되는 거 같아 기분이 좋고 빨리 한번 뵙고 싶다 ㅎㅎ ) 아웃라인(outline)을 잡자 먼저 코드를 구현해 보기 전에 우리가 무엇을 할지, 어떤 식으로 할지 아웃라인을 만들고 시작해 보자. 예를 들어 카운터 앱을 만든다면 다음과 같이 아웃라인을 작성해 볼 수 있다. 여기서 중요한 점은 서로 능동적인 태도로 의견을 나누며 번갈아가며 작성하는 것이다. 초기값 0 표시 + 버튼을 누르면 1씩 올라간다. - 버튼을 누르면 1씩 내려간다. reset 버튼을 누르면 0으로 초기화 … 등등 아웃라인을 따라 코드 구현 이제 아웃라인을 보며 코드를 같이 구현할 건데 이때도 가장 중요한 것은 코딩이 아닌 팀원과 대화를 하면서 문제를 같이 풀어 나가는 것이 중요하다. 코딩을 하면서도 자신이 어떻게 짜려고 하는지 왜 이렇게 하는지에 대해 항상 파트너와 대화를 하면서 진행해야 한다. 그렇지 않으면 진행자(코드 작성자)는 코드만 작성하게 될 것이고 관찰자는 관객 모드로 화면만 계속해서 보게 될 것이다. 위 영상에서 나온 대화 몇 가지를 통해 한번 분석해 보자. (4분 12초부터) 정수님: 나중에 카운터를 계속 갱신해야 되니까 id 속성 값도 필요할까요? 준님: 그렇네요. 저 안에 텍스트를 계속 바꿔주어야 하니깐 뭔가 구분할 수 있는 id가 좀 있으면 좋겠네요. 정수님: counter-value? 어떤 게 좋을까요? 준님: counter-value로 일단 가보고 하다가 어색하면 그때 바꾸어 볼까요? 그냥 간단한 코드 작성이라도 어떻게 짜려는지, id 값을 왜 넣을려는지 그리고 질문(어떤 게 좋을까요?)을 통해 상대방과 끊임없이 대화하려고 하고 있는 걸 볼 수 있다. (5분 37초부터) 정수님: 일단 counter라는 변수를 만들어 볼까요? 준님: counter는 전체 앱인데 counter란 변수명이 전체 앱을 나타내는 걸로 쓰고 있는 것 같다는 생각이 들어요. 정수님: counter value? 준님: counter value가 여기서도 그렇고 똑같이 맞추는 게 어떨까요? 그리고 요즘은 변수를 const 또는 let으로 선언하거든요. counter value 같은 경우는 let으로 변수를 선언하면 더 좋을 것 같아요. 정수님: 제가 너무 옛날에 했어서..ㅎㅎ 준님: 저도 최근에 배워서 이 정도 조금 알고 있습니다.ㅎㅎㅎ 관찰자가 그냥 단순히 진행자가 코드를 작성하는 것만 지켜보는 게 아니라 더 좋은 방법(counter -&gt; counterValue, var -&gt; let)이 있으면 제안하고 이유를 설명함으로 써 더 좋은 코드를 작성하도록 개선하려고 노력하고 있다. 그리고 마지막 두 줄의 대화를 보면 서로 존중하면서 대화를 진행하며 커뮤니케이션 스킬도 늘 수 있을 것 같다는 생각도 든다. (8분 54초부터) 준님: 그리고 textRender인데 생각해 보니깐 getElement인데 counter-value만 가져오는 역할을 해서 이게 좀 헷갈리는 것 같다는 생각이 드는 것 같아요. 어떻게 리팩터링 해보면 좋을까요? 정수님: 어떻게 하면 좋으시겠어요? 생각하시는 걸 한번 보여주시면 getElement 함수에서 준님: getElement 함수에서 여기서 인자로 그냥 selector를 받으면 좋을 거 같거든요. 여기서 selector를 받고 그리고 여기는 byId라고만 하면은 id 값으로 만 가져올 수 있는데 querySelector라는 메서드를 이용하면 id도 가져올 수 있고 class도 가져올 수 있거든요. 그래서 textRender인데 여기서도 뭔가 selector를 받아와야겠네요. 이게 너무 일반적인 네이밍이어서 가지고 그게 좀 어려운 거 같은데 textRender라기보다는 counterRender가 더 어울릴 수 있겠네요. 정수님: 그렇네요. 준님: 지금은 당장 숫자를 업데이트해 주는 게 하나의 엘리먼트뿐인데 너무 일반적인 함수를 만들다 보니깐 뭔가 비대해진 느낌인 것 같아요. 정수님: 하는 역활이 좀 크다는 거네요. 준님: countRender 하고 getElement 해서 여기에 counter-value를 넣어주면 훨씬 낫지 않나 생각하는데 혹시 어떠세요? 정수님: 좋은 것 같아요. 어떻게 하면 더 좋은 구조로 개선할지에 대해 계속해서 의견을 나누고 있다. 위에서는 준님이 어떤 좋은 방법을 제안한 뒤에 이유를 설명하고 나서 정수님도 적극적으로 이해하며 확인을 한 후 수용하여 더 좋은 코드가 나오게 되었다. 페어 프로그래밍 하는 걸 보고 개인적으로 느낀점 계속 티키타카(의견을 주고받으며)를 통해 더 좋은 코드를 짜게 되는 거 같아 너무 좋은 것 같다. 왜 이렇게 짜게 되었는지 페어에게 설명을 해주면서 하게 되어 나도 더욱 근거 있고 정확한 코드를 작성할 수 있고 그걸 페어가 재확인해 주거나 또는 더 좋은 방법이 있으면 페어가 의견을 제시해 줄 수도 있다. 서로에게 잘하는 것을 배울 수 있음. 의사소통(커뮤니케이션) 능력 증가 어떻게 페어 프로그래밍을 더 효율적으로? 가장 이상적인 페어는 실력이 비슷한 페어. 짝을 이룬 두 사람이 실력 차이가 나더라도 한 사람이 일방적으로 강의하는 방식은 권장 X. 가능한 번갈아가며 눈높이를 맞추려는 노력을 해야 두 사람 모두 성장 가능 서로 더 좋은 방식으로 코드를 짜려고 노력해야 됨. 생각하는 걸 멈추지 말자! 진행자: 나 홀로 코딩 금지, 관찰자: 멍 때리지 말고 집중 자주 스위칭하기(진행자 &lt;-&gt; 관찰자) 한 명만 키보드를 잡고 있는 것은 좋지 않다. 정기적인 휴식 갖기 페어 프로그래밍은 다른 사람과 같이 집중하는 것만큼 피로감도 엄청나기 때문에 정기적으로 휴식시간을 갖도록 노력하자 참고: Mathpresso 개발방법론 — 1. 페어 프로그래밍(pair programming) 팀리더와 페어 프로그래밍 어떻게 하죠? *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "woowacourse pair-programming",
    "url": "/woowacourse/2023-02-05-Pair-Programming/"
  },{
    "title": "KPT로 지속적인 피드백하며 성장하기",
    "text": "회고 방법론에 대해 고민하게 된 계기 팀 프로젝트라는 건 쉽지 않은 일이라 생각한다. 많은 사람들이 모여서 같이 작업을 하는 만큼 어떤 일이 생길지 예상을 할 수 없다. 운이 좋게 잘하는 사람들만 만나서 편하게 갈 수도 있겠지만 그렇지 않은 경우가 더 많을 것이다. 마치 팀 프로젝트는 하나의 초기 스타트업과 같다고 생각이 든다. 아직 어떠한 것도 정립되어 있지 않아 모든 것이 백지상태. 좋게 말하면 어느 것을 자유롭게 적용하거나 시도해 볼 수 있지만, 반대로 말하면 수많은 선택지 속에 선택을 해야 되고 그것이 맞는지 틀린 지는 나 같은 주니어에겐 참 어려운 일이다.. 어떤 프로젝트를 만드는 데 기술이 어려울 수도 있겠지만 그것보다 계속 이끌어가고 지속적으로 개선해나갈 수 있는 능력이나 커뮤니케이션 같은 *소프트 스킬들이 더 어려운 것 같다. 그래서 요즘 이에 대한 많은 고민을 하고 있다.. 현재 진행 중인 프로젝트에서 자주 스크럼을 하려고 하지만 이것만으로는 부족함을 많이 느꼈고 동료끼리 어떻게 체계적으로 피드백을 잘 주고받을지, 팀의 단합력, 이해력, 수행력 향상을 생각하던 차에 KPT 회고라는 걸 알게 되었다. 하드 스킬(Hard Skill): 프로그래밍 능력, 소프트웨어 디자인 및 설계 등 업무 수행에 직접적으로 필요한 능력 소프트 스킬(Soft Skill): 하드 스킬을 효율적으로 활용할 수 있게 도와주는 소통 능력, 실행력, 리더십 등 대인 관계와 관련된 정서적 능력 곧 있으면 우테코에서 많은 팀작업이나 페어 프로그래밍을 할 예정이므로 이번에 KPT 회고에 대해 알아보고 정리해 그때 적용해 보려고 한다. 조만간 페어 프로그래밍에 대해서도 찾아보고 정리할 계획이다. 다음 글에 대해 너무 공감이 가서 한번 읽어보면 좋겠다. 팀이 현재 아무런 잡음 없이 잘되고 있는 것처럼 보이는 것은 물론 잘되어가고 있는 경우도 있겠지만 사실 상처가 곪아가고 있는 중일 수도 있다. 조용하다고 당신의 조직이 건강한가 간단하게 요약해보자면 조직이 사람들로 이뤄진 이상 크고 작은 실수가 생기지 않을 리 없다. 그러므로 실수와 문제가 없는 조직일수록 무언가 감추는 것이 많다고 생각해야 옳다. 시끄러울 정도로 실수를 드러내고 지적하는 조직이 조용한 조직보다 성과가 높을뿐더러 높은 성과가 오래도록 유지된다. 그렇기 때문에 조직의 건강성은 무결점의 ‘정적인 상태’가 아니라 문제를 끊임없이 제기하고 그것을 고쳐 나가려는 동적인 과정에서 찾아야 한다. “지혜란 무엇을 아는지 그리고 무엇을 모르는지를 아는 것이다.” - 공자 KPT란? KPT는 Keep, Problem, Try의 약자로 세 가지 관점으로 분류하여 회고를 진행하는 회고 방법으로 다음과 같이 3가지의 타입을 작성하고 공유하면서 서로가 의견을 주고받으며 해결책을 이끌어 낼 수 있는 회고 방법론이다. Keep: 잘하고 있는 부분, 계속해서 했으면 좋겠는 부분 Problem: 문제가 있는 부분, 개선이 필요한 부분 Try: 문제가 있는 부분을 개선할 수 있도록 우리가 시도해 볼 수 있는 부분 KPT 진행 과정 진행 순서 시간은 50분 정도로 진행하는데 여유시간 10분 정도를 추가적으로 잡아주자 (총 1시간). KPT 사전 설명 - 5분 Keep, Problem 작성 - 5분 서로 Keep, Problem 공유 - 10분 Try 작성 - 7분 서로 Try 공유 - 8분 앞으로 적용할 Try &amp; Action 선정 - 15분 1. KPT 사전 설명 KPT를 팀에 적용하려고 하면 뭔지 모르는 팀원도 있을 수 있으므로 간단하게 설명을 하고 진행하는 게 좋다. 팀의 리더가 먼저 KPT에 대해 공부하고 공유할 자료를 만들어 설명해 주면 좋을 것 같다. 2. Keep, Problem 작성 두 가지의 다른 색깔(ex) Keep - 파랑, Problem - 빨강)의 포스트잇을 팀원에게 나눠준다. 그리고 팀원은 5분 동안 자신의 생각대로 Keep이나 Problem에 대해 작성한다. Keep에는 계속했으면 좋겠는 부분, Problem에는 개선이 필요한 부분을 작성할 수 있다. (여기서 타임 타이머는 언제든지 확인할 수 있게 모두가 잘 보이는 곳에 두도록 하자) 3. 서로 Keep, Problem 공유 각자 돌아가면서 자기가 작성한 Keep과 Problem을 읽고 진행자에게 넘겨주면 진행자는 포스트잇을 화이트보드의 Keep, Problem 영역에 붙인다. 각자 공유가 다 된 후에 이해가 안 되는 부분은 작성자에게 다시 물어볼 수 있다. 이때 작성자는 이해할 수 있도록 설명을 해줄 수 있는데 불필요한 논쟁과 토론이 되지 않도록 진행자가 잘 중재해 줘야 된다. 여기서 중요한 것은 현재 진행되는 프로젝트에서 좋았던 점(Keep), 불편한 점(Problem) 이 구성원들에게 잘 보이게 공유가 되는 것이 중요! 4. Try 작성 이제 Try를 작성하는데 아무거나 내는 것보다는 Problem의 해결책에 대해 작성하는 게 좋다. 여기서 Try를 작성할 때 주의할 점은 구체적이고 실천적이어야 한다. ~~를 잘해보자(ex) 공유를 잘하자~)라는 식의 Try 이면 지켜지기 어렵다. 해결할 수 있는 구체적인 솔루션을 제공해야 지켜질 수 있다. 예를 들어 공유가 잘 되고 있지 않다라는 Problem이 있으면 공유를 잘하자~라는 애매한 Try보다는 ~~을 하면 문서화해서 노션이나 구글 docs에 기록을 해서 공유하자라는 구체적인 솔루션을 제공해 줄 수 있다. 5. 서로 Try 공유 이번에도 돌아가면서 자기가 작성한 Try를 읽고 진행자에게 넘겨주면 진행자는 포스트잇을 화이트보드 Try 영역에 붙인다. 여기서도 또한, 이해가 안 되는 게 있으면 작성자에게 물어볼 수 있다. 6. 앞으로 적용할 Try &amp; Action 선정 Try를 다 공유했다면 다음 KPT 전까지 Action을 취해서 개선할 수 있도록 Try를 선정해야 된다. 한 사람당 2 ~ 3개의 투표권을 가지고 각자 마음에 드는 Try에 투표를 진행한다. 그러면 가장 문제가 있는 Try가 상위권에 올라오게 된다. Try를 많이 뽑게 되면 오히려 집중력이 떨어지기 때문에 한 번의 KPT에서 Try는 2 ~ 3개가 적당하다. 그리고 각 Try마다 담당자를 선정해 적어두면 된다. 예를 들어, ~~을 하면 문서화해서 노션이나 구글 docs에 기록을 해서 공유하자가 Try로 선정되었다면 담당자가 이 Try에 대해 문서화할 양식 및 공유 방법(노션 or docs)에 대해 문서로 정리해 팀에 공유하면 된다. (추가적으로 다음 KPT까지 담당자가 Task를 완료할 수 있도록 팀 리더가 일정 기간이 지나면 진행 상황에 대해 체크할 필요가 있다.) 다음 글에 Action Item에 대해 상세한 예들이 있으니 참고해 보자! 팀 문화의 탄생 KPT 진행 팁 및 주의할 점 남 탓을 하지 말자 남 탓만 해서는 변화가 있을 수 없다. 내가 먼저 변화된 모습을 보여줘야 설득력 있음. 진척 회의와 회고를 분리하자 회고는 어떻게 하면 효율적으로 일할 수 있는 것인가?라는 일의 진행 방식에 대해 논하는 것 일의 진행 상황과 보고는 여기서 논하지 말고 자신들의 업무 방식을 제3자 입장에서 객관적인 시선으로 봐보자 회고 기간 회고는 1 주일 단위 정도에서 실시하면 좋다 첫 회고 같은 경우 길어질 수 있지만, 1주일 단위가 정착하면 회고 자체를 짧은 시간에 할 수 있다. 회고을 지속적으로 진행시켜 나가면 서서히 회고 방법 자체도 능숙해지게 된다. 지난주보다 이번 주, 이번 주보다 다음 주에 점점 더 잘 하게 되는 것 회고의 ‘습관화’에서 ‘일상화’를 목표로 회고을 계속 이어 가면, 팀은 스스로 현장을 개선해 나갈 것이다라는 의식이 싹트게 된다. 1단계 목표는 정기적인 ‘회고’를 습관화하는 것 다음 단계가 되면, 회고 자체가 당연하게 되어, 일상 업무 속에서 항시적으로 할 수 있게 된다. 그렇게 ‘회고 ‘를 일상화 해 버리는 것이 두 번째 단계 Keep이 나오지 않는다. 자신의 프로젝트에 좋은 점은 하나도 없고, 문제투성이 혹은 이상이 너무 높거나 원래 이전 회고에서 나온 Try가 전부 제대로 되지 않았다면 Try를 낸 방법 자체에 문제가 있는 것이므로 그것부터 개선 필요 Problem이 고민 상담이 되는 경우 Problem을 낼 때 단순히 고민하고 있는 것을 말하는 것으로 Try가 도출되지는 않는다. 실제로 일어난 불편한(안 좋은) 부분을 내자 Try가 구체적이어야 한다. Try를 생각할 때 “기분”을 반영시켜 버리는 것은 해서는 안 될 일이다. 다음 KPT의 회고 때 무엇이 되어 있고 무엇이 안 되어 있는지 명확하게 판정할 수 없다. 참고: 소프트웨어 개발자가 ‘소프트 스킬’ 쌓는 방법 KPT하는 스타트업은 성장한다 실수와 문제가 없는 스타트업은 없다 팀 문화의 탄생 회고에 대한 고찰 - KPT 진행의 노하우에 대하여 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "agile Retrospective KPT team etc reminiscence",
    "url": "/etc/reminiscence/2023-01-28-KPT/"
  },{
    "title": "JPA - 프록시(Proxy)",
    "text": "다음과 같이 회원 이름만 출력하는 코드가 있을 때 과연 멤버와 팀을 다 조회할 필요가 있을까? 조회해도 되지만 굳이 불필요하다는 걸 알 수 있다. 어떻게 개선할 수 있을까? 이를 위해 프록시(Proxy), 지연 로딩(Lazy Loading), 즉시 로딩(Eager Loading)에 대해 한번 알아보자. public void printUser(String memberId) { Member member = em.find(Member.class, memberId); Team team = member.getTeam(); System.out.println(\"회원 이름: \" + member.getUseranme()); } 프록시(Proxy) 프록시는 대리(행위), 대리권, 대리인 등의 의미를 가지는데 여기서는 프록시 객체가 실제 객체의 참조를 보관하다 실제 사용할 때 실제 객체에 접근한다고 생각하면 된다. 프록시의 특징으로는 실제 클래스를 상속 받아서 만들어졌다. 실제 클래스와 겉 모양이 같다. 프록시 객체는 실제 객체의 참조(target)를 보관한다. 프록시 객체를 호출하면 프록시 객체는 실체 객체의 메소드 호출한다. em.find() vs em.getReference() em.find()는 데이터베이스를 통해서 실제 엔티티 객체를 조회해오고, em.getReference()는 데이터베이스 조회를 미루는 가짜(프록시) 엔티티 객체를 조회해온다. ... Member findMember = em.find(Member.class, member.getId()); System.out.println(\"findMember: \" + findMember.getClass()); // -&gt; findMember: Member 객체 ... Member refMember = em.getReference(Member.class, member.getId()); System.out.println(\"refMember: \" + refMember.getClass()); // -&gt; refMember: 프록시 객체(ex) Member$HibernateProxy$odcVHpjy) 프록시 객체 초기화 과정 Member member = em.getReference(Member.class, “id1”); member.getName(); getName() 호출 (처음에 Member target에 값이 없으면 2번 진행, 있으면 5번) JPA가 영속성 컨텍스트에 초기화 요청 영속성 컨텍스트가 DB를 조회한 후 실제 Entity를 생성해서 target에 연결 target.getName()으로 진짜 Member의 getName을 접근해 이름을 반환 프록시의 특징 프록시 객체는 처음 사용할 때 한 번만 초기화 처음 한 번만 초기화하면 target에 값이 채워져, target에 값이 있을 때부터는 바로 5번(초기화 과정 5번)을 실행한다. 즉, DB에 쿼리를 또 날리지 않는다. 프록시 객체를 초기화할 때, 프록시 객체가 실제 엔티티로 바뀌는 것은 아님, 초기화되면 프록시 객체를 통해서 실제 엔티티에 접근 가능 프록시 객체를 초기화하고 getClass를 해봐도 그대로 프록시 객체임 프록시 객체는 원본 엔티티를 상속받음, 따라서 타입 체크 시 주의해야 함 원본 엔티티와 ==를 하면 비교 실패할 수가 있기 때문에 instance of를 사용 영속성 컨텍스트에 찾는 엔티티가 이미 있으면 em.getReference()를 호출해 도 실제 엔티티 반환 Member findMember = em.find(Member.class, member.getId()); System.out.println(\"findMember: \" + findMember.getClass()); Member refMember = em.getReference(Member.class, member.getId()); System.out.println(\"refMember: \" + refMember.getClass()); Systemout.println(\"findMember == refMember:\" + (findMember == refMember)); // -&gt; findMember: Member 객체 // -&gt; refMember: Member 객체 // -&gt; findMember == refMember: true 분명 em.getReference() 하면 프록시가 나온다 했는데 왜 실제 엔티티가 나왔을까? JPA에서는 같은 인스턴스의 ==에 대해 같은 영속성 컨텍스트 안에서 조회하면 항상 같다고 나옴.(이게 실제 객체든 프록시든 간에 상관없이 마치 자바 컬렉션에서 가져온 걸 == 비교하듯이) 이미 멤버를 영속성 컨텍스트(1차 캐시)에 올려놨는데 굳이 프록시로 받을 필요가 없어서(프록시로 받으면 오히려 손해) 그럼 반대로 em.getReference()를 먼저 하고 뒤에 em.find()를 하게 되면 어떻게 될까? 둘 다 프록시 객체가 반환되게 된다. 영속성 컨텍스트의 도움을 받을 수 없는 준영속 상태일 때, 프록시를 초기화하면 문제 발생 Member refMember = em.getReference(Member.class, member.getId()); System.out.println(\"refMember: \" + refMember.getClass()); // Proxy em.detach(refMember); //영속성 분리 refMember.getUsername(); //Exception 발생! 하이버네이트는 org.hibernate.LazyInitializationException 예외를 터트림 프록시 확인 EntityManagerFactory로 부터 getPersistenceUnitUtil() 프록시 인스턴스의 초기화 여부 확인 PersistenceUnitUtil.isLoaded(Object entity) 초기화 시 true 프록시 클래스 확인 방법 entity.getClass().getName() ex) Entity$HibernateProxy$QPwAxVTy 프록시 강제 초기화 org.hibernate.Hibernate.initialize(entity); 참고: JPA 표준은 강제 초기화 없음 강제 호출: member.getName() 근데 실제로는 getReference 이런 건 잘 쓰지 않는다. 하지만 이제 즉시 로딩과 지연 로딩에 대해 설명할 건데 앞의 프록시 매커니즘을 잘 알고 있어야 즉시 로딩, 지연 로딩에 대해 깊이 있게 이해할 수 있기 때문에 자세하게 설명했다. JPA가 제공하는 지연 로딩과 즉시 로딩에 대해 알아보자. 지연 로딩(Lazy Loading), 즉시 로딩(Eager Loading) 지연 로딩 (FetchType.LAZY) 단순히 member 정보만 사용하는 비즈니스 로직이 있다고 하면 Member를 조회할 때 Team도 함께 조회한다고 하면 손해이다. 그래서 JPA는 지연로딩이라는 Option을 제공한다. @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; @ManyToOne(fetch = FetchType.LAZY) @JoinColumn(name = \"TEAM_ID\") private Team team; .. } 위와 같이 FetchType.LAZY를 주게 되면 Member를 조회하더라도 Team은 프록시 객체로 조회하게 되어서 나중에 실제로 team을 사용하는 시점에 팀을 초기화(DB 조회) 할 수 있다. 즉시 로딩 (FetchType.EAGER) 하지만 Member와 Team을 자주 함께 사용한다고 하면? 즉시 로딩이라는 Option도 사용할 수 있다. @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; @ManyToOne(fetch = FetchType.EAGER) @JoinColumn(name = \"TEAM_ID\") private Team team; .. } 위와 같이 FetchType.EAGER를 주게 되면 Member 조회시 항상 Team도 조회하게 된다. JPA 구현체는 가능하면 조인을 사용해서 SQL 한번에 함께 조회한다. 프록시와 즉시 로딩 주의 가급적 지연 로딩만 사용하자(특히 실무에서) 즉시 로딩을 적용하면 예상하지 못한 SQL이 발생한다. 어떤 엔티티를 조회하면 즉시 로딩으로 연결된 엔티티를 다 Join 하기 때문에 즉시 로딩은 JPQL에서 N+1 문제를 일으킨다. N+1: 최초 쿼리를 한 개 날렸는데 추가로 N 개가 나가는 것 @ManyToOne, @OneToOne은 기본이 즉시 로딩 LAZY로 설정해 주자 @OneToMany, @ManyToMany는 기본이 지연 로딩 지연 로딩 활용 - 실무 모든 연관관계에 지연 로딩을 사용하자! 실무에서 즉시 로딩을 사용하지 말자! JPQL fetch 조인이나, 엔티티 그래프 기능을 사용하자! 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA Proxy Lazy Eager study jpa",
    "url": "/study/jpa/2023-01-21-proxy/"
  },{
    "title": "우아한 글쟁이 되기 세미나 정리 및 후기",
    "text": "이 글은 우아한 테크 세미나에서 강연을 하신 김지헌(허니몬)님의 우아한 글쟁이 되기에 대한 정리 및 후기입니다. 참고 링크: https://www.youtube.com/watch?v=E8J4RKoGpf8&amp;list=PLgXGHBqgT2TtGi82mCZWuhMu-nQy301ew 작년에 블로그에 글을 한 80개 정도 작성한 거 같은데 작성하면 할수록 글쓰기에 대한 욕심이 더 커지는 것 같다. 최근에 우테코 5기 프리코스를 하면서 많은 사람들의 회고나 정리 글을 읽어 봤는데 글을 잘 쓰시는 분들이 너무 많아서 배울 것도 많았고 느낀 점도 많았다. 나도 글을 더 잘 써서 누구나 더 읽기 쉽고 재밌는 글을 제공하고 싶다는 생각이 들었다. 작년 회고에도 적었지만 이번년도 목표 중 꾸준히 기록하고 정리하기라는 목표가 있기 때문에 좀 더 체계적이고 전문적으로 그 기반을 다지고 싶어서 고민하던 중 해당 세미나를 접하게 되었고 계속해서 보기 위해 정리 및 후기를 작성 하였다! High Tech - High Touch 현재 디지털의 영향으로 스마트폰에서 엄청난 시각적 콘텐츠가 쏟아지고 있다. 이에 따라 앞으로 페이스북 담벼락에서 글자가 사라질 수도 있고 문예지의 폐간, 신문의 종말을 예언하는 분석도 잇따른다. 하지만 그런 와중에 SNS와 다양한 매체를 통해 자신을 표현하고 싶어 하는 욕구도 커지고 있다. 그래서 브런치나 트위터 등을 통해 자기의 생각이나 느낀 것을 표현하는 것을 많이 갈망하고 있다. 다른 사람의 생각과 글을 만나고 소통을 하고 서로 피드백도 주고 하다 보면 어느새 기분전환도 된다는 것이다. 이는 우리 삶에 더 많은 첨단 기술(하이 테크)을 도입할수록 더 많은 높은 감성(하이 터치)을 갈망하게 된다는 미래학자 존 네이스비트가 말한 High Tech - High Touch 원리와 같다고 할 수 있다. 글이 사라지는 시대, 글로 소통하는 이유를 한번 읽어 보면 좋을 것 같다. ‘글쓰기’는 자신의 생각을 표현할 수 있는 가장 ‘효과적인 방법’이다. 말을 잘하지 않아도, 부끄러움이 많아도 글쓰기는 자기가 생각한 의도를 충분하게 생각한 뒤에 명확하게 정리해서 전할 수 있기 때문에 가장 효과적으로 생각을 표현할 수 있다. 그리고 후에 수정하거나 삭제도 가능하다. 자신만의 ‘기록 체계’를 구축하자! One Source! Multi Write! 하나의 소재를 글을 쓸 수 있는 곳이라면 어디에서든 쓸 수 있게 자신만의 ‘기록 체계’를 구축하자. 예를 들어서 회사 맥북, 스마트폰 회사에서 잠깐 시간 날 때나, 혹은 출퇴근이나 이동할 때 잠깐잠깐식 글을 쓸 수 있게 환경 구축 개인 PC, 개인 랩탑 집에서나 카페 등 편안하게 할 수 있는 곳에서도 글을 작성할 수 있게 환경 구축 위처럼 구축하게 되면 집, 이동, 카페, 회사 등 어디에서든 여유가 있을 때마다 작성할 수 있다. ‘글감(Source)’을 모으자 이제 글을 쓰려고 하면 ‘글감’ 즉, ‘소재’를 모아야 하는데 어떻게 모을 수 있을까? 글을 쓰려고 할 때 소재를 정하는 것이 아니라 평소에 괜찮은 정보나 관심있는 정보들을 봤을 때 미리미리 글감들을 저장해놓자! 웹 클리퍼를 이용해서 글감 수집하면 편함 구글 Keep Evernote Web Clipper Notion ‘양식(Template, 틀)’을 이용하자 ‘글쓰기’는 목적(회의록, 메일, 블로그 등)에 따라 그 양식이 달리 지기 때문에 적절한 양식을 만들어두고 이를 활용하면 편하다. 예)보고서 개요: 전달하고자 하는 내용을 한눈에 볼 수 있게 정리 상세: 그 내용을 풀어 쓴다. 기 승 전 결 정리: 추가적인 의견 등 기재 참고 예)블로그 운을 띄운다. 필요한 경우 결론이나 전하고 싶은 글을 쓴다 말하고자 하는 의도에 맞춰 머리말을 작성한다. 머리말 사이를 채운다 전달하는데 적절한 코드, 이미지 등을 사용 한번 읽어본다. 수정한다. 읽어본다. 공유한다. ‘필력’은 많이 읽고 많이 써야 는다. 글쓰기에 대한 방법만 안다고 해서 바로 잘 쓸 수 있을까? 아니다. 글쓰기도 수많은 글을 읽어보기도 하고 직접 써보기도 하면서 직접 겪어봐야 늘 수 있다. 직접 쓰는 것뿐만 아니라 다른 사람의 글을 많이 읽어보는 것만으로도 엄청난 도움이 될 수 있기 때문에 책을 많이 읽도록 해보자! 글쓰기 도구 아래와 같이 많은 글쓰기 도구들이 있으니 자기한테 맞는 것을 찾아 써보자. 허니몬님은 Notion + VS Code를 사용하고 계시고 현재 저도 마찬가지로 Notion + VS Code를 사용하고 있는데 강추합니다! 구글 드라이브 LibreOffice Ultra text editor Notepad++ Haroopad ATOM Visual Studio Code Evernote Notion 좋은 글 쓰기 꾸준하게 쓰기 계속 쓰기 포기하지 않고 쓰기 지칠 때 쉬었다 쓰기 쓰기 싫을 때는 놀러가서 쓰기 ‘글쓰기’가 능숙해진다면 조금 더 많은 기회가 찾아온다. 개발자에게 개발 실력 뿐 아니라 문서화 능력 즉, 글쓰기 능력도 매우 중요하다. 그리고 부가적으로 그 글쓰기 능력으로 인해 출판 제의, 강의 제의, 이직 제안 등 조금 더 많은 기회가 찾아올지도 모른다 ㅎㅎ. 그러니 우리 모두 꾸준히 글쓰기 실력도 늘려보자! 후기 정확히 내가 원하던 것(세부적으로 글을 어떻게 잘 쓸 수 있는지?) 과는 약간 방향성이 달랐지만 글쓰기에 베이스가 될 수 있는 큰 틀을 마련하고 엇갈려 있는 정보를 정돈하는데 매우 좋았다. 지난 1년 동안 글쓰기를 하는 동안 실제로 지속적이고 체계적이게 매일 기록할 필요가 있음을 느끼고 있었고, 그리고 그 글쓰기 양식 또한 어느 정도 만들어야 편하겠다고 느끼고 있었지만 간단하게만 적용하고 있어 약간 뒤죽박죽이었는데 이 세미나를 보고 깔끔하게 정돈된 것 같고 앞으로 나아갈 방향이 틀리지 않았단 것에 안심했다. 이를 바탕으로 다시 꾸준하게 글을 써보며 글쓰기 실력을 늘려보려한다! *오타가 있거나 피드백 주실 부분이 있으면 편하게 말씀해 주세요.",
    "tags": "seminar writing etc",
    "url": "/etc/seminar/2023-01-14-writing/"
  },{
    "title": "Nginx는 무엇이고 왜 사용할까?",
    "text": "Nginx는 웹서버 점유율 1등으로 네이버, 카카오, 깃허브, 넷플릭스 등 다양한 곳에서 사용하고 있다. 어떻게 Apache를 제치고 Nginx가 많이 사용하게 되었는지 알아보자 Nginx Nginx는 비동기 이벤트 기반 구조의 경량 웹 서버이다. 정적 파일 제공해 주는 웹 서버로 활용되기도 하고 프록시 서버 및 로드밸런서로 활용되기도 한다. Nginx vs Apache Apache와는 어떤 차이점이 있길래 Apache를 제치고 1등이 되었을까? Apache 동작 방식 Apache는 새로운 클라이언트 요청이 들어오면 새로운 프로세스나 쓰레드를 생성하여 처리했다. 하지만 프로세스를 생성하는 시간이 오래 걸리기 때문에 요청이 들어오기 전 프로세스를 미리 생성하는 PREFORK 방식을 사용한다. 만약 만들어 놓은 프로세스가 모두 할당되었다면 추가로 만든다. 이런 구조는 확장성이 좋아 개발하기 쉽다는 장점이 있다. 모듈이라는 개념으로 수많은 기능을 덧붙일 수 있다. 이 모듈을 통해 다른 프로그램과의 연동도 가능하다. 하지만 1999년대부터 사용자 수가 급격히 늘어나면서부터 문제가 생기기 시작했는데 서버에 동시에 연결된 커넥션이 많아졌을 때 더 이상 커넥션을 형성하지 못하는 C10K(Connection 10000 Problem) 현상이 일어나기 시작했다. 이 C10K 현상은 아파치 서버의 구조가 주 문제였다. 첫 번째로 커넥션이 형성될 때마다 프로세스가 할당되기 때문에 메모리가 부족하였고 두 번째로 위에서 말한 확장성이라는 장점이 오히려 단점이 되어 무거운 프로그램이 되었다. 그리고 세 번째로 많은 커넥션 요청이 들어오면 CPU는 계속해서 프로세스를 바꿔가며(Context Switching) 일해야 됐기 때문에 CPU에 부하가 계속해서 증가했다. -&gt; 즉, 아파치의 구조는 수많은 동시 커넥션을 처리하기에는 부적합했다고 할 수 있다. Nginx 동작 방식 트래픽이 증가하면서 Apache의 한계를 극복하기 위해 Nginx가 등장하였다. Nginx는 *Event-Driven 구조로 동작하기 때문에 한 개의 프로세스만 생성하여 사용하고 비동기 방식으로 요청들을 동시적으로 처리할 수 있다. Event-Driven: 자동으로, 혹은 정해진 순서에 따라 발생하는 게 아니라 어떤 일에 대한 반응으로 일어나는 구조 Nginx의 구조 Nginx는 하나의 master process와 다수의 Worker Process로 구성될 수 있다. master process는 설정 파일을 읽고 설정에 맞게 worker process를 생성한다. 이 worker process가 생성될 때 각자 지정된 listen 소켓을 배정받는다. 그리고 그 소켓에 클라이언트의 여러 요청들을 받고 처리할 수 있다. 이런 connection 형성, 제거, 새로운 요청을 처리하는 것을 event라고 한다. 이 event들을 OS 커널이 queue 형식으로 worker process에게 전달해 준다. 이 event는 queue에 담긴 상태에서 worker process가 처리해 줄 때까지 비동기 방식으로 대기하다가 worker process가 하나의 쓰레드로 event를 꺼내 처리한다. -&gt; 이렇게 되면 worker process가 쉬지 않고 계속해서 일을 하기 때문에 자원을 효율적으로 사용 가능(apache는 요청이 없다면 프로세스가 방치된다.) +시간이 오래 걸리는 작업 같은 경우 Thread Pool에 event를 위임하고 큐 안의 다른 event를 처리한다. 이러한 worker process는 보통 CPU의 코어 개수만큼 생성하기 때문에 Context Switching 사용도 줄일 수 있다. 하지만 개발자가 기능 추가를 시도했다가 돌아가고 있는 워커 프로세스를 종료하게 되면 해당 워커 프로세스가 관리하고 있던 커넥션과 관련된 요청을 더 이상 처리할 수 없게 되는 문제가 생기게 된다. 그래서 Nginx는 개발자가 직접 모듈을 만들기가 까다롭다는 단점이 있다. 하지만 단점을 커버하는 아래와 같은 장점들이 있다. 동시 커넥션 양 최소 10배 증가(일반적으로 100 ~ 1000배 증가) 동일한 커넥션 수일 때 속도 2배 향상 동적 설정 변경 ex) 운영 도중에도 서버 추가 가능 Apache Multi Processing Module(Apache MPM) 이에따라 최근에 Apache도 MPM(Multi Processing Module)이라는 모듈을 추가해서 성능을 개선하게 되는데 MPM 이란 Apache 서버를 어떤 방식으로 운영할지 선택할 수 있게 해주는 모듈이다. 안정성이나 하위 호환이 필요하다면 기존의 PREFORK 방식, 성능 향상이 필요하다면 WORKER라는 스레드를 만들어 WORKER가 요청을 처리하는 방식으로 사용할 수 있다. 그러면 무조건 이제 Nginx를 써야되는거야? 앞에서 Nginx를 계속해서 칭찬하긴 했지만 아직까지 누군가가 압도적으로 점유하고 있는 게 아니고 Nginx와 Apache가 호각으로 1, 2등을 다투고 있다. 그 이유는 Apache도 Nginx보다 오래전부터 업데이트를 해왔기 때문에 그만큼 안전성, 확장성 측면에서는 Nginx보다 더 뛰어나다. 그렇기 때문에 상황에 맞게 유동적으로 선택하면 될 것이다. 애초에 이 두 웹서버는 탄생하게 된 이유부터 다르다. 아파치 때는 안정성과 확장성이 중요했고 Nginx 때는 트래픽이 빠르게 치고 올라오던 시절이라 동시 커넥션이 중요했기 때문이다. Nginx와 Apache에 대해 알아봤으니 이제 Nginx가 제공해 주는 기능들에 대해 한번 알아보자 리버스 프록시(Reverse Proxy) 클라이언트 요청을 대신 받아 내부 서버로 전달해 주는 것을 리버스 프록시라고 하는데 서버 정보를 클라이언트로부터 숨겨줍니다. 예를 들어 아래처럼 구성하여 /로 요청 시 3000번 포트로 매핑해주고 /api로 요청하면 8080번 포트로 매핑해줄 수 있습니다. 로드밸런싱(Load Balancing) 하나의 서버에서 받는 요청을 여러 대의 서버가 분산 처리할 수 있도록 요청을 나누어 주어 성능, 확장성 및 신뢰성을 향상시킬 수 있다. 캐싱(Caching) 클라이언트가 요청한 내용을 캐싱 하여 같은 요청이 오면 캐시에 저장된 내용을 전송해 전송 시간을 절약할 수 있고 불필요한 외부 전송을 막을 수 있다. 참고: 우테코 테코톡(Nginx) *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Nginx Apache web-server proxy project",
    "url": "/project/2023-01-07-nginx/"
  },{
    "title": "애송이의 2022년 회고 (feat. 우테코 5기 최종 결과)",
    "text": "일 년의 회고는 처음 작성해 보는데 일단 개발자가 되기로 마음먹은 지 첫 1년이 되는 해이기도 하고 올해 많은 일이 있었기 때문에 회고해 보고자 한다! 일단 다들 2022년 회고 1. Java, Spring, Clean Code, 객체 지향 등 이론 공부 개발을 백엔드 Spring 쪽으로 시작하기로 마음먹었지만 내가 진로 방향을 늦게 잡은 편이라 최적의 시간으로 성장을 하고 싶었기 때문에 돈을 주고 빠르게 지식을 습득할 수 있는 인강 커리큘럼을 찾으려고 했다. 그렇게 직접 들어보면서 나에게 맞는 강의를 찾다 내 개발 인생의 이정표인 김영한 선생님을 만나게 되었다.. 제일 처음 다음과 같은 강의로 접하게 되었는데 친절한 설명, 말이 필요없는 강의 퀄리티, 깔끔한 강의 자료, 꼼꼼한 QnA 답변까지 모두 마음에 들어 영한님 커리큘럼을 따르기로 결정했고 지금 아래 그림과 같은 상황이다…ㅋㅋㅋㅋ 많은 강의를 듣고 나서도 확신하여 말할 수 있는 것은 절대 후회 없다는 것이다. 어떤 기술을 설명할 때 제일 처음 Version부터 V1, V2, V3 순으로 차근차근 버전 업 시켜가며 보여주는데 얼마나 시간을 많이 쏟으시며 연구하신지 감이 안 잡힌다.. 배민 개발 팀장으로도 팀을 이끌고 계셔서 정말 바쁘실 텐데 👍 현재 더 많은 강의가 추가로 나왔는데 내가 많이 궁금해하던 부분(JdbcTemplate, MyBatis, JPA 데이터 접근 비교, AOP 등)이 강의들 안에 있는 걸 보고 충동구매해버릴 뻔했다.. 하지만 강의만 듣는다고 해서 그것이 내 것이 되는 게 아니다. 그래서 일단 현재 들은 것들을 우선순위로 하여 몸에 체득시키고 나면 그다음 들을 예정이다! 그리고 Spring 공부를 하다 보니 다시 한번 베이스 언어인 Java의 중요성을 깨닫고 Java 공부를 다시 하며 그에 따라 동시에 CleanCode, TDD, 객체 지향 등에 대해 공부를 하는 좋은 시간이었다. 하지만 역시 아직 뉴비의 입장에서는 어렵고 잘 모르겠는 부분도 많아서..ㅠㅠ 앞으로 계속 반복하여 읽고 체득할 필요가 있겠다. 내년의 나 힘내자! 💪 이번 연도 구입한 책! 객체 지향의 5가지 원칙(SOLID)에 관심 있다면 다음 글을 추천드립니다! 예를 들어가며 SOLID에 대해 알아보자(1) 예를 들어가며 SOLID에 대해 알아보자(2) 2. 프로젝트 진행 이론만 공부한다고 과연 그 기술을 안다고 할 수 있을까? 직접 적용해 보며 체득하기 위해 프로젝트들을 진행했었는데 이번 연도에는 총 3개의 프로젝트를 진행했었다. 스터디, 모임 서비스 개인 프로젝트 12월 ~ 2월 진행 학교 개발자 커뮤니티 서비스 팀 프로젝트 3월 ~ 11월 진행 마스크 소개팅 서비스 팀 프로젝트 8월 ~ ing..(진행중) 프로젝트들을 진행하면서 Spring, JPA, Querydsl, Junit 등을 직접 적용해보며 한층 더 성장해볼 수 있었고 다음과 같은 경험들도 해보는 등 이론만 공부 해서는 알 수 없는 것들을 많이 경험, 고민했던 뜻깊은 시간들이었다. Git Branch 전략 어떤 깃 브랜치 전략을 사용해야 할까? Docker 왜 도커(docker)를 사용할까? Spring Security Spring Security 과정을 이해해보자 Spring Security를 구현해보자 비동기 처리 쿼리 개선 JWT 적용 JWT를 선택한 이유(+Redis) JWT를 좀 더 안전하게 저장해보자 코드 리뷰 문화 도입 지속 가능한 SW 개발을 위한 코드리뷰 세미나 정리 및 후기 Spring Rest Docs 도입 WebSocket, STOMP, SockJS WebSocket, STOMP, SockJS에 대해 알아보자 … 3. 꾸준하게 성장하기!! 기술은 언제든지 더 좋은 게 나올 수 있기 때문에 개발자는 지속적으로 공부할 필요가 있다! 그러기 위해서 미리부터 시스템을 구축하여 꾸준히 성장하기로 마음먹었었는데 그 덕분에 이번 연도 큰 슬럼프가 오지 않고 잘 버티며 성장할 수 있었던 것 같다. 🙃 꾸준한 블로깅(주 1회 이상) 2022년 총 80개 작성! 꾸준한 깃허브 commit 4. 정보 처리 기사 취득 이번 연도는 정말 바쁜 때여서 정처기를 동시에 진행할까 말까 정말 고민을 많이 했었는데 결과적으로 정말 잘했다는 생각이 든다! 나중에 취득할 생각 하니깐… 음.. 말을 아끼겠습니다ㅋㅋㅋ 그래도 정처기를 하면서 꽤나 도움이 됐다고 생각하기 때문에 만약 컴퓨터 자격증 중에 하나를 취득해야 된다면 정처기를 추천합니다. 5. 우테코 5기 최종 합격!!!!! … 우테코는 사실 나에게 유니콘과 같은 부류의 상상의 영역 쪽이었다. 설명회를 들으면서 저기서 크루들과 같이 코딩하는 상상만 해도 진짜 설레고 가슴이 뛰었었는데 내가 합격이라니.. 😳 솔직히 지금 합격이 발표된 지 며칠이 지났는데도 아직까지 실감이 잘 안 나고 우테코 생각만 하면 심장이 빠르게 뛸 만큼 너무 기쁘고 설렌다. 우테코 지원 과정에 대한 후기는 다음과 같이 작성해놨으니 생략하겠다! 우테코 5기 프리코스 1주차 회고 우테코 5기 프리코스 2주차 회고 우테코 5기 프리코스 3주차 회고 우테코 5기 프리코스 4주차 회고 우테코 5기 최종 코딩 테스트 회고 혹시 궁금하거나 물을 게 있다면 댓글이나 이메일(pjhg410@gmail.com)로 보내주시면 답해드리겠습니다! 2023년 목표 요즘 다른 분들의 글쓰기나 회고 등을 많이 읽어보는데 정말 글도 잘 쓰시고 열심히 사시는 분들이 많아 더 열심히 노력해야겠다는 생각이 든다. 나도 다른 분들께 좋은 영향을 줄 수 있는 좋은 개발자가 되고 싶다! 1. 우테코 5기 열심히 활동하기 나에게 정말 좋은 기회가 주어진 만큼 내년은 우테코에 모든 걸 올인하려고 한다. 우테코에서 만날 크루들과 열심히 미션도 하고 싶고, 페어 프로그래밍도 하고 싶고, 팀 프로젝트도 하고 싶고, 같이 수다도 떨고 싶고, 같이 운동도 하고 싶고… ㅋㅋㅋㅋㅋㅋㅋ 정말 하고 싶은 게 많다. 크루 분들.. 저랑 같이 많이 놀아주세요ㅠㅠ 2. 인적 네트워킹 쌓기 솔직히 말하자면 부끄럽지만 지금까지 팀보다는 조금 더 개인플레이로 인생을 많이 살아온 것 같다.. 그에 따른 장단점이 있지만 개인적으로 단점이 좀 더 커서 사람들과 많이 만나며 인적 네트워킹을 쌓아보고 싶다. 그러기 위해선 지금 많이 위축되어 있는 나 자신과 성격을 고쳐야 될 것 같은데 쉽지 않아 보인다..ㅜㅜ 힘내보자 💪 3. 꾸준히 운동하기 작년 초중반까지는 꾸준히 헬스를 했었는데 2학기 시작할 때부터 일정이 엄청나게 바쁘기 시작해서 운동하는 날이 불규칙해졌고 밥도 생략하는 적이 많아졌다. 요즘 개인적으로 헬스보단 재밌게 할 수 있는 클라이밍, 수영, 스카이다이빙 등 이런 쪽에 관심이 생겨서 이런 쪽으로 취미를 만들어 보고 싶다! 이번 우테코 크루들과 같이 코딩도하고 운동 같은 취미 생활도 같이 공유해보며 재밌게 보내보고 싶다. 4. 꾸준히 기록하고 정리하기 나는 원래 글이나 책과 친하지 않았기 때문에 블로그를 하기 전에는 글에 전혀 관심이 없었다. 하지만 블로그를 자꾸 적다 보니 어떻게 하면 다른 사람이 더 이해하기 쉽게 쓸 수 있을까? 더 재밌게 쓸 수 있을까 생각을 하게 되고 재미를 느끼게 되었다. 하지만 뭔가 항상 적을 때 아쉽고 기억이 안 나는 부분이 있어서 글을 적고 나중에서야 아 그것도 적을 걸! 하곤 했던 기억이 난다. 이를 위해 매일 그날의 일을 따로 기록하여 나중에 적을 때 좀 더 편하고 체계적이도록 개선해 봐야겠다! +추가적으로 말도 잘하고 싶다..ㅠㅋㅋㅋ 사람들 앞에 서면 긴장해서 머리가 하얗게 된다ㅠㅠ 5. 마인드 리셋 다른 분들의 글을 읽다가 정말 좋은 게 있어서 나도 꼭 다음과 같이 바꿔보고 싶어 적어봤다. 진짜 나의 행복 찾기 자신감 가지고 행동하기 타인의 행복을 진심으로 바라고 응원할 수 있는 긍정적인 사고 가지기 여기까지 읽어주셔서 감사합니다. 내년에도 더 나은 제가 될 수 있도록 열심히 노력 하겠습니다. 다른 분들도 고생 많으셨고 내년에도 화이팅입니다! 💪💪 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "2022 reminiscence woowacourse etc",
    "url": "/etc/reminiscence/2022-12-31-reminiscence/"
  },{
    "title": "예를 들어가며 SOLID에 대해 알아보자(2)",
    "text": "이전 글을 안 읽고 오셨다면 읽고 오시는 것을 추천드립니다. 3. 리스코프 치환 원칙(Liskov Substitution Principle) 리스코프 치환 법칙은 앞서 설명한 개방 폐쇄 원칙을 받쳐 주는 다형성에 관한 원칙을 제공한다. 리스코프 치환 원칙이 지켜지지 않으면 다형성에 기반한 개방 폐쇄 원칙 또한 지켜지지 않기 때문에 중요하다. 상위 타입의 객체를 하위 타입의 객체로 치환해도 정상적으로 동작해야 한다. 간단하게 예를 들어보면 //상위 타입 SuperClass, 하위 타입 SubClass public void method(SuperClass sc){ sc.someMethod(); } 위처럼 코드가 있을 때 아래처럼 객체를 전달해도 정상적으로 동작해야 되는 것이 리스코프 치환 원칙이다. someMethod(new SubClass()); 리스코프 치환 원칙을 지키지 않았을 때 문제 a. 직사각형-정사각형 문제 정사각형(Square)을 직사각형(Rectangle)의 특수한 경우로 보고 상속받도록 구현을 했다고 가정을 하자. 그래서 정사각형의 경우 setWidth()나 setHeigh()가 길이가 같도록 재정의 하였다. public class Rectange { private int width; private int height; public void setWidth(int width) { this.width = width; } public void setHeight(int height) { this.height = height; } public int getWidth() { return width; } public int getHeight() { return width; } } public class Square extends Rectangle { @Override public void setWidth(int width) { super.setWidth(width); super.setHeight(width); } @Override public void setHeight(int height) { super.setWidth(height); super.setHeight(height); } } 그 다음 이제 Rectangle을 이용해 코드를 구현해보자 public void increaseHeight(Rectangle rec) { if (rec.getHeight() &lt;= rec.getWidth()){ rec.setHeight(rec.getWidth() + 10); } } increaseHeight() 메소드의 의도는 height가 width보다 작거나 같은 경우 높이를 늘리는 것이다. 근데 파라미터에 Square 객체가 전달되면 이 의도는 깨지게 된다. Square 같은 경우는 높이와 폭을 다 같게 만들기 때문에 이 메소드를 실행해도 원래 의도와 다르게 된다. 여기서 직사각형, 정사각형 문제는 개념적으로 상속 관계에 있더라도 실제 구현할 때는 상속 관계가 아닐 수도 있다는 것을 보여준다. increaseHeight() 같은 기능이 필요하다면 실제로는 상속받아 구현하는 것이 아닌 별개의 타입으로 구현해 줘야 된다. b. 상위 타입에서 지정한 리턴 값의 범위에 해당하지 않는값 리턴 public class CopyUtil { public static void copy(InputStream is, OutputStream out) { byte[] data = new byte[512]; int len = -1; while ((len = is.read(data)) != -1) { out.write(data, 0 , len); } } } InputStream의 read() 메소드는 스트림 끝에 도달해 더 이상 데이터를 읽어 올 수 없을때 -1을 리턴한다고 정의되어있고 CopyUtil.copy() 메소드는 이 규칙에 따라 is.read()의 리턴 값이 -1이 아닐 때까지 반복해서 데이터를 읽어와 쓴다. 하지만 아래처럼 InputStream을 구현해서 파라미터에 넣으면 어떻게 될까? public class SatanInputStream implements InputStream { public int read(byte[] data) { ... return 0; //데이터가 없을 때 0을 리턴하도록 구현 } } 이렇게 되면 is.read()가 더 읽을 것이 없더라도 0을 반환하기 때문에 while문이 끝나지 않아 CopyUtil.copy() 메소드는 무한 루프 상태에 빠지게 된다. 여기서도 이런 문제가 생기는 이유는 하위 타입인 SatanInputStream이 상위 타입인 InputStream을 올바르게 대체하지 않았기 때문이다. 리스코프 치환 원칙 정리 리스코프 치환 원칙은 기능의 명세에 대한 내용이다. 하위 타입 구현이 명세에서 벗어나게 되면 비정상적으로 동작할 수 있기 때문에 상위 타입에서 정의한 명세를 벗어나지 않는 범위에서 구현해야 한다. 위반 사례로는 주로 다음 것들이 있다. 명시된 명세에서 벗어난 값을 리턴 명시된 명세에서 벗어난 익셉션을 발생 명시된 명세에서 벗어난 기능을 수행 4. 인터페이스 분리 원칙(Interface Segregation Principle) 인터페이스 분리 원칙은 클라이언트는 자신이 이용하지 않는 메소드에 의존하지 않아야 된다는 원칙이다. 이 원칙은 C나 C++ 같이 컴파일과 링크를 직접 해주는 언어를 사용할 때 장점이 잘 드러나기 때문에 C++로 예를 들어보자. 인터페이스 분리 원칙을 지키지 않았을 때 아래와 같은 기능을 제공하는 ArticleService 클래스를 구현할 때 헤더 파일인 ArticleService.h 파일에 클래스의 인터페이스 명세가 코딩되고, ArticleService.cpp 파일에는 구현이 코딩된다. 최종 실행 파일을 만들려면 각각의 UI와 ArticleService.cpp를 컴파일한 결과 오브젝트 파일을 만들어내고, 그 오브젝트 파일들을 링크하게 된다. 그 과정은 간략하게 다음과 같다. 그런데 만약에 기능 중 게시글 목록 읽기와 관련된 함수의 변경이 발생하게 되었다고 가정해보자. 우선 변경이 일어난 부분인 ArticleService.h, ArticleService.cpp, 게시글 목록 UI 파일에 변경을 반영한 뒤에 컴파일하여 다시 오브젝트 파일을 생성하게 될 것이다. 하지만 이것만 변경되는 것이 아니고 ArticleService.h 파일이 변경되었기 때문에 이 헤더 파일을 사용하는 게시글 작성 UI와 게시글 삭제 UI의 소스 코드도 다시 컴파일하여 오브젝트 파일을 만들어 주어야 한다. 즉, 변경이 없는 파일들도 재컴파일 해주어야 되는 불필요한 상황이 발생한 것이다. 만약에 다음과 같이 인터페이스들을 분리했다면? 각각의 기능은 개별적인 인터페이스에 의존하고 있기 때문에 ArticleListService.h 인터페이스에 변경이 발생하더라도 게시글 목록 UI만 영향을 받고 나머지는 영향을 받지 않는다. 물론 C++이 아닌 자바 언어를 사용하고 있다면 자바 가상 머신이 동적으로 링크 과정을 해주기 때문에 위와 같은 소스 재컴파일 문제는 발생하지 않는다. 하지만 인터페이스 분리 원칙이 재컴파일 문제만 관련 있는 것은 아니다. 적절한 인터페이스 분리는 단일 책임 원칙과도 연결되는데 하나의 타입에 여러 기능이 있을 경우 한 기능 변화로 다른 기능이 영향을 받을 가능성이 높아진다. 따라서 사용하는 기능만 제공하도록 인터페이스를 분리함으로써 한 기능에 대한 변경의 여파를 최소화할 수도 있다. 5. 의존 역전 원칙(Dependency Inversion Principle) 의존 역전 원칙은 고수준 모듈이 저수준 모듈의 구현에 의존해서는 안 되고 저수준 모듈이 고수준 모듈에서 정의한 추상 타입에 의존해야 된다는 것이다. 여기서 말하는 고수준 모듈이란 어떤 의미 있는 단일 기능을 제공하는 모듈이고, 저수준 모듈은 고수준 모듈의 기능을 구현하기 위해 필요한 하위 기능의 실제 구현이라 할 수 있다. 고수준 모듈이 저수준 모듈에 의존할 때 문제 상품의 가격을 결정하는 정책을 생각해 보면 고수준 모듈로 다음과 같이 나올 수 있다. 쿠폰을 적용해서 가격 할인을 받을 수 있다. 쿠폰은 동시에 한 개만 적용 가능하다. 저수준 모듈로 들어가 보면 일정 금액 할인 쿠폰, 비율 할인 쿠폰 등 다양한 쿠폰이 존재할 수 있다. 여기서 쿠폰을 이용한 가격 계산 모듈(고수준)이 개별적인 쿠폰(저수준) 구현에 의존하게 되면 아래처럼 새로운 쿠폰 구현이 추가되거나 변경될 때마다 가격 계산 모듈이 변경되는 상황이 발생한다. public int calculate() { ... if (someCondition) { CouponType1 type1 = ... } else { // 쿠폰2 추가에 따라 // 가격 계산 모듈 변경 CouponType2 type2 = ... ... } } 이런 상황은 프로그램 변경을 어렵게 만든다. 우리가 원하는 것은 저수준 모듈이 변경되더라도 고수준 모듈은 변경되지 않는 것인데 이것이 바로 의존 역전 원칙이다. 의존 역전 원칙을 통한 변경의 유연함 의존 역전 원칙은 방금과 같은 문제를 역으로 뒤집어 저수준 모듈이 고수준 모듈을 의존하게 만들어 해결한다. 어떻게 가능할까? 바로 다음과 같이 추상화를 통해 가능하다. 원래 FlowController(고수준 모듈)는 FileDataReader(저수준 모듈)를 의존하고 있었으나 ByteSource로 추상화를 하여 FlowController와 FileDataReader가 모두 추상 타입인 ByteSource에 의존하도록 했다. 즉, 고수준 모듈과 저수준 모듈이 모두 추상 타입에 의존하게 만들어 고수준 모듈의 변경 없이 저수준 모듈을 변경할 수 있는 유연함을 얻게 되었다. 마무리 여기까지 OOP의 5가지 원칙인 SOLID에 대해 예를 들어가며 살펴보았다. 확실히 정의만 봤을 때는 뭔가 잘 와닿지 않고 추상적이었는데 예를 통해 살펴보니 한층 깊게 알아볼 수 있는 좋은 시간이었다. 추상화와 다형성에 대해 얘기가 많이 나오는데 이 둘을 잘 응용할 수 있는 능력을 길러야 될 필요가 있을 것 같다. 참고: 개발자가 반드시 정복해야 할 객체지향과 디자인 패턴 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java OOP SOLID etc",
    "url": "/etc/java/2022-12-29-OOP-Ex(2)/"
  },{
    "title": "예를 들어가며 SOLID에 대해 알아보자(1)",
    "text": "이전에 OOP의 5가지 원칙(SOLID)에 대해 알아봤지만 간단하게 정리 해놨기 때문에 잘 이해하기 힘들 수 있다. 상세한 예를 들어가며 알아가보자. 1. 단일 책임 원칙(Single Responsibility Principle) 단일 책임 원칙은 클래스는 단 한 개의 책임을 가져야 된다는 간단한 원칙이다. 하지만 역으로 가장 어려운 원칙이기도 하다. 한 개의 책임에 대한 정의가 모호하고 하나의 책임을 설계하기 위해 상당한 경험이 필요하기 때문이다. 먼저 단일 책임 원칙을 위반하면 어떤 문제점이 있는지 보자. a. 변경을 어렵게 만든다. public class DataViewer { //두 가지 역활(load, update) public void display() { String data = loadHtml(); updateGui(data); } public String loadHtml() { HttpClient client = new HttpClient(); client.connet(url); return client.getResponse(); } private void updateGui(String data) { GuiData guiModel = parseDataToGuiData(data); tableUI.changeData(guiModel); } private GuiData parseDataToGuiData(String data){ //파싱 코드 } ... } 위 코드의 display 메소드는 loadHtml()을 통해 읽어 온 HTML 응답 문자열을 updateGui()를 통해 데이터를 변경시키고 있다. 위의 DataViewer 클래스를 잘 사용하고 있다가 나중에 데이터를 제공하는 서버가 HTTP 프로토콜에서 소켓 기반의 프로토콜로 변경되면 어떻게 될까? public class DataViewer { public void display() { byte[] data = loadHtml(); //변경 필요 updateGui(data); } public byte[] loadHtml() { //변경 필요 SocketClient client = new SocketClient(); //변경 필요 client.connet(server, port); //변경 필요 return client.read(); //변경 필요 } private void updateGui(byte[] data) { //변경 필요 GuiData guiModel = parseDataToGuiData(data); tableUI.changeData(guiModel); } private GuiData parseDataToGuiData(byte[] data){ //변경 필요 //파싱 코드 //변경 필요 } ... } 데이터를 읽어 오는 기능의 변화로 위와 같은 많은 코드의 수정이 필요할 것이다. 이러한 코드 수정은 두 개의 책임이 한 클래스에 아주 밀접하게 결합되어 있어서 발생했다. 책임의 개수가 많아질수록 한 책임의 기능 변화가 다른 책임에 주는 영향은 많아진다. 데이터 읽기와 데이터를 화면에 보여주는 책임을 두 개의 클래스로 분리하고 두 클래스 간의 주고받을 데이터를 알맞게 추상화하면 위와 같은 상황을 막을 수 있다. b. 재사용이 어렵다. 또한 위와 같이 단일 책임 원칙을 위반하는 경우 재사용을 어렵게 만든다. 앞의 DataViwer 관계는 위의 그림과 같다. HttpClient 패키지와 GuiComp 패키지가 각각 별도의 jar 파일로 제공된다고 하면 데이터를 읽어 오는 기능이 필요한 DataRequiredClient 클래스를 만들경우 DataViewer 클래스와 HttpClient만 필요하지만 실제로는 DataViewer가 GuiComp를 필요 하므로 GuiComp jar까지도 필요하므로 실제 사용하지 않는 기능이 의존하는 jar파일 까지 필요하다. 하지만 위처럼 단일 책임 원칙에 따라 책임이 분리되었다면 데이터를 읽어 오는것과 상관없는 GuiComp 패키지나 datadisplay 패키지는 포함시킬 필요가 없어진다. 2. 개방 폐쇄 원칙(Open Closed Principle) 개방 폐쇄 원칙은 확장에는 열려 있고 변경에는 닫혀 있어야 된다는 것인데 즉, 기능을 변경하거나 확장할 수 있으면서 그 기능을 사용하는 코드는 수정하지 않는다. 기능을 변경하는데 그 기능을 사용하는 코드를 변경하지 말라니..? 뭔가 모순되는 말이지만 다음과 같은 방법으로 가능하다. a. 추상화 FileByteSource: 파일에서 byte를 읽어 오는 클래스 SocketByteSource: 소켓으로 byte를 읽어 오는 클래스 위 그림에서 메모리에서 byte를 읽어 오는 기능을 추가해야 할 경우 ByteSource 인터페이스를 상속받은 MemoryByteSource 클래스를 구현함으로 기능 추가가 가능하다. 그리고 새로운 기능이 추가되었지만, 이 새로운 기능을 사용할 FlowContrller 클래스의 코드는 변경되지 않는다. 즉, 새로운 기능을 확장하면서도 기능을 사용하는 기존 코드는 변경하지 않은 것이다. 이를 개방 폐쇄 원칙은 (사용되는 기능의) 확장에는 열려 있고 (기능을 사용하는 코드의) 변경에는 닫혀 있다고 표현한다. b. 상속 클라이언트의 요청이 왔을 때 데이터를 HTTP 응답 프로토콜에 맞춰 데이터를 전송해주는 ResponseSender가 있다고 가정하자. public class ResponseSender { private Data data; public ResponseSender(Data data) { this.data = data; } public Data getData() { return data; } public void send() { sendHeader() sendBody(); } protected void sendHeader() { // 헤더 데이터 전송 } protected void sendBody() { // 텍스트로 데이터 전송 } } ResponseSender 클래스의 send() 메소드는 sendHeader(), sendBody()를 호출하며 HTTP 응답 데이터를 생성한다. sendHeader()와 sendBody()는 protected 공개 범위로 하위 클래스에서 이 두 메소드를 오버라이딩 할 수 있다. 만약 압축해서 데이터를 전송하는 기능을 추가하고 싶다면 아래와 같이 오버라이딩 해주면 된다. public class ZippedResponseSender extends ResponseSender { public ZippedResponseSender(Data data) { super(data); } @Override protected void sendBody() { // 데이터 압축 처리 } } ZippedResponseSender 클래스는 기존 기능에 압축 기능을 추가하는데 ResponseSender 클래스의 코드는 변경되지 않았다. 즉, ResponseSender 클래스는 확장에는 열려 있으면서 변경에는 닫혀 있다고 할 수 있다. 개방 폐쇄 원칙이 깨질 때 주요 증상 추상화나 다형성을 이용해 개방 폐쇄 원칙을 구현하기 때문에 이것이 잘 지켜지지 않은 코드는 개방 페쇄 원칙을 어기게 되는데, 주로 개방 폐쇄 원칙을 어기는 코드의 특징은 다음과 같다. 다운 캐스팅 사용 다음과 같은 Character 관계가 있다. 하지만 아래와 같이 특정 타입인 경우 별도 처리를 하도록 drawCharacter() 메소드를 구현하게되면 Character 클래스가 확장될 때 drawCharacter() 메소드도 같이 수정되어 변경에 닫혀 있지 않게 된다. public void drawCharacter(Character character) { if (character instanceof Missile) { //타입 확인 Missile missile = (Missile) character; //타입 다운 캐스팅 missile.drawSpecific(); }else { character.draw(); } } 그래서 위 코드의 경우 타입이 Missile이면 타입 변환 뒤 drawSpecific() 메소드를 호출하므로 이 메소드가 실제로 객체마다 다르게 동작할 수 있는 변화 대상인지 확인해보고 앞으로 다르게 동작할 가능성이 높다면 이 메소드를 추상화해 Character 타입에 추가해야 한다. 비슷한 if-else 블록 존재 Enemy 캐릭터의 움직이는 경로가 몇가지 패턴에 따라 이동하는 코드를 다음과 같이 작성하게 되면 Enemy 클래스에 새로운 경로 패턴을 추가해야 할 경우 darw() 메소드에 새로운 if 블록이 계속해서 추가된다. 즉 변경에 닫혀 있지 않다. 어떻게 바꿔야 될까? public class Enemy extends Character { private int pathPattern; public Enemy(int pathPattern) { this.pathpattern = pathPattern; } public void draw() { if (pathPattern == 1) { x += 4; } else if (pathPattern == 2) { y += 10; } else if (pathPattern == 4) { x += 4; y += 10; } ...; // 그려주는 코드 } } 경로가 앞으로 계속해서 확장(변경)되기 때문에 이 부분을 추상화하여 표현하면 된다. 그렇게하면 다음과 같이 경로 패턴을 추상화하여 Enemy에서 추상화 타입을 사용하는 구조로 바뀐다. public class Enemy extends Character { private PathPattern pathPattern; public Enemy(PathPattern pathPattern) { this.pathpattern = pathPattern; } public void draw() { int x = pathPattern.nextX(); int y = pathPattern.nexyY(); ...; // 그려주는 코드 } } 이렇게 되면 이제 새로운 이동 패턴이 생기더라도 draw() 메소드는 변경되지 않으며, PathPattern 구현 클래스만 새로 추가해주면 된다. 즉, 개방 폐쇄 원칙은 변화가 예상되는 것을 추상화해서 변경의 유연함을 얻도록 해주는 것이다. 변화되는 부분을 추상화 하지 못하면 개방 폐쇄 원칙을 지킬 수 없게 되어 시간이 흐를수록 기능 변경이나 확장이 어렵다. 글이 길어져 여기서 한번 끊고 다음글에서 나머지 원칙인 리스코프 치환 원칙, 인터페이스 분리 원칙, 의존 역전 원칙에 대해 자세히 알아보자. 참고: 개발자가 반드시 정복해야 할 객체지향과 디자인 패턴 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java OOP SOLID etc",
    "url": "/etc/java/2022-12-25-OOP-Ex(1)/"
  },{
    "title": "우테코 5기 최종 코딩 테스트 회고",
    "text": "🙌 1차 합격 지난 한 달간 프리코스에서 정말 좋은 경험을 하며 많은 성장을 했었는데 정말 감사하게도 최종 코딩 테스트 대상자로도 선발되었다!! 🙇🙇🙇 선발 과정은 프리코스에서 진행한 미션의 결과와 미션에 대한 소감문, 지원서에 작성한 내용을 바탕으로 선발했다고 한다. 12월 14일에 오후에 결과가 나왔는데 결과를 보기 전 덜덜 떨었다..😱 그래서 메일을 열고 내용을 볼 때 무서워서 메모장으로 가리고 천천히 내렸던 기억이 난다 ㅋㅋㅋㅋ (예전에 게임에서 카드깡이나 강화 같은 거 할 때 해보셨나요? ㅋㅋㅋㅋㅋ) 📃 최종 테스트 준비 12월 17일 토요일에 우테코 5기 선발 과정의 최종 테스트가 예정되어 있었다. 1차 결과가 나오기 전에 연습 삼아 이전 기수의 최종 코테 문제를 풀어봤었는데 5시간 안에 풀기에는 어림도 없었다. 그래서 남은 3일간 이 시간 안에 풀기 위해 문제들을 최적화할 방법을 분석하기 시작했다. 1. 핵심 과정 분석 먼저 문제를 푸는데 내가 어떤 과정으로 풀어야 할지 나에게 맞는 방법으로 정형화하기로 했다. 그렇게 아래와 같은 방법을 찾아내게 되었고 이 방법을 몸에 체득화 시키려고 반복했다. routine 전체적으로 간단히 읽기 Flow chart 작성 기능 목록 작성(상세히 읽으며) 주어진 테스트 코드 확인 순서대로 코드 구현 그리고 구현 과정 중에 시간이 어디서 가장 많이 잡아먹히는지 확인한 결과 당연히 주요 기능들쪽이였다. 이 주요 기능을 코드로 구현하기 전에 미리 설계를 잘해놔야 구현할 때 시간이 많이 단축되었다. 그래서 주요 기능을 구현하기 전에는 미리 어떤 로직 순서로 이루어지는지 고려할 조건은 무엇이 있는지 상세하게 설계한 후에 구현하려 했다. 이렇게 한 결과 설계와 구현을 따로 분리하여 구현 과정 중 중간중간 머리가 꼬이는 걸 방지할 수 있었으며 그 결과 시간을 많이 단축할 수 있었다. 2. 부가적인 부분 분석 그리고 15일, 16일에 최종 코딩 테스트 환경처럼 미리 연습을 해보며 추가적으로 시간을 더 단축할 수 있는 곳을 찾아보게 되었다. 초기 세팅: 이 부분은 혹시 시험 때 긴장해서 실수할까 봐도 적어놨다. 특히 4번 부분이 정말 중요한 것 같다. 5시간 안에 빠르게 구현하려면 값들을 확인할 때 디버깅 기능을 많이 사용하는데 테스트해 본 결과 기본 설정인 gradle로 진행하게 되면 매 디버깅마다 20초 정도 기다려야 되지만 intellij IDEA로 했을 때 3초 만에 가능했기 때문에 이것이 쌓이고 쌓이면 엄청 크게 작용할 것이라 생각했다. 구현 기능 목록 포맷: 미리 구현 기능을 작성할 때 Format을 준비해놨다. 문제 제출 제일 중요한 제출 부분때 실수할까 적어놓음 자주 사용할 수 있는 코드 준비 Try Catch private String input() { try { String input = inputView.input(); validator.validateInput(input); return input; } catch (IllegalArgumentException exception) { System.out.println(exception.getMessage()); return input(); } } 정규식 Test //에러 테스트 양식(assertThatThrownBy) assertThatThrownBy(() -&gt; 테스트할 메소드()) .isInstanceOf(IllegalArgumentException.class) .hasMessage(에러 메시지); //파마리터 여러개 테스트 양식 @ParameterizedTest @ValueSource(strings = {\"\", \"123\"}) @DisplayName(\"다리 길이 입력 크기가 1보다 작거나 2보다 큰 경우 예외 처리\") void validateBridgeLength(String input) { assertThatThrownBy(() -&gt; validator.validateBridgeSize(input)) .isInstanceOf(IllegalArgumentException.class) .hasMessage(ErrorMessage.INCORRECT_BRIDGE_SIZE); } branch 실수 했을 경우 대비(git cherrypick 과정) 1. git checkout ${hashcode} a. 돌아가고 싶은 커밋(해시코드)으로 checkout (ex. git checkout 6aa2570) 2. git checkout -b parkmuhyeun a. 만들고 싶은 branch 생성 3. git cherry-pick ed0eb3c^..a3638de a. A^..B (A에서 B까지 commit 복사) 4. git push 👨‍💻 최종 테스트 진행 아무래도 제한 시간 5시간 안에 구현하다 보니깐 당일날 시험을 치기 전까지도 코드 퀄리티는 어떻게 챙겨야 할까, 내가 준비한 방법이 맞을까 등 고민이 많았었는데 위 메일을 받고 조금 걱정을 덜고 시험을 맞이할 수 있었다. 정말 배려 깊은 우테코 ㅠㅠ 5시간이라는 급박한 시간제한을 제일 걱정했기 때문에 이번에 최종 테스트를 위해 연습할 때는 돌아가는 프로그램을 먼저 만들고 퀄리티는 그 뒤에 신경을 써보자로 목표를 잡았다. 전략을 잘 준비한 효과인지..!! 최종 코딩 테스트를 치를 때 실수나 큰 에러 없이 적절한 시간 안에 첫 번째 목표인 돌아가는 프로그램에 도달했다. 물론, 이번 최종 테스트 문제가 이전 기수들의 최종 테스트보다 좀 더 구현할 것이 적었기 때문에 그럴 수도 있겠지만 어쨌든 너무 기뻤다. 일단 돌아가는 프로그램을 만들었기 때문에 여기서 퀄리티를 어떻게 높일까 고민했다. 모든 걸 다 챙기면 좋겠지만 시간제한 때문에 그럴 수 없기 때문에 선택해야 됐고 아키텍처를 좀 더 분리해서 더 깔끔한 설계를 할까 아니면 클린 코드와 테스트 등을 챙기고 요구사항들을 다시 볼까 고민했다. 결국 후자를 선택했는데 전자 같은 경우는 얼마나 걸릴지도 모르고 혹시 고쳤다가 백업해야 되는 상황이 올 수도 있었기 때문에 후자를 선택해 적용했다. 그렇게 놓친 에러 처리, 테스트, 상수 분리, 메소드 분리, 문서 추가 처리 등을 추가적으로 적용하며 마무리했다. 최종 제출 코드 https://github.com/parkmuhyeun/java-menu/tree/parkmuhyeun 🎤 최종 테스트 후기 역시 인간의 욕심은 끝이 없다고 했는가.. ㅜㅜ 끝나고 나니 이건 왜 이렇게 안 나눴고 저건 왜 이렇게 짰었지 하면서 마음에 걸리는 게 상당히 많았다. 아마 빠른 구현에 초점을 맞춘 방식의 폐해인 것 같다. 하지만 그 5시간 동안 나의 모든 걸 쏟아붓고 나왔기 때문에 정말 후련했었고 후회없이 집에 갔던 기억이 난다. 코드를 제출할 때 소감도 적는 곳이 있는데 그때 막상 소감을 적으려고 하니깐 앞선 시간 동안 모든 집중력을 다 쏟아낸 탓인지 머릿속에 어떤 문장도 생각나지 않았다..ㅋㅋㅋㅋ 머리에 과부하가 걸린 건가.. 그래서 일단 그동안 너무 감사했었기에 감사 인사 정도만 적고 제출했다. 이제 진짜로 우테코의 모든 선발 과정이 끝이 났다.. 지난 한 달 동안 정말 바빴지만 행복한 날들이었다. 짧은 기간이었지만 많은 고민들을 해볼 수 있었으며 많은 성장을 할 수 있었고 앞으로의 방향성도 생각해 볼 수 있었다. 한 달인데도 이런 값진 경험을 했는데 이 과정을 10개월간 지속적으로 한다면 과연 어떤 느낌일까? 생각만 해도 행복사할지도..ㅋㅋ 결과가 어찌 됐든 이 한 달간 최선을 다하여 임하였고 그 과정 속에서 너무나 값진 것들을 많이 얻었기 때문에 후에 지원을 고민을 하시는 분이 있다면 꼭 추천드립니다. 감사했습니다! *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "final-test woowacourse",
    "url": "/woowacourse/2022-12-18-Final-Test/"
  },{
    "title": "static과 싱글톤",
    "text": "static static은 new 연산을 통해 생성한 객체와 달리 메모리에 한번 할당되면 프로그램이 종료하기 전까지 사라지지 않는다. 일반적으로 Class는 static 영역에 생성되고 new 연산을 통해 생성한 객체는 heap 영역에 생성된다. heap 영역은 GC(Garbage Collector)에 의해 관리되어 사용되지 않는 메모리를 해제시켜준다. 하지만 static 영역은 GC가 관리해 주지 않아 프로그램 종료까지 메모리가 할당된 채로 존재하므로 무분별하게 static을 사용하는 것은 좋지 않다. Garbage Collector에 대해 더 알고 싶다면 다음 글을 참고하자! static vs non-static static: 클래스당 하나만 생성되며 동일한 클래스의 모든 객체들에 의해 공유된다. non-static: 객체마다 별도로 존재한다. static: 객체를 생성하기 전에 이미 생성되어 따로 객체를 생성하지 않아도 사용 가능하다. non-static: 객체를 생성해야 사용가능하다. static: 객체가 사라져도 사라지지 않으며 프로그램 종료시에 사라진다. non-static: 객체와 생명주기가 같다. public class Printer { public static String printStatic() { System.out.println(\"static 출력\"); } public String print() { System.out.println(\"일반 출력\"); } } Printer.printStatic(); //O (static 메소드로 객체를 생성하지 않아도 사용 가능) Printer.print(); //X (static 메소드가 아니므로 객체 생성후 사용 가능) Printer printer = new Printer(); printer.print(); //O + main이 static인 이유 main 메서드는 프로그램을 시작하는 부분으로 제일 먼저 실행되기 때문에 객체를 생성하지 않은 채로 작업을 수행할 수 있어야 되기 때문에 static이다. 실행 과정 코드를 실행하면 컴파일러가 자바 소스코드(.java)를 자바 바이트 코드(.class)로 변환 Class Loader를 통해 class 파일을 메모리 영역에 로드 Runtime Data Area 중 Method Area(=Class, Static area)라고 불리는 영역에 Class Variable이 저장되는데, static 변수도 여기 포함된다. JVM은 Method Area에 로드된 main()을 실행 JVM에 대해 더 상세하게 공부하고 싶다면 JVM 글를 참고하자! 싱글톤(Singleton) static에 대해 알아봤으니 이제 static을 활용한 싱글톤 패턴에 대해 알아보자. 싱글톤 패턴이란 객체의 인스턴스가 오직 1개만 생성되는 패턴을 의미한다. 싱글톤 패턴을 구현하는 방법에는 다양한 방법이 있지만 두 가지 정도만 알아보자. Eager Initialization public class Singleton { private static Singleton instance = new Singleton(); private Singleton() {} // 생성자에 접근하지 못하도록 private public static Singleton getInstance() { return instance; } } 가장 간단한 방법으로 static을 통해 해당 클래스를 Class Loader가 로딩할 때 객체를 생성한다. 하지만 이렇게 구현하게 되면 이 객체를 사용하지 않더라도 객체가 무조건 생성되기 때문에 자원 낭비라는 단점이 있다. Lazy Initialization class Singleton { private static Singleton instance; private Singleton() {} // 생성자에 접근하지 못하도록 private //객체가 존재하지 않으면 생성해주고 존재하면 기존 객체를 반환 public static Singleton getInstance() { if (instance == null){ instance = new Singleton(); } return instance; } } 이 방법 같은 경우는 첫 번째 방법에 자원의 낭비를 해결해 주지만 멀티 스레드 환경에서는 동기화 문제가 발생할 수 있다. 관심 있다면 나머지 방법들은 더 찾아서 공부해 보자! 그래서 싱글톤 패턴을 사용하면 뭐가 좋을까? 어떤 객체를 필요할 때마다 생성하는 것이 아니라 한 번만 생성해서 계속 활용하기 때문에 메모리도 아낄 수 있고 속도 측면에서도 좋다고 할 수 있다. 그리고 싱글톤 같은 경우는 static을 활용하여 전역으로 사용되는 인스턴스이기 때문에 쉽게 접근할 수 있다는 이점이 있다. 하지만 접근하기 쉽다는 것은 동시성 문제가 일어날 수도 있으니 조심해야 한다. 그럼 싱글톤에는 어떤 문제점들이 있을까? 싱글톤 패턴은 위와 같은 장점이 있지만 아래의 문제점들을 수반한다. 지저분한 구현 코드. private 생성자로 인해 상속이 불가능 테스트하기 어려움 생성 방식 제한적 -&gt; Mock, 동적 객체 주입 어려움 인스턴스가 자원을 공유하고 있기 때문에 매번 상태 초기화 서버 환경에서 싱글톤이 한 개만 생성됨을 보장하지 못한다. 언제든지 접근할 수 있는 전역 상태로 사용되어 위험 싱글톤 컨테이너 위와 같이 많은 문제점이 있을 수 있기 때문에 안티 패턴으로도 불려 잘 고려해서 사용해야 한다. 하지만 스프링 컨테이너 같은 프레임워크의 힘을 빌려 싱글톤 패턴의 단점을 보완하면서 장점들도 같이 사용할 수 있다. 싱글톤 컨테이너에 대해 궁금하다면 싱글톤 컨테이너 글 참고 스프링 컨테이너가 객체를 생성하고 관리해 주어 앞선 싱글톤의 단점들을 제거해 준다. 싱글톤 패턴을 위한 지저분한 코드 제거 private 생성자 제거 객체지향적으로 개발 가능(DIP, OCP) 프레임워크를 통해 1개의 객체 생성 보장 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java static singleton etc",
    "url": "/etc/java/2022-12-13-Static/"
  },{
    "title": "EC2 배포과정에서 OS에 따른 명령어 차이에 의한 오류",
    "text": "EC2에 배포하는 과정에서 테스트부분에서 계속 오류가 발생했었는데 어떤 에러가 났었고 어떻게 해결했는지 한번 보자 부족한 에러 로그 위 사진처럼 DefaultCacheAwareContextLoaderDelegate 에러와 BeanCreationException이 발생했었다. 어느 곳에서 빈이 제대로 생성이 안된건 알 수 있었지만 정확히 어디에서 발생한지 자세하게 보여주지 않아 한참 지레짐작으로 끙끙앓고 있었다. 그러다 명령어 옵션들을 보던 중 —scan 옵션을 같이 이용하면 더 자세한 것들을 제공해줄 것 같아 ./gradlew —scan clean build으로 입력해보았는데 아래처럼 빌드 스캔 링크를 제공해주었다. 그 링크에 들어가 email을 입력하면 gradle에서 email로 빌드 스캔 결과를 제공해준다. 빌드 스캔 결과를 보면 다음과 같이 빌드 중 어떤 과정이 일어났는지 상세하게 적혀있다. OS에 따른 명령어 차이에 의한 에러 EmbeddedRedisConfig에서 빈 생성이 제대로 안된 걸 확인할 수 있었고 어떤 명령어(“cmd.exe”)를 수행하지 못하고 있었다. 해당 클래스는 EmbeddedRedis를 위한 설정파일로 실행, 정지 그리고 추가 기능에 대한 설정이 적혀있다. 그 중, 예전에 EmbeddedRedis 충돌 방지를 위한 코드를 설정한 적이 있었는데 해당 port를 사용중인 프로세스를 확인하는 쉘을 실행하는 부분이 문제를 일으키고 있는걸 확인했다. 현재 로컬에서는 윈도우 환경이기 때문에 다음과 같은 코드로 문제없이 잘 진행하고 있었다. private Process executeGrepProcessCommand(int port) throwsIOException { String command = String.format(\"netstat -nao | find \\\"LISTEN\\\" | find \\\"%d\\\"\", port); String[] shell = {\"cmd.exe\", \"/y\", \"/c\", command}; return Runtime.getRuntime().exec(shell); } 하지만 EC2 환경은 Linux로 위 코드가 제대로 실행 되지 않았기 때문에 계속 EmbeddedRedisConfig가 생성되지 않아 오류를 일으키고 있었던 것이다. private Process executeGrepProcessCommand(int port) throwsIOException { String command = String.format(\"netstat -nat | grep LISTEN|grep %d\", port); String[] shell = {\"/bin/sh\", \"-c\", command}; return Runtime.getRuntime().exec(shell); } 그래서 위와 같이 Linux에 맞는 코드들로 변경시켜 주었더니 잘 되는 것을 확인하였다! *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "error os ec2 command project",
    "url": "/project/2022-12-07-os-command/"
  },{
    "title": "WebSocket, STOMP, SockJS에 대해 알아보자",
    "text": "이번에 채팅을 구현할 때 WebScocket, STOMP, SockJS를 이용해서 구현했는데 각각은 무엇이고 왜 쓰는지 알아보자 HTTP vs WebSocket 우리는 보통 웹 프로토콜로 HTTP(HyperText Transfer Protocol)를 사용해왔는데 이 HTTP는 비연결성으로 클라이언트와 서버간 연결을 유지하지 않기 때문에 매번 정보를 주고 받을 때 마다 연결을 맺고 끊는 과정이 필요한데 이 과정에서 많은 비용이 들기 때문에 실시간 정보를 주고 받는 곳(채팅, 게임 등)에는 적합하지 않았다. 그래서 효율적인 양방향 통신을 구현하기 위한 기술인 웹소켓(WebSocket)이 등장하게 되었다. 웹소켓을 사용하게 되면 클라이언트와 서버가 한번 연결을 맺으면 계속 유지되고 서로 양방향 통신이 가능해진다. SockJS 하지만 웹소켓 같은 경우 모든 환경에서 지원해주지 않았기 때문에 웹소켓을 지원하지 않는 환경에서는 사용할 수 없다는 단점이 있었는데 SockJS를 이용하면 지원하지 않는 환경에서도 가능하게 해준다. SockJS는 다양한 기술을 이용해 웹소켓을 지원하지 않는 환경에서도 정상적으로 동작하도록 해준다. 전송 타입은 크게 다음 3가지로 나눠진다. WebSocket HTTP Streaming HTTP Long Polling STOMP(Simple Text Oriented Messaging Protocol) 메시징 방식만 잘 정의하면 STOMP를 쓰지 않고 WebSocket만으로도 잘 만들 수 있으나 해당 메시지가 어떤 요청인지 어떻게 처리해야 하는지 등 추가적으로 구현해주어야 될 것이 많다. STOMP는 메시지 브로커를 이용하여 쉽게 메시지를 주고 받을 수 있는 프로토콜로 발신자가 메시지를 발행하면 수신자가 그것을 수신하는 Pub(발행) - Sub(구독) 형태로 이루어져 있다. 쉽게 말하면 누군가 어떤 채팅창을 구독(Sub)하고 있다면 그 채팅방에서 채팅(Pub, 발행)을 보내면 구독하고 있던 구독자들은 모두 채팅을 받는 것이다. 구독, 발행하는 과정을 보기전 STOMP가 어떤 형태로 되어있는지 먼저 보자. STOMP는 프레임 단위 프로토콜으로 커맨드, 헤더, 바디로 이루어져 있다. COMMAND header1:value1 header2:value2 Body^@ COMMAND: SUBSCRIBE(구독), SEND(발행), MESSAGE(BroadCasting, 전체 전송) header: destination, type, subscription … 등 예를 들어 클라이언트가 1번 채팅방에 구독을 하면 아래 처럼 발송이 될 수 있다. SUBSCRIBE id: sub-0 destination: /topic/room/1 Pub(발행), Sub(구독) 흐름 아래 그림에서 /app을 발행(/pub)으로 /topic을 구독(/sub)으로 치환해서 보면 더 쉽다. MESSAGE : 헤더 및 페이로드를 포함한 메시지 SimpleAnnotationMethod : @MessageMapping을 이용해 메시지 처리 MessageHandler : 메시지 처리를 위한 계약 SimpleBroker : subscription을 메모리에 저장하고 연결된 client에게 메시지를 보냄 발행(Pub) destination에 /app으로 들어오게 되면 @MessagingMapping 애노테이션이 붙은 스프링 컨트롤러로 매핑되게 되고 컨트롤러에서 메시지를 처리한 후에 /topic으로 브로커에게 전달하면 브로커는 MESSAGE COMMAND를 이용해 구독하고 있는 모든 구독자들에게 response를 전송한다. 구독(Sub) destination에 /topic으로 들어오게 되면 스프링 컨트롤러를 거치지 않고 브로커에게 바로 접근하게 되는데 SUBSCRIBE COMMAND의 경우가 여기에 해당되며 이 경우 브로커가 구독자를 메모리에 저장하여 관리한다. 다음과 같은 상황에 어떤 과정들이 일어나는지 한번 보자 클라이언트0번이 1번 채팅방에 구독 SUBSCRIBE id: sub-0 destination: /topic/room/1 위에서 구독(Sub)흐름에 해당하는 과정이 일어나게 된다. 클라이언트1번이 1번 채팅방에서 메시지를 전송 SEND destination: /app/{@MessageMapping Endpoint} content-type:application/json {\"roomId\":1, \"type\": MESSAGE\", \"sender\":\"client1\"} 위에서 발행(Pub) 흐름에 해당하는 과정이 일어나게 되어 컨트롤러에서 메시지 처리가 일어난 후 모든 구독자에게 메시지를 Broadcasting(전체 전송)을 하기 위해 아래와 같은 MESSAGE COMMAND 전송 MESSAGE destination: /topic/room/1 content-type:application/json subscription:sub-0 message-id:dlfsrch-0 {\"roomId\":1, \"type\": MESSAGE\", \"sender\":\"client1\"} 참고: 웹소켓 지원 환경 https://docs.spring.io/spring-framework/docs/current/reference/html/web.html#websocket-stomp *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Chat WebSocket STOMP SockJS project",
    "url": "/project/2022-11-29-chat/"
  },{
    "title": "우테코 5기 프리코스 4주차 회고",
    "text": "마지막 미션(ㅠㅠ)이기 때문에 지금까지 배웠던 것을 잘 적용하기 위해 코드 구현 시간과 비슷한 정도로 리팩터링에 시간을 많이 쓴 거 같다. 똑같은 걸 보고 또 보고.. 또 봤기 때문에 머리가 터질뻔 했다ㅋㅋ 하지만 그 반복된 시간 속에서 클린 코드라는 좋은 친구를 한 명 얻게 되어 너무 보람찬 시간이었다. 😄 (나만 친구라 생각하는 거 아니지..?) 🧑‍🤝‍🧑 앞으로의 피어 리뷰? 몇 주간 계속된 피드백과 리뷰로 이제 피어 리뷰가 큰 의미가 있을까라는 나의 오만한 생각을 부숴준 우리 스터디원들.. 너무 감사합니다! 피어 리뷰 이번 미션이 마지막으로 프리코스 마지막 후기가 될 것 같아 이번에는 어떤 피드백을 받았다는 단편적인 피드백을 적기보다는 피어 리뷰를 하면서 직접 느낀 앞으로도 꾸준히 피어 리뷰가 필요한 이유를 적어보겠다. 잘못된 습관 Catch 혹은 실수 리마인드 어떠한 잘못된 습관이나 실수 같은 경우는 내가 아무리 코드를 계속 보더라도 발견하지 못할 확률이 높다. 이러한 것을 리뷰어 입장에서는 쉽게 발견할 수 있으며 리뷰를 남겨줄 수 있다. 그리고 내가 또 어느 곳에서 자주 실수하는지도 리마인드 해주는 역할도 할 수 있다. 코드나 혹은 로직의 더 좋은 개선 방법 제공 내가 짠 코드나 로직의 경우 이미 설계 단계에서부터 머리에 각인이 되어버리기 때문에 더 좋은 개선 방법이 있더라도 잘 발견하지 못한다. 그래서 코드에 대해 열린 마음을 가지고 있는 리뷰어가 더 좋은 방법을 제공하는 경우도 많다. 다른 분들의 좋은 코드 흡수 다른 분들에게 리뷰를 받는 것 말고도 다른 분들의 코드를 읽고 리뷰를 하는 것만으로 상당한 경험치가 쌓인다. 각자마다 설계한 방법, 로직, 코드들이 다 다르기 때문에 내가 생각지도 못한 방법들이 있을 수 있다. 커비가 한번 되어보자 흡! 📄 3주차 공통 피드백 2, 3주 차 공통 피드백에 테스트 코드에 대한 이야기가 많았다. 지금까지 테스트 코드를 짜면서 진행하긴 했지만 뭔가 빈약하게 짠 거 같은 느낌이 들어 이번에는 테스트 코드도 더 신경을 써보자 생각하고 미션을 진행했다. 테스트 코드 커버리지 위 화면처럼 테스트 패키지를 오른쪽 클릭 -&gt; More Run/Debug -&gt; Run Test with Coverage를 클릭하면 아래 화면처럼 자기가 작성한 코드에 대한 테스트 커버리지가 나오게 된다. 이번 미션에서는 테스트 커버리지를 100%로 맞춰보자고 마음먹었었는데 달성하여 제출하게 되어 기뻤다! 하지만 문뜩 이런 생각이 들었다. 실제로도 테스트 커버리지가 100%가 될 수 있을까 그리고 그렇게 되면 완벽한 건가? 실제로 토스에서 테스트 커버리지 100%를 1년 6개월 동안 유지해본 경험이 공유된 영상을 보게 되었고 궁금증을 시원하게 해결해주었다. 참고: 테스트 커버리지 100% 높은 테스트 커버리지의 이점 자신있게 누를 수 있는 배포 버튼 거침없는 리팩토링 불필요한 프로덕션 코드 제거 프로덕션 코드에 대한 이해도 상승 점점 쉬워지는 테스트 작성 높은 테스트 커버리지를 유지할 때 단점 느려지는 테스트 -&gt; 생산성이 떨어짐 느려지는 원인 찾아 해결 가능 ( ex)스프링 애플리케이션 컨텍스트 로딩, 불필요한 로깅 설정 제거.. 등) 진짜 어려운 테스트 그리고 가장 중요한 건 커버리지가 100%라 하더라도 버그는 존재할 수 있다. 테스트를 잘못 작성 요구사항에 오해 컴포넌트간 협업 실패 테스트 코드 중복 제거 테스트 코드 같은 경우 같은 테스트에 여러 개의 값을 넣어보고 싶을 때 아래 처럼 @ValueSource를 사용해서 한번에 테스트 해볼 수 있다. // @Test // @DisplayName(\"숫자가 아닌 경우 예외 처리\") // void validateBridgeDigit() { // assertThatThrownBy(() -&gt; validator.validateBridgeSize(\"1a\")) // .isInstanceOf(IllegalArgumentException.class) // .hasMessage(ErrorMessage.INCORRECT_BRIDGE_SIZE); // } // @Test // @DisplayName(\"숫자가 아닌 경우 예외 처리\") // void validateBridgeDigit() { // assertThatThrownBy(() -&gt; validator.validateBridgeSize(\";\")) // .isInstanceOf(IllegalArgumentException.class) // .hasMessage(ErrorMessage.INCORRECT_BRIDGE_SIZE); // } @ParameterizedTest @ValueSource(strings = {\"1a\", \";\"}) @DisplayName(\"숫자가 아닌 경우 예외 처리\") void validateBridgeDigit(String input) { assertThatThrownBy(() -&gt; validator.validateBridgeSize(input)) .isInstanceOf(IllegalArgumentException.class) .hasMessage(ErrorMessage.INCORRECT_BRIDGE_SIZE); } 회고 작성 피드백 코수타(코치들의 수다 타임)에서 코치님께서 회고 같은 경우도 한 번에 작성하는 것보다 그날 그날 거를 작성해놓는 게 좋다고 하셨다. 그때 정말 공감되었던 게 그전 주 차까지 거의 한 번에 작성하면서 뭔가 아님을 느끼고 있었다. 이렇게 하면 힘들기도 하고 내용도 상세하게 기억이 나지 않는 걸 깨닫고 그 주 차에 딱 그렇게 적용해 보고 있던 도중에 들었기 때문에 약간 소름 돋았다.. 피드백 너무 맛있어ㅠㅠ ☝ 지난주 목표 지금까지 1, 2, 3주 차 동안 많은 리뷰, 피드백들을 머리속에 입력했지만 짧은 시간 내에 입력했기 때문에 깊게 소화할 시간이 없어서 이번 주 차에 모든 것들을 잘 정리해 보고 적용해 보자고 최종 목표를 잡았었다. 그래서 지금까지의 피어 리뷰 피드백 + 최종 요구사항 Check List들을 계속 곱씹어가며 미션을 풀었다. 최종 요구사항 Check List 최종 요구사항 Check List 프로그램 실행의 시작점은 Application의 main()이다. build.gradle 파일을 변경할 수 없고, 외부 라이브러리를 사용하지 않는다. Java 코드 컨벤션 가이드를 준수하며 프로그래밍한다. 프로그램 종료 시 System.exit()를 호출하지 않는다. 프로그램 구현이 완료되면 ApplicationTest의 모든 테스트가 성공해야 한다. 테스트가 실패할 경우 0점 처리한다 프로그래밍 요구 사항에서 달리 명시하지 않는 한 파일, 패키지 이름을 수정하거나 이동하지 않는다. indent(인덴트, 들여쓰기) depth를 3이 넘지 않도록 구현한다. 2까지만 허용한다. 3항 연산자를 쓰지 않는다. 함수(또는 메서드)가 한 가지 일만 하도록 최대한 작게 만들어라. JUnit 5와 AssertJ를 이용하여 본인이 정리한 기능 목록이 정상 동작함을 테스트 코드로 확인한다. else 예약어를 쓰지 않는다. 도메인 로직에 단위 테스트를 구현해야 한다. 단, UI(System.out, System.in, Scanner) 로직은 제외한다. 함수(또는 메서드)의 길이가 10라인을 넘어가지 않도록 구현한다. 메서드의 파라미터 개수는 최대 3개까지만 허용한다. InputView, OutputView, BridgeGame, BridgeMaker, BridgeRandomNumberGenerator 클래스의 요구사항을 참고하여 구현한다. 3주차 피드백 비즈니스 로직과 UI 로직을 분리한다 연관성이 있는 상수는 static final 대신 enum을 활용한다 final 키워드를 사용해 값의 변경을 막는다 객체의 상태 접근을 제한한다 객체는 객체스럽게 사용한다 필드(인스턴스 변수)의 수를 줄이기 위해 노력한다 발생할 수 있는 예외 상황에 대해 고민한다 성공하는 케이스 뿐만 아니라 예외에 대한 케이스도 테스트한다 테스트 코드도 코드다 테스트를 위한 코드는 구현 코드에서 분리되어야 한다 2주차 피드백 README.md를 상세히 작성한다 기능 목록을 재검토한다 기능 목록을 업데이트한다 값을 하드 코딩하지 않는다 구현 순서도 코딩 컨벤션이다 변수 이름에 자료형은 사용하지 않는다 한 함수가 한 가지 기능만 담당하게 한다 처음부터 큰 단위의 테스트를 만들지 않는다 1주차 피드백 요구사항을 정확히 준수한다 커밋 메시지를 의미 있게 작성한다 Pull Request를 보내기 전 브랜치를 확인한다 PR을 한 번 작성했다면 닫지 말고 추가 커밋을 한다 이름을 통해 의도를 드러낸다 축약하지 않는다 공백도 코딩 컨벤션이다 공백 라인을 의미 있게 사용한다 space와 tab을 혼용하지 않는다 의미 없는 주석을 달지 않는다 IDE의 코드 자동 정렬 기능을 활용한다 Java에서 제공하는 API를 적극 활용한다 배열 대신 Java Collection을 사용한다 ⏯ 4주차 진행 과정 흐름도 작성 구현 기능 작성 코드 구현 리팩토링 및 제출 1. 흐름도 작성 이번에도 역시 저번과 같이 코드를 구현하기 전 흐름도를 작성하여 핵심 부분을 주축으로 설계하려고 했다. 2. 구현 기능 작성 구현 기능 목록 🚀 구현할 기능 목록 다리 생성 다리를 생성할 때 위, 아래 칸 중 건널 수 있는 칸은 0과 1중 무작위 값 이용 무작위 값이 0인 경우 아래 칸 - D 저장, 1인 경우 위 칸 - U 저장 이동할 칸이 유효한 칸인지 확인 입력 다리 길이 입력 3이상 20이하 숫자 입력 그 외의 경우 예외 처리 입력 크기가 1보다 작거나 2보다 큰 경우 예외 처리 숫자가 아닌 경우 예외 처리 3보다 작거나 20보다 큰 경우 예외 처리 이동할 칸 입력 라운드 마다 위(U), 아래(D) 중 입력 그 외의 경우 예외 처리 U(위) 혹은 D(아래)가 아닌 경우 예외 처리 다리를 건널 때까지 혹은 실패할 때 까지 매 라운드 입력 성공한 경우 -&gt; 게임 결과 출력 -&gt; 게임 종료 실패한 경우 -&gt; 다시 시도할지 여부 입력 다시 시도할지 여부 입력 재시작 시 처음에 만든 다리로 게임 재시작 종료 입력 시 결과 출력 후 게임 종료 R(재시작)과 Q(종료)중 하나의 문자를 입력 그 외의 경우 예외 처리 출력 게임 시작 문구 다리 출력 이동할 칸을 건널 수 있다면 O, 건널 수 없다면 X로 표시 선택하지 않은 칸은 공백 한 칸으로 표시 다리 칸의 구분은 문자열로 구분 현재까지 건넌 다리를 모두 출력 최종 게임 결과 출력 게임 성공 여부 출력 시도 횟수 출력 게임 종료 다리를 끝까지 건너면 종료된다. 다리를 건너다 실패하고 추가 게임 여부에서 종료를 입력한 경우 사용자가 잘못된 값을 입력한 경우(예외 처리) IllegalArgumentException을 발생 “[ERROR]”로 시작하는 메시지를 출력 후 그 부분부터 입력을 다시 받는다. Exception이 아닌 IllegalArgumentException, IllegalStateException 등과 같은 명확한 유형을 처리한다. 3. 코드 구현 이번 미션에서는 경우의 수가 상당히 나눠져 있었는데 다리 건너기 이동 중 막힌 경우 재입력 -&gt; 다리 처음부터 시작(다리는 그대로) 종료 -&gt; 최종 출력 후 종료 다리를 다 통과한 경우 -&gt; 성공 -&gt; 최종 출력 후 종료 이 부분이 서로 엮여 있었기 때문에 처음에 설계를 할 때 상당히 골치 아팠는데 한 번에 설계하려던 것이 문제였다. 동시에 고려하다 보니깐 이쪽도 터지고 저쪽도 터졌다. 그래서 생각을 하던 중 한 번에 다 하려 하지 말고 작은 거 부터 하나하나씩 구현하라는 피드백이 떠올랐고 그 생각을 다시 리마인드하며 끝까지 구현했다! 이번에는 지금까지와 달리 클래스들을 다 제공해 주시고 안에 메소드까지 제공해 주셔서 요구사항을 지키며 틀안에 코드를 작성하여 조립을 해야 됐다. 근데 그 과정이 마치 게임에서의 퀘스트를 하나하나 해결해나가는 것 같아서 매우 재미있게 구현했던 것 같다. 👍 https://github.com/woowacourse-precourse/java-bridge/pull/283 4. 리팩토링 및 제출 처음에도 말했듯이 이번에는 리팩토링 하는데 시간을 상당히 많이 사용했는데 커밋을 보면 코드 구현 관련 커밋 17개, 리팩터링 관련 커밋 17개로 커밋 양이 같다. 실수 없이 잘 적용해 보고 싶어 끊임없이 돌려 봤는데 봐도 봐도 계속 고칠게 보여서 미쳐버릴 뻔 했다. 패키지 분리 테스트 추가 import 정리 enum 분리 Converter 분리 파라미터 줄이기 ..등등 🙇 정리 및 후기 4주 동안 너무 재밌게 몰두하였기 때문에 이 글을 적기 전까지는 마지막이라는 실감이 안 났었는데 마지막 후기 적는 곳에 오니깐 프리코스가 끝난다는 실감이 나기 시작한다… 😭 (분명 내일 미션 메일이 와야 되는데..?) 정기적으로 전문가이신 분들에게 피드백을 받고 그 피드백을 적용하는 이 라이프 사이클이 너무 즐거운 나날들이었다. 이 과정에서 클린 코드라는 좋은 친구를 사길 수 있었고 테스트 코드 작성, 프레임워크 구조 설계, 자바 언어를 더 응용해 보는 등 매주 성장하는 내 모습을 볼 수 있었다. 하지만 다음과 같은 말이 있다. 중요한 것은 어떻게 시작했는가가 아니라 어떻게 끝내는가이다. -앤드류 매튜스 우선 프리코스 과정은 이번 미션으로 마지막이 되지만 앞으로 성장을 향한 나의 성장일기는 계속된다! 앞의 과정 중에 아쉬웠던 것이나 적용하지 못한 부분들을 추가 리팩터링해볼 예정이고 그러고 나서 이 과정 그대로 다시 tdd를 적용해 보면서 재진행할 예정이다. 이런 좋은 기회를 주셔서 너무 감사하고 다시 우테코 크루와 만나서 함께 성장할 수 있었으면! 💪 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java precourse woowacourse",
    "url": "/woowacourse/2022-11-22-Fweek/"
  },{
    "title": "JWT를 좀 더 안전하게 저장해보자",
    "text": "이전 포스트에서 JWT가 뭔지, 왜 사용했는지에 대해 포스팅했기 때문에 이번 글에서는 XSS, CSRF, JWT를 저장할 수 있는 방법들에 대해 알아보고 최종적으로 프로젝트에 어떤 걸 적용하였는지 알아보자 XSS, CSRF 먼저 저장하는 방법을 보기 전에 대표적인 웹 취약점 공격 방법인 XSS와 CSRF에 대해 알아보자 XSS(Cross-Site-Scripting) XSS는 악의적인 사용자가 공격하려는 사이트에 스크립트 넣는 기법을 말한다. 주로 다른 웹사이트와 정보를 교환하는 식으로 작동하므로 Cross-Site-Scripting이라 부른다. 사용자로부터 입력받은 값을 제대로 검사하지 않고 사용할 경우 발생 보통 의도치 않은 행동을 수행시키거나 쿠키나 세션 토큰 등의 민감한 정보를 탈취한다. 주로 자바스크립트를 사용하여 공격하는 경우가 많음 공격 방법에 따라 Stored XSS와 Reflected XSS로 나뉜다. Stored XSS(저장형): 게시판이나 댓글, 닉네임 등에 스크립트가 서버에 저장되어 실행되는 방식 Reflected XSS(반사형): URL 파라미터에 스크립트를 넣어 서버에 저장하지 않고 즉시 스크립트를 만드는 방식 XSS 방지 대책 쿠키에 중요한 정보를 담지 않고 서버에 중요 정보를 저장 정보를 암호화 httponly(자바 스크립트에서 쿠키 접속 막는 옵션) CSRF(Cross-Site Request Forgery) CSRF는 사용자가 자신의 의지와는 무관하게 공격자가 의도한 행위(수정, 삭제, 등록 등)를 특정 웹 사이트에 요청하게 하는 공격을 말한다. 즉, 사용자의 권한을 도용해 서버에 대한 악성 공격을 하는 것이다. CSRF 공격이 이루어질려면 다음 조건이 만족되야 됨 위조 요청을 전송하는 서비스에 피해자가 로그인 상태여야 한다. 피해자는 공격자가 만든 피싱 사이트에 접속해야한다. CSRF 방지 대책 서버에서 요청하는 referrer을 확인하여 domain이 일치하는지 검증 Security Token 사용 JWT를 어디에 저장하는게 좋을까? 일반적으로 LocalStorage, Cookie, Variable(memory)에 저장할 수 있다. 각 방법마다 의견이 다양하므로 어떤 방법을 사용할지는 다음 내용들을 읽어보고 생각해 보자. LocalStorage 간단한 방법으로 편리하다는 장점이 있다. 그리고 자동으로 담기는 쿠키와 다르게 자바스크립트 코드에 의해 헤더에 담기므로 CSRF로부터는 안전하다. 하지만 자바스크립트 코드를 통해 액세스할 수 있음으로 XSS 공격에 취약하다. Cookie 쿠키는 기본적으로 XSS, CSRF 공격에 모두 취약하지만 httpOnly, secure, SameSite 등과 같은 옵션을 주어 자바스크립트에서 쿠키에 접근 불가하게 만들 수 있다. Variable(memory) 다른 방식에 비해 안전하다고 할 수 있지만 variable은 웹 페이지가 리로드(이동, 새로고침)시 사라지기 때문에 UX(User Experience)상 좋지 않다. 사용자 입장에서 화면이 리로드 될 때마다 로그인해야 된다면 매우 귀찮을 것이다. 최종적으로 선택한 방법 결국 최종적으로 그중 안전한 Variable(memory)에 AccessToken을 저장하게 되었는데 하지만 이 경우 앞에서 말했듯이 리로드 시 사라지기 때문에 추가 보충 방법이 필요해 추가적으로 RefreshToken을 이용해 보완하게 되었다. 즉, AccessToken은 variable(memory)에 저장하고 RefreshToken은 HttpOnly, Secure, SameSite 옵션을 적용해서 쿠키에 저장하게 되는데 사용하는 흐름을 한번 보자. 로그인을 하면 서버로부터 AccessToken(헤더), RefreshToken(쿠키)을 받는다. 받은 AccessToken을 Variable(memory)에 저장 API 요청 시마다 헤더에 AccessToken 보내도록 설정 AccessToken이 만료되거나 페이지 리로드 될 때마다 RefreshToken을 이용해 AccessToken을 재발급 받는다. 이렇게 되면 RefreshToken이 CSRF에 의해 사용되더라도 공격자는 AccessToken을 알 수 없기 때문에 CSRF도 막을 수 있고 옵션 쿠키로 XSS도 막을 수 있다. 그리고 리로드마다 토큰이 사라지던 단점도 해결되기 때문에 최종적으로 적용하게 되었다. 참고: XSS, CSRF *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JWT XSS CSRF project",
    "url": "/project/2022-11-21-Jwt-Security/"
  },{
    "title": "우테코 5기 프리코스 3주차 회고",
    "text": "이번 3주 차는 정말 너무나도 바쁜 주였다.. 졸작 마무리에, 학교 과제 폭탄에 예비군까지.. 너무 바빠서 나도 모르게 기절하는 시간이 많았다ㅠㅠ 그 와중에 프리코스 2주 차를 하고 나서 아직도 배울 것이 많고 피드백할 것도 많다고 느꼈기 때문에 더 미친 듯이 살았던 것 같다. 🧑‍🤝‍🧑 피어 리뷰 스터디 2주 차 숫자야구에서 코드를 작성하고 제출할 때까지만 해도 그전 피드백(리뷰, 공통 등)을 바탕으로 많은 개선도 하고 성장도 했기 때문에 상당히 자신감에 차있었던 것 같고 이번 피어 리뷰 스터디를 하기 전까지 설렘에 가득 차 있었다. 하지만 역시 성장을 한 것은 나뿐 아니라 다른 사람도 마찬가지였다. 스터디 팀원의 코드들을 리뷰하는데 정말 깔끔했고 배울 점이 여전히 많았다. 이전 주 차의 피드백을 모조리 흡수해서 더 강해진 팀원들이였다. 주마다 팀원들의 코드 퀄리티가 바뀌는데 그 노력이 코드들과 커밋에서 잘 보였다. 팀원들의 코드를 리뷰하면서 자동으로 2주 차에서 내가 뭐가 부족했었는지 3주 차에는 뭘 더 공부해 적용해 보면 좋을지 큰 틀이 바로 잡혔다. private static final 상수 -&gt; Enum으로 분리 변수명, 메소드명 좀 더 생각해보기 Converter 분리, View 분리 등 역활 분리하기 MVC 적용 리뷰를 하기 전 자신감은 다시 열정으로 바뀌었고 이것을 바탕으로 더 열심히 해 팀원끼리 서로 시너지를 내려고 한다. 2주 차 피어 리뷰 스터디를 하고 느낀 건데 이 스터디가 없었다면 여기까지 이렇게 빠르게 피드백을 할 수 있었을까 하고 생각이 든다. 다시 한번 함께 자라기의 중요성을 느꼈던 시간이었다. 2주차 공통 피드백 이번 공통 피드백에는 글로 된 피드백뿐 아니라 숫자 야구에 대한 피드백 강의도 있었다. 저번에는 깃 관련 영상도 있었는데 이런 귀한 영상들까지 주시다니.. 하나도 안 남기고 싹 긁어먹겠습니다^^7 직접 멘토님의 라이브 코딩을 볼 수 있다니 너무나 좋은 기회였다. 멘토님께서는 하신 과정은 아래와 같다. 요구사항 바탕으로 기능 작성 뼈대(skeleton code) 생성 구체적 코드 작성 도메인 코드를 작성하면서 그에 맞는 흐름제어 코드도 동시에 작성 +추가적으로 기능마다 커밋까지 하시는 걸 보면 추출한 요구 기능에 따라서 정말 스무스하게 완성하시는 걸 볼 수 있다. 지금까지 커밋을 하면서 어떤 기준에 맞춰야 되나 고민을 많이 했다. 기준을 엄격하고 딱딱하게 정해서 할려 했기 때문에 스트레스를 많이 받고 고치는 일도 잦았는데 이 영상을 보고 좀 더 유연성 있게 넣을 수 있도록 바뀌었고 편하게 생각하게 되었다. 그리고 처음에 작성한 기능 목록 같은 경우도 후에 웬만하면 건드리지 않도록 하고 있었는데 이것도 편하게 고치도록 바뀌게 되었다. 공통 피드백에서도 기능 목록은 계속해서 변할 수 있기 때문에 처음에 완벽한 문서를 정리하기보다는 기능을 구현하면서 문서를 계속 업데이트하라고 하신다. 죽은 문서가 아니라 살아있는 문서를 만들기 위해 노력한다. static, final 영상을 보면서 난 아직 사용 언어에 대해서도 잘 활용하지 못한다는 걸 깨달았다. 둘을 이용해서 선언하면 뭔가 변하지 않는다는 건 어렴풋이 알고 있었는데 둘의 차이를 정확하게 설명해 보라 하면 하지 못했을 것 같다. private static final, private final 선언된 변수를 사용하면 둘 다 재할당하지 못하는 건 같다. 하지만 private static final 같은 경우는 아래와 같이 선언과 동시에 초기화해주어야 하고, private final 같은 경우는 객체 생성 시에도 초기화가 가능하다. private static final String test = \"test\"; private final int number; public 생성자(int number){ this.number = number; } 그리고 private static final 같은 경우 메모리에 한 번 올라가면 같은 값을 클래스 내부 전체 필드, 메서드에서 공유하고, private final은 새로운 객체가 생성될 때마다 새로운 값이 할당된다. 그래서 왜 사용할까? static 같은 경우 잘 사용하면 메모리 측면에서도 효율적이고 속도 측면에서도 빠를 수 있다. 매번 인스턴스를 생성할 필요도 없고 객체를 생성하지 않고도 사용 가능하기 때문이다. 그리고 final 같은 경우 readOnly로 만드는 특성 때문에 얘가 불변한다는 것을 보장해 주기 때문에 나중에 유지 보수 관점에서도 추적하기 쉽다. 하지만 그만큼 조심해서 사용해야 한다. static 같은 경우 프로그램이 종료할 때까지 메모리에 할당된 채로 존재하기 때문에 많이 사용하면 성능에 좋지 않다. 그리고 static은 객체를 생성하지 않고 여러 곳에서 데이터를 사용할 수 있기 때문에 객체 지향 프로그래밍 원칙(캡슐화)을 위반한다. 그렇기 때문에 막 사용하는 게 아니고 적절한 곳에 사용하면 된다 테스트 코드의 중요성 테스트 코드를 작성하는 것이 귀찮게 느껴진 적이 누구라도 있을 것이다. 나도 기능을 다 구현하고 나서 지친 상태에서 테스트 코드를 생략하고 바로 그다음 기능을 작성했던 적이 있다. 하지만 다음 기능을 구현할 때도 이전에 구현했던 것이 제대로 돌아갈까 불안한 마음으로 다음 기능을 작성했던 기억이 난다. 그 기능을 완벽하게 짰다고 자신할 수 있으면 모를까 하지만 어떤 사람도 실수를 안 할 수는 없을 것이다. 그렇기 때문에 그 실수한 상태를 모르고 넘어가서 추가 기능을 구현하다 치명적인 에러가 발생하면 앞으로 되돌리는데 더 오랜 시간을 쏟게 될 것이다. 나도 프로젝트에서 진행하던 중 시간이 촉박해 빠르게 기능부터 연달아 구현했던 적이 있는데 오히려 어느 부분에 막혔을 때 어디부터 잘못된 건지 계속 되돌려 찾아봐야 하는 딜레마 상태에 빠졌던 적이 있다. 오히려 그 테스트 코드 작성 안 한 시간 보다 시간을 배로 날려먹는 낭비를 겪었다. 그렇기 때문에 전부터 테스트 코드가 중요하다는것은 머리로 알고 있었는데 이번에 제대로 적용해보면서 더욱 중요성을 체감해 볼 수 있었다. 이번에 도메인마다 단위 테스트를 진행하면서 꼼꼼히 한 결과, 테스트 코드를 작성하기 위해 약간의 시간이 추가 되었지만 나중에 통합적으로 테스트를 했을 때 오히려 깔끔하게 한 번에 통과가 되어 깜짝 놀랐다. 이처럼 테스트 코드로 각각의 기능 확인에 대한 빠른 피드백으로 신뢰성을 증가시키며 개발을 이어나갈 수 있는 것 같다. 그리고 매번 직접 수동으로 번거롭게 돌릴 필요 없이 컴퓨터가 자동으로 test 할 수 있도록 중요 자산이 남는 것이다. 이 자산은 후에도 유지 보수 혹은 인수인계 할 때도 유용하게 쓰일 수 있기 때문에 앞으로도 테스트 코드는 꼭 작성하도록 하자. MVC 도입 지난주 목표로 책임을 좀 더 나누어 결합성을 낮추는 방법으로 MVC 패턴을 도입했다. 하지만 막상 처음부터 MVC 패턴으로 코드를 구현하려고 하니 막막했고 뭐부터 시작해야 될지 감이 안 잡혔다. 그래서 고민하던 도중 프로그램 설계 도식화에 관한 글에 대해 읽게 되었는데 어떤 일을 하기 전에 전체적인 계획 및 도식화하는 작업을 하여 명확화 하는 것이다. 그렇게 지금 하려는 로또 프로그램에 대한 흐름도를 먼저 작성하여 적용해 보게 되었고 흐름도를 작성하고 나니깐 어떤 걸 먼저 작성할지 어떻게 나누어야 할지 조금씩 감이 잡히게 되어 코드를 작성하기 시작했다. https://medium.com/29cm/%EC%9C%A0%EC%A0%80-%EC%A3%BC%EB%AC%B8-%EC%B7%A8%EC%86%8C-%EA%B8%B0%EB%8A%A5-java-%EC%A0%84%ED%99%98%EA%B8%B0-d218e5ecb874 또한 한참 구현하던 중간에 검증은 view, controller, domain 어디서 하는 게 좋은지 비즈니스 로직은 domain이 좋은지 service 가 좋은지 등 머릿속에 고민이 많아지니깐 이 mvc의 기본적인 개념들에 대해 점점 헷갈리기 시작했는데 다음과 같은 글들을 통해 많이 도움을 받았다. 검색하다 보면 우테코의 10분 톡이 자주 나오는데 정말 알기 쉽게 설명들을 잘해주셔서 강추한다. https://www.youtube.com/watch?v=uoVNJkyXX0I https://murphymoon.tistory.com/entry/%EC%9A%B0%EC%95%84%ED%95%9C-%ED%85%8C%ED%81%AC-MVC-%EB%A6%AC%EB%B7%B0-%EB%A0%88%EC%9D%B4%EC%96%B4-MVC-%ED%8C%A8%ED%84%B4-5%EB%A0%88%EC%9D%B4%EC%96%B4 3주 차 진행 과정 구현 기능 작성 흐름도 작성 코드 구현 리팩토링 및 제출 1. 구현 기능 작성 구현 기능 목록 🚀 구현할 기능 목록 로또 발행 로또 번호 범위는 1 ~ 45까지이다. 1보다 작거나 45보다 크면 예외 처리 중복되지 않는 6개의 숫자를 뽑는다. 중복 되면 예외 처리 6개가 아니면 예외 처리 구입 금액만큼 해당하는 로또 발행 로또 1장의 가격은 1000원 당첨 번호와 로또 발행 번호 비교 6개 번호 일치 -&gt; 1등(2,000,000,000) 5개 번호 + 보너스 번호 일치 -&gt; 2등(30,000,000) 5개 번호 일치 -&gt; 3등(1,500,000) 4개 번호 일치 -&gt; 4등(50,000) 3개 번호 일치 -&gt; 5등(5,000) 입력 로또 구입금액 입력 1000원 단위로 입력 입력이 3자리 수 이하이면 예외 처리 숫자가 아닐 시 예외 처리 1000원으로 나누어 떨어지지 않는 경우 예외 처리 당첨 번호 입력 쉼표를 기준으로 6개의 당첨 번호 입력 6개가 아니면 예외 처리(콤마(,)로 나눴을 때) 1 ~ 45 범위 숫자가 아닐 시 예외 처리 중복 일시 예외 처리 보너스 번호 입력 1 ~ 45 범위 숫자가 아닐 시 예외 처리 당첨번호와 중복 일시 예외 처리 출력 발행한 로또 수량 및 번호 출력 로또 번호는 오름차순으로 출력 당첨 내역 출력 수익률 출력 수익률은 소수점 둘째 자리에서 반올림한다. 예외처리 사용자가 잘못된 값 입력한 경우 IllegalArgumentException 발생시키고 에러메시지 출력 후 종료 2. 흐름도 작성 위에 작성했기 때문에 생략 3. 코드 구현 Enum 사용 1주 차의 매직넘버에서 2주 차의 private static final을 거쳐 드디어 Enum을 사용해 보게 됐는데 확실히 Enum을 통해 구현함으로 코드가 더 단순해지고 가독성이 좋아졌다. 그리고 아래처럼 다양하게 활용할 수 있어 편리한 것 같다. public enum LottoStatus { START(1), END(45), SIZE(6), LIMIT(1), PRICE(1000), PERCENT(100), DELIMITER_LENGTH(3); private final int value; LottoStatus(int value) { this.value = value; } public int getValue() { return value; } ... } public enum NoticeMessage { PURCHASING_AMOUNT{ @Override public String toString() { return \"구입금액을 입력해 주세요.\"; } }, LOTTO_COUNT{ @Override public String toString() { return \"개를 구매했습니다.\"; } }, LUCKY_NUMBER{ @Override public String toString() { return \"당첨 번호를 입력해 주세요.\"; } } ... } https://github.com/woowacourse-precourse/java-lotto/pull/528 4. 리팩토링 및 제출 이번에 새롭게 추가된 요구사항이 있어서 그것들을 중점으로 기존 요구사항까지 다시 확인해 보며 추가적으로 리팩토링하여 제출했다. 함수(또는 메서드)의 길이가 15라인을 넘어가지 않도록 구현한다. 함수(또는 메서드)가 한 가지 일만 잘 하도록 구현한다. else 예약어를 쓰지 않는다. Java Enum을 적용한다. 도메인 로직에 단위 테스트를 구현해야 한다. 단, UI(System.out, System.in, Scanner) 로직은 제외한다. 핵심 로직을 구현하는 코드와 UI를 담당하는 로직을 분리해 구현한다. 정리 및 후기 이번 3주 차도 Enum, 도식화, MVC 등 여러 가지로 성장할 수 있었던 뜻깊은 주차였다. MVC를 적용해 처음부터 설계하고 하나하나 분리하여 구현해 봄으로 써 뭔가 퍼즐 조각을 맞추는 듯한 느낌이 들어서 매우 재밌게 몰두했던 것 같다. 특히 제일 처음에 뭐부터 하면 좋을지 아무것도 모르겠을 때 Flow chart를 그리고 나서 딱 명확해졌는데 그때 전율이 왔다 ㅋㅋ 마치 미로에서 지도를 발견한 것처럼 어디로 가야 할지 딱 보였던 것 같다. 벌써 다음 주가 4주 차로 마지막 미션이다ㅠㅠ.. 그동안 많은 피드백으로 열심히 성장해왔으나 과연 그동안 배웠던 것을 나는 잘 적용하고 있는가라고 생각하면 막상 또 고민이 된다. 지금까지 많은 것들을 단기간에 압축해서 배웠기 때문에 익숙하지 않아 잘 다듬지 않으면 체득한 것이라 할 수 없다고 생각한다. 그렇기 때문에 다음 주의 최종 목표는 지금까지 한 것들을 종합하여 정리해 적용해 보며 잘 마무리하는 것이다. 이번 코수타(코치들의 수다 타임)에서도 포비가 이런 말을 했던 기억이 난다. “욕심을 버려라. 누군가 tdd, oop, 클린 코드를 하더라도 내가 그전 걸 소화하지 않고 따라 한다면 그건 의미 없다” “이런 쓰레기(tdd) 같은 거 안 하면 어때” 라고 한건 안 비밀ㅋㅋㅋㅋㅋㅋ *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java precourse woowacourse",
    "url": "/woowacourse/2022-11-15-Tweek/"
  },{
    "title": "우테코 5기 프리코스 2주차 회고",
    "text": "공통 피드백 1주차가 끝이 나고 공통 피드백이 왔다. 요구사항을 정확히 준수한다 커밋 메시지를 의미 있게 작성한다 git을 통해 관리할 자원에 대해서도 고려한다 Pull Request를 보내기 전 브랜치를 확인한다 PR을 한 번 작성했다면 닫지 말고 추가 커밋을 한다 이름을 통해 의도를 드러낸다 축약하지 않는다 공백도 코딩 컨벤션이다 공백 라인을 의미 있게 사용한다 space와 tab을 혼용하지 않는다 의미 없는 주석을 달지 않는다 IDE의 코드 자동 정렬 기능을 활용한다 Java에서 제공하는 API를 적극 활용한다 배열 대신 Java Collection을 사용한다 비록 1:1이 아닌 공통 피드백이지만 누군가에게 이렇게 상세한 피드백을 듣는다는 게 처음이라 너무 좋아 습득할 때까지 계속 반복하여 읽었다. 1주 차에 직접 컨벤션들을 찾아보고 정리를 한 게 있어 좀 더 수월하게 읽혔던 것 같다. 피어 리뷰 스터디 1주 차가 끝난 뒤 깃허브에 우테코 커뮤니티가 생성되었는데 그중 핵심 기능은 피어 리뷰라고 생각한다. 하지만 지금 프리코스 참가자만 무려 3000명 정도 되기 때문에 그냥 올린다면 수많은 글에 묻혀갈 확률이 높다고 생각했고 그래서 피어 리뷰 스터디를 구하게 되었다. 다행히 1주 차가 끝난 다음날 바로 스터디를 구하는 사람들이 있어 참가하게 되었고 해당 스터디 내용은 아래처럼 진행하게 되었다. 피어 리뷰 code review ref https://github.com/woowacourse/woowacourse-docs/tree/code-review/codereview https://soojin.ro/review/review-comments 코드 리뷰를 하고 나니 사람들마다 코드 작성 방법(코딩 스타일, 구현 방법)이 다 달랐던 것 같고 각자 방법마다 배울 점도 있었고 아쉬운 점도 있었기 때문에 그런 부분에 대해서 코드 리뷰도 하고 서로 리뷰를 하다가 토론할 주제가 있으면 서로 의견을 주고받기도 했다. 그중 두 가지 주제를 보자면 1. 부정문을 지양하라 우리 코드들에서 if로 조건을 확인할 때 아래처럼 !(부정문)로 쓰여진게 꽤 있었는데 사람은 부정문을 보면 익숙하지 않기 때문에 한 번 더 생각해야 되서 지양해야 된다는 컨벤션이 있었다. if (!isValid) return -1; 로직... return result; 위에 대한 의견으로 두 가지가 나올 수 있었는데, 첫 번째는 아래처럼 설계하여 긍정문을 사용하는 것 if(isValid) { 로직 return result; } return -1; 두 번째는 부정조건문을 사용하는 예외 경우가 있었다. Early Return을 사용할 때 Form Validation할 때 보안 또는 검사하는 로직에서 나는 원래 두 번째로 했기 때문에 이번 2주 차에서는 첫 번째로 한번 해보자 생각했고 아래처럼 수정하게 되었다. 음.. 지금까지 두 번째로만 사용해와서 그런가 뭔가 고치고도 어색했던 것 같다. 2. 중간 변수가 선언돼야 될까? 우리 코드들을 보면 어떤 것에 반환을 받으면 가독성을 위해 아래의 예처럼 중간 변수를 선언해 주고 있었다. User user = getUser() nickname = getNickname(user); return nickname; 하지만 이전에 박재성 님의 우테코 세미나에서 이런 변수 중복 없이 바로 넣었던 기억이 있어 어차피 이름을 명확하게 지어서 이미 가독성이 확보된 상태에서 추가적으로 뭔가 더 해줄 필요가 없지 않을까 의견을 전달했다. 그리고 좀 더 확실하게 하기 위해 클린 코드 책에서도 추가적인 내용을 공부하였는데 중복을 없애라는 표현이 있었고 두 번째 사진에서 세 번째 사진처럼 중복을 제거하는 걸로 보아 좀 더 납득할 수 있었다. 지난 주 목표 지난 주에 아래 두 가지 목표를 세웠었다. 컨벤션, 클린코드에 대해 더 공부하고 익숙해지기 객체 지향에 대한 공부 컨벤션, 클린코드에 대해 더 공부하고 익숙해지기 1주 차를 진행하면서 컨벤션에 익숙하지 않았기 때문에 1주 차 코드를 보면 같은 컨벤션이라도 뒤죽박죽이다. 예를 들어 어느 곳은 1줄짜리에도 중괄호를 했고 어느 곳은 하지 않았다. 또는 boolean 반환하는 메서드도 어느 곳은 isXXX 가 되있고 어느 곳은 안 되어있다. 이 역시 코드 리뷰에서 지적 당했다ㅠㅠ 그래서 이번 2주 차에는 이런 컨벤션 부분에서는 실수를 하지 않기 위해 1주 차 컨벤션 다시 복습, 피어 리뷰에서 지적된 것 공부, 공통 피드백, 2주차 우테코 에서 제공한 컨벤션을 추가 정리하고 나서 코드 구현에 들어가게 되었다. 1주차 컨벤션 피어 리뷰 공통 피드백 2주차 추가 정리 확실히 이렇게 하고 코드를 구현하니깐 익숙해진 것인지 컨벤션을 계속해서 들여다 볼일이 잘 없어서 시간이 단축되었고 1주 차 보다 좀 더 수월하게 코드를 구현한 것 같다. 객체 지향에 대한 공부 1주 차에서 설계를 하는데 객체 지향에 대해 부족한 게 많은 것 같아 개발자가 반드시 정복해야 할 객체 지향과 디자인 패턴이라는 책을 보고 추가 공부를 하게 되었다. 그중 2주 차를 할 때 중점적으로 참고한 것에 대해 간단하게 정리해 보자면 객체 지향 설계 과정 객체 지향 설계란 다음의 작업을 반복하는 과정이라 할 수 있다. 제공해야 할 기능을 세분화하고, 그 기능을 알맞은 객체에 할당한다. A. 기능을 구현하는데 필요한 데이터를 객체에 추가한다. 객체에 데이터를 먼저 추가하고 그 데이터를 B. 기능은 최대한 캡슐화해서 구현한다. 객체 간에 어떻게 메시지를 주고받을 지 결정한다. 과정1과 과정2를 개발하는 동안 지속적으로 반복한다. 예를 들어 아래와 같은 기능 목록이 있으면 파일에서 데이터 읽기 데이터를 암호화하기 파일에 데이터 쓰기 이들 기능을 제공할 객체 후보를 찾고 각 객체가 어떻게 연결되는지 그려볼 수 있다. 객체의 크기는 한 번에 완성되기 보다는 구현을 진행하는 과정에서 점진적으로 명확해진다. 위 그림에서 암호화 객체는 실제로는 다음의 두 기능을 함께 제공하고 있다. 흐름 제어 (데이터 읽고, 암호화하고, 데이터 쓰고) 데이터 암호화 처음에는 이것이 불명확한 경우가 많다. 구현을 진행하는 과정에서 암호화 알고리즘을 변경해야 할 때, 데이터 암호화 기능과 흐름 제어가 한 객체에 섞여 있다는 것을 알게 될 수도 있다. 또는, 암호화 기능만 테스트하고 싶은데, 흐름 제어 기능과 암호화 기능이 섞여 있어서 암호화 기능만 테스트하는 것이 힘들 때 알게 될 수도 있다. 구현 과정에서 이렇게 한 클래스에 여러 책임이 섞여 있다는 것을 알게 되면 아래와 같이 객체를 만들어서 책임을 분리하게 된다. 이처럼 객체 설계는 한 번에 완성되지 않고 구현을 진행해 나가면서 점진적으로 완성된다. 이는 최초에 만든 설계가 완벽하지 않으며, 개발이 진행되면서 설계도 함께 변경된다는 것을 의미한다. 따라서 설계를 할 때에는 변경되는 부분을 고려한 유연한 구조를 갖도록 노력해야 한다. 이를 기반으로 2주차 숫자 야구 게임에서는 처음에는 Computer, User가 생성되었고 그리고 구현하는 과정에서 결과 객체인 Ball 객체와 흐름제어 객체인 Game 객체가 점진적으로 구현되었다. private 테스트 코드? 그리고 점진적으로 구현되는 과정에서 클래스마다 각 메소드를 테스트하는 과정을 거치면서 진행하게 되었는데 1주 차에서도 테스트 코드를 작성하며 진행했기 때문에 큰 어려움은 없었다. 하지만 어떤 기능을 나타내는 메소드가 private인 경우 테스트를 할 수 없었기 때문에 이를 public으로 해야 되나 아니면 Java의 Reflection이라는 걸 이용해서 해야 되나 아니면 그냥 넘어가야 되나 하고많은 고민을 했다. 그냥 모든 메소드를 테스트하려면은 public으로 하면 편하겠지만 그렇게 되면 접근제어자라는 의미가 없다고 생각했다. 그리고 Reflection을 사용하더라도 private 메소드에 대한 테스트는 깨지기 쉬운 테스트가 되고 테스트에 대한 비용을 증가시키는 요인이 될 수 있다고 한다. 또한 리플렉션 자체 역시 컴파일 에러를 유발하지 못하므로 최대한 사용을 자제해야 된다. 그래서 테스트 같은 경우 우선 public 메소드를 대상으로 진행하게 되었고 성공하는 케이스뿐 아니라 실패하는 케이스까지 모두 나누어 테스트를 진행하여 private 메소드까지 커버할 수 있는 테스트 코드를 작성하게 되었다. 2주차 진행 과정 위에 과정들을 하고 2주차를 시작했기 때문에 상당히 바쁜 시간이였던 것 같다. 그리고 코드를 구현하기 전까지 추가 컨벤션, 라이브러리 분석, 구현 기능 목록까지 작성했기 때문에 구현을 시작하기 까지 시간이 상당히 걸렸다. 우테코에서 제공한 컨벤션에 맞춰 추가 컨벤션 작성 라이브러리 분석 구현 기능 목록 작성 코드 구현 리팩터링 및 제출 1. 추가 컨벤션 작성 1주차에서는 직접 찾아 정리한 것이기 때문에 2주차에는 우테코에 제공한 컨벤션에 맞춰 추가적으로 작성했다. 참고: https://github.com/woowacourse/woowacourse-docs/tree/main/styleguide/java https://google.github.io/styleguide/javaguide.html https://github.com/JunHoPark93/google-java-styleguide 추가 컨벤션(Convention) 자바 컨벤션(Java Convention) 순서와 공간 임포트는 다음과 같은 단계를 따른다: 하나의 블럭안에 static 임포트 포함 하나의 블럭안에 non-static 임포트 포함 만약에 static과 non-static이 둘 다 있다면, 개행을 하고 두 개의 블럭으로 나눈다. 그 이외에는 개행이 있으면 안된다. 클래스에는 static 임포트를 하지 않는다. static 임포트는 static 중처버 클래스에 사용되지 않는다. 그것들은 일반적인 임포트를 사용한다. 괄호는 선택사항에서도 쓰인다. 괄호는 if, else, for, do, while 구문에 쓰이는데 몸체가 없거나 한 줄의 구문에도 괄호가 쓰인다. 비어있지 않은 블럭: K &amp; R 스타일 괄호는 비어있지 않은 블럭과 block-like construct에서 Kernighan과 Ritchie 스타일(Egyptian brackets)을 따른다. 여는 괄호 앞에는 줄 바꿈이 없음 여는 괄호 다음에 줄 바꿈 닫는 괄호 전에 줄 바꿈 닫는 괄호 다음에 줄 바꿈, 그런데 이것은 오직 구문이 끝나거나 메소드, 생성자, 클래스가 끝났을 때 적용된다. 예를들어 else나 콤마뒤에 나오는 부분은 줄 바꿈을 하지 않는다. 열 제한: 120 Java 코드의 열 제한은 120자입니다. “문자”는 유니코드 코드 포인트를 의미합니다. 패키지 이름 패키지명은 전부 소문자로 단순히 서로 뭍여서 연속된 단어로 이루어져 있다. (언더스코어 없음) 예를들어 com.example.deepspace같은 형식이다. com.example.deepSpace혹은com.example.deep_space 는 잘못되었다. 클래스 이름 클래스 이름은 UpperCamelCase 이다. 클래스 이름은 전형적으로 명사나 명사 구이다. 예를들어, Character 혹은 ImmutableList 처럼 말이다. 인터페이스의 이름은 명사나 명사구가 될 수 있다. 예를들어 List. 그러나 가끔은 형용사나 형용사구가 대신 쓰이기도 한다 (예를들어 Readable) 테스트 클래스들은 테스트하려는 클래스의 이름이 앞에오고 끝에 Test를 붙여준다. 예를들어 HashTest 혹은 HashIntegrationTest 함수 이름 함수 이름은 lowerCamelCase 이다. 함수 이름은 전형적으로 동사 혹은 동사 구이다. 예를들어, sendMessage 나 stop이다. 언더스코어는 JUnit 테스트에서 논리적 컴포넌트를 분리시키기 위해 각각을 lowerCamelCase로 변경시켜 나올수 있다. 하나의 전형적인 패턴은_ 이다. 예를들어 pop_emptyStack. 테스트 메소드를 작성하는 하나의 정확한 방법은 없다. 상수 이름 상수는 CONSTANT_CASE를 사용한다: 모두 대문자이고 각 단어는 하나의 언더스코어로 구분하는 형식. 하지만 정확히 상수는 무엇인가? 상수는 static final 필드 인데 그것은 변경될 수 없고 그것들의 메소드는 부작용이 보여서는 안된다. 이것은 원시타입, 문자열 그리고 불변 타입, 불변타입의 불변 컬렉션을 포함한다. 만약 어떤 인스턴스의 상태가 바뀐다면 그것은 상수가아니다. 상수가 아닌 필드의 이름 상수가 아닌 필드 이름은 (static 같은) lowerCamelCase로 작성한다. 이러한 이름들은 전형적으로 명사나 명사구이다. 예를들어, computedValues 혹은 index. 파라미터 이름 파라미터 이름은 lowerCamelCase 이다. public 메서드에서 한개의 문자를 가진 파라미터는 피해야 한다. 지역변수 이름 지역변수는 lowerCamelCase 이다. 심지어 final 이나 불변, 지역변수는 상수로 간주되어서는 안되고 상수 스타일로 기술해서도 안된다. @Override: 항상 사용한다 @Override가 사용가능할 때 이 애노테이션을 붙인다. 이것은 클래스가 슈퍼 클래스의 메서드를 오버라이딩을 하는 것을 나타내기도하고 인터페이스의 메서드를 구현하는 것을 나타낼 수도 있다. 예외: 부모 함수가 @Deprecated가 되면 @Override를 생략할 수 있다. 예외 잡기: 생략 하지 말것 아래 명시되있는 것말고 예외를 잡고 아무것도 안하는 것은 거의 있을 수 없다. (전형적인 반응은 로그를 남기는 것 혹은 불가능하다고 간주되면 AssertionError로 다시 던져준다) 정말로 캐치블럭에서 아무것도 하지 않는것이 정당하다면 주석을 남기는것으로 정당화한다. try { int i = Integer.parseInt(response); return handleNumericResponse(i); } catch (NumberFormatException ok) { // 숫자가 아니다; 괜찮으니 그냥 넘어간다 } return handleTextResponse(response); 예외: 테스트에서 예외를 잡는 부분은 expected, 혹은 expected로 시작하는 이름을 지으면서 무시할 수 있다. 다음 예제는 테스트에서 예외가 나오는게 확실한 상황에서 사용되는 대중적인 형식으로 주석이 필요가 없다. try { emptyStack.pop(); fail(); } catch (NoSuchElementException expected) { } 추가 요구사항 indent(인덴트, 들여쓰기) depth를 3이 넘지 않도록 구현한다. 2까지만 허용한다. 예를 들어 while문 안에 if문이 있으면 들여쓰기는 2이다. 힌트: indent(인덴트, 들여쓰기) depth를 줄이는 좋은 방법은 함수(또는 메서드)를 분리하면 된다. 3항 연산자를 쓰지 않는다. 함수(또는 메서드)가 한 가지 일만 하도록 최대한 작게 만들어라. JUnit 5와 AssertJ를 이용하여 본인이 정리한 기능 목록이 정상 동작함을 테스트 코드로 확인한다. 테스트 도구 사용법이 익숙하지 않다면 test/java/study를 참고하여 학습한 후 테스트를 구현한다. 라이브러리 camp.nextstep.edu.missionutils에서 제공하는 Randoms 및 Console API를 사용하여 구현해야 한다. Random 값 추출은 camp.nextstep.edu.missionutils.Randoms의 pickNumberInRange()를 활용한다. 사용자가 입력하는 값은 camp.nextstep.edu.missionutils.Console의 readLine()을 활용한다. 깃 컨벤션 깃 컨벤션의 경우 저번과 같기 때문에 생략 https://github.com/parkmuhyeun/java-onboarding/blob/parkmuhyeun/docs/Convention.md#%EA%B9%83-%EC%BB%A8%EB%B2%A4%EC%85%98git-convention 2. 라이브러리 분석 이번에 프로그래밍 요구사항 중에 camp.nextstep.edu.missionutils 에서 제공하는 Random, Console API를 사용하여 구현해야 되기 때문에 어떻게 사용해야 될지, 어떤 함정이 있을지 알아보기 위해 먼저 Random, Console에 대해 분석을 하였다. 그리고 마지막에 ApplicationTest에서도 기존의 Assertions를 사용하지 않고 직접 만들어 사용한 Assertions에 대해서도 궁금증이 생겨 추가적으로 분석했다. 라이브러리 분석 🔍 라이브러리 분석 Console 분석 readLine() 사용자의 입력을 받는 메소드 getInstance() 전역변수로 선언되 있는 Scanner 반환 null이거나 닫혀있을 때는 새로운 Scanner 생성 isClosed() scanner의 sourceClosed 변수 반환 sourceClosed : 자원이 종료되었는지 확인 변수(Boolean is true if source is done) getDeclaredField() Java Reflection을 사용하면 특정 인스턴스의 멤버변수, 메소드 등에 접근 할 수 있다. private인 멤버변수에 접근하기 위해서 getDeclaredFields 사용 setAccessible() getDeclaredField 메소드로 멤버변수 정보에는 접근 가능하나 값에는 접근이 불가능하다. private 멤버 변수 값을 read/write 하기 위해서는 setAcccesible을 통해 접근을 허용 해줘야 된다. Randoms 분석 ThreadLocalRandom : 자바7에서 추가된 기능으로 스레드 별로 난수 생성을 할 수 있는 랜덤 클래스, current() 라는 정적 메서드를 통해 객체를 얻도록 되어 있다. pickNumberInList() validateNumbers() 체크 후 파라미터로 넘어온 list 중 에서 랜덤 값 반환 pickNumberInRange() validateRange() 체크 후 startInClusive부터 endInclusive 사이 숫자 중 랜덤 숫자 반환 pickUniqueNumbersInRange() validateRange() 체크 validateCount() 체크 후 list에 startInclusive부터 endInclusive 까지 숫자를 추가해 shuffle() 한 뒤 subList()를 이용해 count 수만큼 반환 중복되지 않는 하나의 리스트에서 모든 수를 반환하기 때문에 반환된 숫자는 모두 unique하다. shuffle() 파라미터로 넘어온 list 내의 숫자 섞기 validateNumbers() 파라미터로 넘어온 list가 비었으면 IllegalArgumentException 발생 validateRange() startInclusive가 endInclusive보다 작으면 IllegalArgumentException 발생 endInclusive가 Integer 최대값과 같은 경우 IllegalArgumentException 발생 전체 범위가 Integer 최대값이 넘어 가는 경우 IllegalArgumentException 발생 validateCount() count가 0보다 작거나 전체 개수 보다 크면 IllegalArgumentException 발생 Assertions 분석 assertSimpleTest() assertTimeoutPreemptively(SIMPLE_TEST_TIMEOUT, executable) 테스트가 SIMPLE_TEST_TIMEOUT 안에 실행되는지 테스트 assertTimeoutPreemptively() : Executable을 실행해 TIMEOUT이 지나는 순간 테스트를 종료해 테스트가 성공한지 확인 assertRandomTest() 테스트가 RANDOM_TEST_TIMEOUT 안에 제대로 실행되는지 테스트 MockedStatic 객체를 이용해 static 메소드 테스트 verification이 실행되면 value값들 Return 왜 MockedStatic을 썼을까 궁금했다. Mockito는 final과 static 메서드를 mocking 하는걸 지원하지 않음 mocking은 이 static mock이 생성된 쓰레드에만 영향을 미치며 다른 쓰레드에서 이 객체를 사용하는 건 안전하지 않다. 이 객체의 ScopedMock.close()가 호출되면 static mock이 해제된다. 이 객체가 닫히지 않으면 static mock 객체는 시작 쓰레드에서 활성 상태로 유지된다. 따라서 예를 들어 JUnit 규칙이나 확장을 사용해 명시적으로 관리되는 경우가 아니면 try-with-resources 문 안에서 이 객체를 만드는 것이 좋다. assertRandomNumberInListTest pickNumberInList(anyList())가 RANDOM_TEST_TIMEOUT 안에 제대로 실행되는지 테스트 특정한 값이 아닌 임의이 값에 대해 실행하고 싶을 때 ArgumentMatchers를 이용해 인자 값을 지정하면 된다. Matchers 클래스는 anyList()뿐 아니라 anyInt(), anyString(), anyLong() 등 다양한 메서드를 제공한다. assertRandomNumberInRangeTest pickNumberInRange(anyInt(), anyInt())가 RANDOM_TEST_TIMEOUT 안에 제대로 실행되는지 테스트 assertRandomUniqueNumbersInRangeTest pickUniqueNumbersInRange(anyInt(), anyInt(), anyInt())가 RANDOM_TEST_TIMEOUT 안에 제대로 실행되는지 테스트 assertShuffleTest shuffle(anyList())가 RANDOM_TEST_TIMEOUT 안에 제대로 실행되는지 테스트 참고: https://onlyfor-me-blog.tistory.com/445 3. 구현 기능 목록 작성 구현 기능 목록 🚀 구현할 기능 목록 컴퓨터 랜덤 숫자 생성 1부터 9 사이의 정수 세자리는 서로 다른 수 사용자 입력 서로 다른 숫자 세 개 입력 잘못된 값 입력시 IllegalArgumentException 발생 세 개 입력 하지 않았을 시 숫자 말고 다른 문자 입력 시 같은 숫자 입력 시 숫자 비교 컴퓨터의 숫자와 입력한 숫자 비교 결과 출력 볼, 스트라이크 개수 표시 (없으면 낫싱) 둘 다 있으면 볼, 스트라이크 순서로 출력 3스트라이크시 게임 종료 3스트라이크가 아닐 시 다시 사용자 입력 게임 종료시 추가 게임 여부 입력 1: 재시작 2: 완전히 종료 그 외의 값 입력시 IllegalArgumentException 발생 4. 코드 구현 https://github.com/woowacourse-precourse/java-baseball/pull/886 5. 리팩터링 및 제출 그렇게 코드를 구현하고 나서 다시 코드를 보며 좀 더 명확히 할 수 있는 건 없는지 혹은 안 지킨 컨벤션은 없는지 확인하며 추가적으로 리팩터링하여 최종적으로 제출하게 되었다. 좀 더 명확히 할 수 있는 것 메소드로 분리하여 명확히 하기 변수명 체크 메소드 분리했을 때 파라미터로 넘어 온 것들은 이름이 대충 지어져있는 것 수정 for문에서도 i, j 같은 변수명 의미있는 변수명으로 수정 불필요한 것 제거 정리 및 후기 코드를 구현하기 전에 피어 코드 리뷰 스터디, 객체 지향 공부, 추가 컨벤션 정리, 라이브러리 분석 등 상당히 많은 시간을 써서 비교적 시작을 늦게 한거 같아 불안한 마음이 있었지만 확실히 이렇게 사전에 미리 공부하고 설계한 결과 구현할 때 수월하게 진행이 되어 시간이 부족하지 않았다. 다시 한번 구현 전 좋은 설계에 대한 중요성을 깨닫는 시간이었다. 하지만 아직 내가 프로젝트에 사용하던 구조인 MVC와는 상당히 거리가 멀었고 클래스를 분리했지만 여전히 Game 클래스에서 실행 흐름과 입출력 기능에 관해 동시 책임을 가지고 있었기 때문에 유연하지 않은 것 같았다. 그렇기 때문에 다음 주 목표로는 MVC 패턴에 대해 공부해서 도입해 볼 예정이다. 직접 프로젝트에서 스프링의 도움을 받아 자주 접해봤지만 직접 구조를 처음부터 설계하고 구현할 생각에 약간 두렵기도 하다. 하지만 동시에 설레는 이 마음은 무엇일까… ㅋㅋㅋ 또, 다음 주에 나는 어떻게 성장해있을지 궁금해진다. 우테코 2주 차 부터는 깃허브 커뮤니티가 생겼는데 위에서 말한 피어 리뷰 말고도 아고라, 학습 컨텐츠, 주간 회고록 카테고리가 있다. 많은 양의 좋은 글들이 올라오는데 뭔가 빠르게 지나가는 느낌이 들어 묻히는 글도 많은 것 같아 아쉽다. 말하는 것은 지식의 영역이고 듣는 것은 지혜의 영역이라고 한다. 말하는 것만큼 듣는 것도 중요하다고 최근에 좀 더 느끼게 되어서 시간이 날 때마다 올라온 글들을 읽고 의견을 남기는데 이는 상대에게도 독자가 생기게 되어 좋고 나에게도 상대방의 지식과 경험을 습득할 수 있어 좋은 것 같다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java precourse woowacourse",
    "url": "/woowacourse/2022-11-08-Sweek/"
  },{
    "title": "우테코 5기 프리코스 1주차 회고",
    "text": "프리코스 시작 오랫동안 기다리던 우테코 프리코스가 드디어 시작되었다! 이번 5기에서는 모든 참가자에게 배움의 기회를 제공하기 위해 이전과 달리 1차 코딩테스트를 없애고 모든 참가자들이 프리코스에 참가할 수 있는 기회를 주었는데 이번에 프론트, 백엔드, 모바일 합쳐 총 3300명 정도가 지원한것 같다. 이런 기회를 제공해 준 우테코에게 감사하고 4주간 많은 걸 배우고 성장해 개발 생태계에 좋은 영향을 줄 수 있도록 성장하여 기여하는 게 보답하는 길이지 않을까 싶다. 프리코스 1주차 이번 1주 차 미션은 이전과는 달리 코딩 테스트 7문제 였는데 아마 2주 차부터 숫자 야구, 자동차 경주 같은 미션을 줄 거라 예상이 된다. 이번 미션은 그냥 풀기에 미션 난이도는 어렵지 않았지만 일주일 동안 이 7문제를 미션으로 내준 것이기 때문에 단순하게 푸는 것만이 중요한 게 아닌 것 같아 바로 시작하지 않았다. 시작하기 전 자바 컨벤션, 클린 코드, 깃 컨벤션 등 미리 사전 공부를 하였고 어떻게 설계를 해야 될지 많은 고민을 했던 것 같다. 하지만 역시 설계는 쉽지 않았고 오히려 처음부터 설계 전체를 생각하니 머리에 과부하가 왔다. 그리고 컨벤션에 익숙하지 않았기 때문에 더 어려움을 겪었던 것 같다. 이번에 사전 공부한 것을 최대한 지키면서 짜려고 노력했지만 아직 부족함을 많이 느꼈다. 하지만 어떻게 코드를 짜야 할지 많은 생각을 해보게 되었고 아직 첫 주 차지만 생각보다 많은 걸 얻어 두 번째 미션에서 더 잘해내어 성장할 수 있을 것 같다. 진행 과정 자바 컨벤션, 클린 코드, 깃 컨벤션 등 사전 공부 문제 이해, 로직 구상 구현 기능 목록 작성 코드 구현 리뷰 및 과제 제출 1. 컨벤션, 클린 코드 등 사전 공부 참고한 자료: 자바 컨벤션: https://naver.github.io/hackday-conventions-java/ 깃허브 컨벤션: https://gist.github.com/stephenparish/9941e89d80e2bc58a153 우테코 cleancode docs: https://github.com/woowacourse/woowacourse-docs/blob/main/cleancode/pr_checklist.md 자바 컨벤션(Java Convention) 공통(Common) 변수명, 클래스명, 메서드명 등에는 영문/숫자/언더스코어만 허용 한국어 발음대로 표기 금지 좋은 예 - asset(자산) 나쁜 예 - jasan(자산) 패키지(Package) 패키지 이름은 소문자로 구성 ex) package com.mu.apigateway 인터페이스(Interface) 인터페이스 이름에 대문자 카멜표기법 적용 ex) CamelCase 인터페이스 이름에 명사/형용사 사용 클래스(Class) 클래스 이름에 대문자 카멜표기법 적용 ex) CamelCase 클래스 이름에 명사 사용 테스트 클래스는 ‘Test’로 끝맺음 메서드(Method) 메서드 이름에 소문자 카멜표기법 적용 camelCase 메서드 이름은 동사/전치사로 시작 ex) toString(), renderHtml() 변수(Variable) 상수는 대문자와 언더스코어로 구성 ex) public final int CHECK = 1; ex) public final String SECRET_KEY = “secret”; 변수에 소문자 카멜표기법 적용 ex) private int accessToken; 클린 코드(Clean code) 추가 체크리스트 한 메서드에 오직 한 단계의 들여쓰기(indent)만 허용했는지? else 예약어를 쓰지 않았는가? 모든 원시값과 문자열을 포장했는가? 콜렉션에 대해 일급 콜렉션을 적용했는가? 3개 이상의 인스턴스 변수를 가진 클래스를 구현하지 않았는가? getter/setter 없이 구현했는가? 단, DTO는 허용 메소드의 인자 수를 제한했는가? 최대 3개 까지 허용, 가능하면 줄이기 위해 노력 코드 한 줄에 점(.)을 하나만 허용했는가? 메소드가 한가지 일만 담당하도록 구현했는가? 클래스를 작게 유지하기 위해 노력했는가? 깃 컨벤션(Git Convention) 커밋 메시지 형식 &lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; &lt;BLANK LINE&gt; &lt;body&gt; &lt;BLANK LINE&gt; &lt;footer&gt; 커밋 메시지의 기본형식은 위와 같고 100자를 넘을 수 없습니다. type feat : 새로운 기능 추가 fix : 버그 수정 docs : 문서 수정 style: 코드 포맷 변경 refactor : 코드 리팩토링 test : 테스트 코드 추가 chore : 빌드, 패키지 매니저 수정 scope 커밋 변경 위치를 지정 ex) $location, $browser, $compile, $rootScope, ngHref, ngClick, ngView, etc… subject 코드 변경 사항에 대한 짧은 요약 현재 시제의 명령어 사용 첫글자 대문자 사용 X 끝에 점(.) 사용 X body 변화에 대한 동기와 이전 행동과의 대조를 포함 현재 시제의 명령어 사용 footer 모든 주요 변경 사항에 대한 설명, 정당성 및 마이그레이션 참고사항 참고 예제 feat($browser): onUrlChange event (popstate/hashchange/polling) Added new event to $browser: - forward popstate event if available - forward hashchange event if popstate not available - do polling when neither popstate nor hashchange available Breaks $browser.onHashChange, which was removed (use onUrlChange instead) docs(guide): updated fixed docs from Google Docs Couple of typos fixed: - indentation - batchLogbatchLog -&gt; batchLog - start periodic checking - missing brace style($location): add couple of missing semi colons 2. 문제 이해, 로직 구상 각 문제를 이해하고 코드를 구현하기 전 어떤 로직으로 풀어야 될지 어떻게 설계해야 될지 많은 고민을 했는데 위에서 말했듯이 한 번에 설계하기엔 아직 나에게 무리였다… 그래서 구현 기능 목록을 작성해 하나하나 풀어나가려고 계획했다. 3. 구현 기능 목록 작성 구현 기능 목록 🚀 구현할 기능 목록 PROBLEM1 페이지 오류 검사 페이지 범위 (3 ~ 398) 벗어날 시 예외 처리 왼쪽페이지 홀수 &amp;&amp; 오른쪽 페이지 짝수 아닐시 예외 처리 왼쪽 페이지 +1 = 오른쪽 페이지가 아닐시 예외 처리 해당 페이지의 가장 큰 숫자 계산 각 자리 수 더한 것, 각 자리 수 곱한 것 중 큰 것 반환 점수 비교 포비가 이기면 1 반한 크롱이 이기면 2 반환 무승부시 0 반환 PROBLEM2 연속된 중복 문자 제거 연속된 중복 문자는 한번에 제거 연속된 중복 문자가 없을때 까지 반복 후 반환 반환한 문자열이 같은 경우 ”” 공백인 경우 PROBLEM3 각 자리수로 분할 해당 숫자가 3, 6, 9인지 검사 number까지 손뼉 수 계산 PROBLEM4 반대 문자로 변경 ’ ‘ -&gt; ‘ ‘ 대문자, 소문자 구별 맞는 페어 문자로 변경(끝 번호 - 자기 번호 + 시작 번호) PROBLEM5 높은 금액 순으로 바꿀 수 있는 화폐로 교환 50000 -&gt; 10000 -&gt; 5000 -&gt; 1000 -&gt; 500 -&gt; 100 -&gt; 50 -&gt; 10 -&gt; 1 PROBLEM6 닉네임마다 나올 수 있는 연속적인 문자 저장 나올 때 마다 해시맵을 이용해 +1씩 저장 저장된 연속적인 문자 중 중복이 있는 유저 확인 해시멥에 값이 2이상이면 중복 문제(연속적인 문자가 겹치는 유저) 있는 이메일 리스트 반환 오름차순 정렬, 중복 제거 PROBLEM7 친구 추가 기능 구현 사용자와 함께 아는 친구를 가진 유저 확인 타임 라인 유저 확인 높은 점수 순으로 최대 5명 반환 추천 점수가 0점은 제외 추천 점수가 같은 경우 이름순으로 4. 코드 구현 https://github.com/parkmuhyeun/java-onboarding 5. 리뷰 및 과제 제출 문제들을 리뷰 하던 도중 6번 문제에서 더 개선 할 수 있는 방법을 떠올리게 되었고 개선하여 최종적으로 제출하였다. 6번 문제의 기능 중 닉네임마다 나올 수 있는 연속적인 문자를 가져오는 부분에서 나는 기존에 모든 가능성을 다 가져와 연산했었다. 그렇게 되면 예를 들어 “카타리나”같은 경우 “카타”, “카타리”, “카타리나”, “타리”, “타리나”, “리나”로 시간 복잡도가 n * (n-1) / 2 즉 O(n^2)이 걸려 총 시간 복잡도가 10000(최대 크루 수) * 19(max 닉네임 길이) * 19으로 3610000이 걸렸다. 이렇게 하여도 문제없으나 다음과 같이 줄이면 더 개선할 수 있다. 어차피 세 글자부터는 이미 두 글자에서 중복으로 처리되기 때문에 연산할 필요가 없어서 두 글자의 조합만 추가하여 계산해도 된다. 즉, “카타리나”에서 “카타”, “타리”, “리나”만으로만 검사해도 “카타리”, “카타리나”, “타리나” 같은 세 글자 이상 포함된 다른 크루들도 처리할 수 있다. 개선하게 되면 n-1이 걸려 O(n)으로 총 시간 복잡도는 10000 * 19 = 190000으로 단축되게 된다. 그렇게 개선하고 나서 Github에 Pull Request하고 우테코 플랫폼에도 제출하여 1주 차는 이렇게 마무리하게 되었다. 정리 및 후기 이번에 상당히 많이 고민하며 설계하고 구현해 보았는데 부족한 점을 많이 느꼈다. 물론 컨벤션도 익숙하지 않지만 특히 설계 부분에서 많이 당황했다. 그동안 객체 지향에 대해 어느 정도 알고 있다고 생각했었는데 그 생각이 완전히 깨졌으며 아직 한참 멀었구나 느꼈다. 그래서 다음 미션까지 두 가지의 목표를 세웠다. 컨벤션, 클린코드에 대해 더 공부하고 익숙해지기 객체 지향에 대한 공부 책 - 개발자가 반드시 정복해야 할 객체 지향과 디자인 패턴 이번에 5기 프리코스에서 새로 우테코 커뮤니티라는 시스템을 도입한다고 했다. 현재는 Slack으로 하고 있는데 2주 차부터는 추가적으로 Github에 커뮤니티를 열어준다고 해서 기대 중이다! Slack에서 다들 유용한 정보 공유, 에러 해결 방법, QnA 등을 이용해 서로 함께 도와가며 힘내고 있다. 이번 주에 Slack 없이 혼자 코딩했으면 너무 쓸쓸했을 것 같다.. 다 같이 파이팅 해가며 함께 진행하는 이 분위기가 너무 좋았는데 이것이 우테코에서 의도했던 바가 아닐까. 우테코 Slack 개발에서는 끝없이 배울 것이 나오고 변화가 빠르기 때문에 공유 문화가 더욱 중요하다고 생각되고 그만큼 협력, 커뮤니케이션 같은 함께 자라기의 중요성이 커졌다. 그렇기 때문에 나도 Slack에서 유용한 정보를 최대한 공유하려고 하고, 많은 분들에게 도움드리려고 노력하고 있다. 그리고 그만큼 다른 분들에게서도 많은 것을 배우고 얻어 가고 있다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java precourse woowacourse",
    "url": "/woowacourse/2022-11-01-Fweek/"
  },{
    "title": "Java - Stream API",
    "text": "우리는 아래 코드처럼 더욱 간결하고 가독성좋은 코드를 본적이 있을 것이다. 이것을 가능하게 해준 Stream API에 대해 알아보자 //Stream 사용한경우 String[] arr = {\"test4\", \"test1\", \"test3\", \"test2\"} List&lt;String&gt; list = Arrays.asList(arr); list.stream().sorted().forEach(System.out::println); //Stream 사용하지 않은 경우 String[] arr = {\"test4\", \"test1\", \"test3\", \"test2\"} List&lt;String&gt; list = Arrays.asList(arr); Collections.sort(arr); for (String s : list) { System.out.println(str); } Stream API란? Java 8부터 Stream API와 람다식, 함수형 인터페이스 등을 지원하면서 Java도 함수형으로 프로그래밍을 할 수 있게 되었다. Stream API는 Collection의 요소를 Stream을 통해 함수형 연산을 지원하는 패키지로 데이터를 처리하는데 자주 사용하는 함수들이 정의 되있다. 왜 함수형 프로그래밍(Functional Programming)이 등장하게 되었을까? 명령형 프로그래밍을 기반으로 했을 때 소프트웨어의 크기가 커질수록 복잡하게 얽혀있는 코드를 관리하기 힘들었기 때문에 이를 해결하기 위해 함수형 프로그래밍이 등장하게 되었다. 함수형 프로그래밍은 모든 것을 순수 함수로 나누어 문제를 해결하는 기법으로, 작은 문제를 해결하기 위한 함수를 작성하여 가독성을 높이고 유지보수를 용이하게 해준다. Stream API의 특징 원본 데이터 조작X Stream API는 원본의 데이터를 조회하여 원본의 데이터가 아닌 별도의 요소들로 Stream을 생성하기 때문에 가공처리를 하더라도 Stream 요소들에서 처리되어 원본 데이터를 변경하지 않는다. 재사용 불가 Stream API는 재사용이 불가하기 때문에 필요할 때 마다 매번 생성해서 사용해야 된다. 만약 닫힌 Stream을 다시 사용하게 되면 IllegalStateException이 발생 반복적 처리 Stream 안에는 반복 문법이 숨겨져 있기 때문에 보다 간결하고 가독성 좋은 코드를 작성할 수 있다. Stream API의 처리 단계 스트림은 데이터를 처리하기 위해 다양한 연산들을 지원하지만 스트림에 대한 연산은 크게 생성하기, 가공하기, 결과만들기의 3가지 단계로 나누어볼 수 있다. List&lt;String&gt; list = Arrays.asList(\"abcde\", \"abc\", \"a\", \"abcdef\", \"ab\"); list .stream() //생성 .filter(s -&gt; s.length() &gt;= 5) //가공 .map(String::toUpperCase) //가공 .sorted() //가공 .collect(Collections.toList()); //결과 만들기 // =&gt; \"abcde\", \"abcdef\" Stream API의 연산 위에서 연산에는 크게 3가지 종류가 있다고 했는데 각 연산들에 대해서 자세히 알아보자. Stream 생성 Stream API를 사용하려면 우선 Stream을 생성해줘야 되는데 Type에 따른 생성방법을 보자 //Collection(list, set 등) List&lt;String&gt; list = Arrays.asList(\"test1\", \"test2\", \"test3\"); Stream&lt;String&gt; stream = list.stream(); //Array Stream&lt;String&gt; stream = Stream.of(new String[] {\"test1\", \"test2\", \"test3\"}); Stream&lt;String&gt; stream = Arrays.stream(new String[] {\"test1\", \"test2\", \"test3\"}); //Primitive(int, long 등) IntStream stream = IntStream.range(1, 100); //1부터 100까지의 숫자를 갖는 stream Stream 가공 생성한 Stream 객체 요소들을 가공해주기 위한 과정으로 여러개의 중간연산이 연결되도록 반환값으로 Stream을 반환한다. Filter //filter(): 조건에 맞는 데이터만 추출 List&lt;String&gt; list = Arrays.asList(\"tasb\", \"badf\", \"abdh\", \"tnga\", \"canh\"); Stream&lt;String&gt; stream = list .stream() .filter(s -&gt; s.startsWith(\"t\")); // -&gt; tasb, tnga Map //map(): 기존의 Stream 요소들을 변환해 새로운 Stream을 형성하는 연산 List&lt;String&gt; list = Arrays.asList(\"tasb\", \"badf\", \"abdh\", \"tnga\", \"canh\"); Stream&lt;String&gt; stream = list .stream() .map(s -&gt; s.toUpperCase()); // -&gt; TASB, BADF, ABDH, TNGA, CANH Sorted //sorted(): Stream 요소들을 정렬 List&lt;String&gt; list = Arrays.asList(\"tasb\", \"badf\", \"abdh\", \"tnga\", \"canh\"); Stream&lt;String&gt; stream = list .stream() .sort() // -&gt; abdh, badf, canh, tasb, tnga Stream&lt;String&gt; stream = list .stream() .sort(Comparator.reverseOrder()) // -&gt; tnga, tasb, canh, badf, abdh Distinct //distinct(): Stream 요소들에 중복된 데이터 제거 List&lt;String&gt; list = Arrays.asList(\"tasb\", \"badf\", \"abdh\", \"tnga\", \"canh\", \"tasb\", \"badf\"); Stream&lt;String&gt; stream = list .stream() .distinct() // -&gt; tasb, badf, abdh, tnga, canh // *우리가 생성한 클래스에 Distinct를 사용하려면 equals와 hashCode를 오버라이드 해야만 제대로 적용 가능 Stream 결과 만들기 앞에서 생성하고 가공된 Stream을 결과로 만드는 과정이다. 최댓값(max), 최솟값(min), 총합(sum), 평균(average), 갯수(count) // 최대값, 최솟값, 평균 같은 경우 비어있는 경우 값을 정할 수 없기 때문에 Optional로 반환하게 된다. OptionalInt min = IntStream.of(1, 2, 3, 4, 5).min(); // -&gt; 1 int max = Intstream.of.max().ifPresent(System.out::println) // -&gt; 5 int average = Intstream.of.average().orElse(-1); // -&gt; -1 // 총합, 갯수 같은 경우 비어있어도 0으로 특정할 수 있기 때문에 원시값을 반환 가능 long sum = IntStream.of(1, 2, 3, 4, 5).sum(); // -&gt; 15 long count = IntStream.of(1, 2, 3, 4, 5).count(); // -&gt; 5 Collect //collect(): List, Set, Map 등 다른 Type의 결과로 수집하고 싶은 경우에 사용 //Student 객체(id, name, age) List&lt;Student&gt; students = Arrays.asList( new Student(1, \"Ann\", 18); new Student(2, \"John\", 17); new Student(3, \"Annie\", 19); new Student(4, \"Mary\", 18); ); List&lt;String&gt; names = students.stream() .map(Student::getAge) .collect(Collectors.toList()); // -&gt; 18, 17, 19, 18 Match // anyMatch(): 해당 조건에 하나라도 만족하는지 // allMatch(): 해당 조건에 모두 만족하는지 // noneMatch(): 해당 조건에 모두 만족하지 않는지 List&lt;String&gt; names = Arrays.asList(\"Ann\", \"John\", \"Annie\", \"Mary\"); boolean anyMatch = names.stream() .anyMatch(n -&gt; n.contains(\"y\")); // -&gt; true boolean allMatch = names.stream() .allMatch(n -&gt; n.contains(\"A\")); // -&gt; false boolean anyMatch = names.stream() .noneMatch(n -&gt; n.contains(\"z\")); // -&gt; true ForEach //forEach(): Stream 요소들을 대상으로 특정한 연산을 수행하고 싶은 경우 사용 List&lt;String&gt; names = Arrays.asList(\"Ann\", \"John\", \"Annie\", \"Mary\"); names.stream() .forEach(System.out::println); // Ann // John // Annie // Mary 참고: https://mangkyu.tistory.com/112 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java Stream FP etc",
    "url": "/etc/java/2022-10-26-Stream/"
  },{
    "title": "Spring - Transaction(트랜잭션)",
    "text": "왜 Transation을 사용할까? 규모가 큰 프로그램을 개발하다 보면 여러 개의 쿼리를 처리해야 상황이 많다. 이 때, 처리하던 도중에 중간에 문제가 생겨버리면 시스템에 큰 오류가 생길 수 있기 때문에 다시 DB의 데이터들은 이전 상태로 되돌아가야 한다. 이렇게 여러 작업을 진행하다 문제가 생겼을 경우 이전 상태로 롤백하기 위해 사용되는 것이 Transaction이다. 어떻게(How)? Transaction은 여러개의 쿼리를 하나의 커넥션으로 묶어 DB에 전송하는데 도중에 문제가 생기면 모든 작업을 이전 상태로 되돌린다. 이를 위해 Transaction은 여러개의 쿼리를 처리할 때 동일한 Connection 객체를 공유한다. Spring이 제공하는 Transaction 기술 Transaction 동기화 개발자가 직접 코드를 작성해서 여러개의 작업을 하나의 트랜잭션으로 관리하려면 굉장히 불필요하고 중복적인 작업이 많을 것이다. 이를 위해 Spring에서 트랜잭션 동기화(Transaction Synchronization) 기술을 제공하고 있다. 트랜잭션 동기화는 트랜잭션을 하기 위한 Connection 객체를 특별한 저장소에 보관해두고 필요할 때 사용할 수 있는 기술이다. 아래와 같이 적용할 수 있다. // 동기화 시작 TransactionSynchronizeManager.initSynchronization(); Connection connection = DataSourceUtils.getConnection(dataSource); // 작업 진행 // 동기화 종료 DataSourceUtils.releaseConnection(connection, dataSource); TransactionSynchronizeManager.unbindResource(dataSource); TransactionSynchronizeManager.clearSynchronization(); 하지만 개발자가 위와 같이 JDBC 종속적인 트랜잭션 동기화 코드를 사용한다면 다른 기술(Hibernate, JTA 등)을 사용할 때 문제가 발생하게 되는데 이런 기술 종속적인 문제를 해결하기 위해 Spring은 트랜잭션 관리 부분을 추상화한 기술을 제공한다. Transaction 추상화 Spring은 PlatformTransactionManager를 이용해 트랜잭션 기술을 공통화한 트랜잭션 추상화 기술을 제공하고 있다. 이로 인해 종속적이지 않고 공통적으로 트랜잭션을 처리할 수 있다. 아래와 같이 트랜잭션을 커밋하고 롤백할 수 있다. public Object invoke(MethodInvoation invoation) throws Throwable { TransactionStatus transactionStatus = this.transactionManager.getTransaction(new DefaultTransactionDefinition()); try { //성공일시 commit Object return = invoation.proceed(); this.transactionManager.commit(transactionStatus); return return; } catch (Exception e) { //에러 발생시 rollback this.transactionManager.rollback(transactionStatus); throw e; } } 위와 같은 코드는 비즈니스 로직 코드와 트랜잭션 관리 코드가 함께 있어 2가지 책임을 가지는데 *AOP를 이용해 핵심 비즈니스 로직과 부가적인 로직으로 분리할 수 있다. AOP(Aspect Oriented Programming): 관점 지향 프로그래밍으로 어떤 관점을 기준으로 비즈니스 로직과 공통 모듈로 분리하는 것. AOP를 이용한 Transaction 분리 public void addUsers(List&lt;User&gt; users) { TransactionStatus transactionStatus = this.transactionManager.getTransaction(new DefaultTransactionDefinition()); try { //비즈니스 로직 for (User user: users) { if(isEmailNotDuplicated(user.getEmail())){ userRepository.save(user); } } this.transactionManager.commit(transactionStatus); } catch (Exception e) { this.transactionManager.rollback(transactionStatus); throw e } } 위를 보면 트랜잭션 관리 코드와 비즈니스 로직 코드가 함께 있어 여러 책임을 가지므로 SRP에 위반되어 분리하는 것이 좋다. 그래서 Spring은 해당 로직을 클래스 밖으로 빼내 별도의 모듈로 만드는 AOP를 적용하여 @Transactional 애노테이션을 제공한다. 이를 적용하면 아래처럼 깔끔하게 비즈니스 로직만 남기고 Transaction을 적용할 수 있다. SRP(Single Responsibility Principle): 클래스는 단 하나의 책임만 가져야 한다. @Service @Transactional @RequiredArgsConstructor public class UserService { private final UserRepository userRepository; public void addUsers(List&lt;User&gt; users) { for (User user : users) { if (isEmailNotDuplicated(user.getEmail())) { userRepository.save(user); } } } } 스프링에서 Transaction 활용 Transaction을 아무데서나 적용하는 것은 좋지않고 데이터를 처리하는 비즈니스 로직을 담고있는 서비스 계층에 적용하는 것이 좋다. 그리고 전체적으로는 읽기전용인 트랜잭션 애노테이션을 선언하고 데이터가 추가, 수정, 삭제가 되는 작업이 이루어지는 메소드에 따로 @Transactional 애노테이션을 선언하는 것이 더 좋다. @Service @RequiredArgsConstructor @Transactional(readOnly = true) public class UserService { private final UserRepository userRepository; public List&lt;User&gt; getUsers() { return userRepository.findAll(); } @Transactional public void addUsers(List&lt;User&gt; users) { for (User user : users) { if (isEmailNotDuplicated(user.getEmail())) { userRepository.save(user); } } } } 참고: https://mangkyu.tistory.com/m/154 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring Transaction study spring",
    "url": "/study/spring/2022-10-17-Transactional/"
  },{
    "title": "Java - List, Set, Map",
    "text": "Java의 List, Set, Map에 대해 알아보자 List List 같은 경우 데이터를 순차적으로 저장하는데 데이터의 중복과 null을 허용한다. ArrayList 단방향 포인트 구조로 각 데이터에 대한 인덱스를 가지고 있어 데이터 검색에 적합하다. 하지만 삽입, 삭제 시 해당 데이터 이후 모든 데이터가 복사되므로 삽입, 삭제가 빈번한 데이터에는 부적합 LinkedList 양방향 포인터 구조로 데이터의 삽입, 삭제 시 해당 노드의 주소지만 바꾸면 되므로 삽입, 삭제가 빈번한 데이터에 적합. 하지만 데이터 검색시 처음부터 노드를 순회하므로 검색에 부적합. Vector ArrayList와 동일한 구조로, 내부에서 자동으로 동기화를 지원해줘서 병렬 처리에 안전하지만, 느리고 무거워서 잘 사용하지 않는다. Set Set 같은 경우 순서없이 Key로만 데이터를 저장하는데 Key의 중복을 허용하지 않는다. HashSet 저장 순서를 유지하지 않는 데이터의 집합으로 해시 알고리즘을 이용해 검색 속도가 매우 빠르다. LinkedHashSet 링크 리스트를 사용하며 저장 순서를 유지하는 HashSet TreeSet 데이터가 정렬된 집합으로 Red Black Tree로 구현이 되어있고 Comparator를 이용해 정렬 방법 지정 가능 Map Map 같은 경우 순서없이 Key, Value로 데이터를 저장하는데 Value는 중복을 허용하지만 Key의 중복을 허용하지 않음. HashMap Key와 Value로 데이터를 저장하는데 저장 순서를 유지하지않고 배열의 index는 내부 해쉬 함수를 통해 계산한다. HashMap같은 경우 동기화를 보장하지 않기 때문에 검색하는 속도가 빠르지만 신뢰성과 안정성이 떨어짐. LinkedHashMap 링크리스트를 이용하며 저장 순서를 유지하는 HashMap TreeMap 키값이 기본적으로 오름차순 정렬되어 저장되고 레드 블랙 트리로 구현된다. 키값에 대한 정렬방법을 Comparator로 지정가능 HashTable 동기화를 보장하기 떄문에 멀티 쓰레드 환경에서 사용가능하다. 하지만 모든 메서드에서 동기화 락을 걸기 때문에 매우 느림. ConCurrentHashMap ConCurrentHashMap 같은 경우도 동기화를 보장해서 멀티 쓰레드 환경에서 사용가능한데 HashTable의 단점을 보완하기 위해서 나옴. 어떤 Entry를 조작하는 경우에만 락을 걸기 때문에 HashTable보다 데이터를 다루는 속도가 빠르다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java list set map etc",
    "url": "/etc/java/2022-10-09-List-Map-Set/"
  },{
    "title": "쿠키와 세션의 차이는 뭘까?",
    "text": "쿠키 쿠키는 클라이언트에 저장되는 키와 값으로 이루어진 파일로 이름, 값, 유효 시간, 경로 등을 포함하고 있다. 클라이언트 상태 정보를 브라우저에 저장해 서버에 요청시 Cookie를 넘겨줄 수 있다. 쿠키는 항상 서버로 넘어가기 때문에 최소한의 정보만 사용하는게 좋고 서버에 전송하지 않고 웹 브라우저 내부에서 데이터를 저장하고 싶으면 웹스토리지를 사용하면 된다. 쿠키는 사용자 로그인 세션 관리나 장바구니 같은 곳에서 활용할 수 있다. 웹스토리지(로컬 스토리지, 세션 스토리지) 로컬스토리지: 브라우저를 닫아도 남아있는 스토리지 세션 스토리지: 브라우저를 닫으면 사라지는 스토리지 쿠키에 대해 더 자세히 알고 싶으면 다음글 참고 쿠키 동작 방식 쿠키의 동작 방식을 로그인 예를 통해 한번 보자 로그인을 하면 서버로 부터 member의 상태값 1을 쿠키에 저장하게 된다. 그 뒤로 서버에 요청하게 되면 쿠키의 memberId 1이 전달되어 누구인지 구별 세션 일정시간 동안 같은 브라우저로 들어오는 요청을 하나의 상태로 보고 그 상태를 유지하는 기술이다. 세션 같은 경우도 쿠키를 사용해서 값을 주고받으며 클라이언트 상태 정보를 유지하는데 쿠키에 중요 정보는 넣지 않고 세션 아이디를 넣어서 비교적 안전하게 저장하게 된다. 세션 동작 방식 세션의 경우도 로그인을 예를 통해 동작 방식을 확인해보자 1.. 로그인을 하면 서버의 세션 저장소에 sessiondId와 그에 맞는 member를 저장한다. 2.. 그리고 sessionId를 쿠키로 전달하게 되고 그 뒤로 서버에 요청하게 되면 sessionId를 서버로 전달해 구별하게 된다. 쿠키 vs 세션 쿠키 같은 경우 저장 위치는 클라이언트이고 세션은 서버에 저장되게 되기 때문에 쿠키 같은 경우 보안에 취약할 수 있다. 하지만 세션 같은 경우 세션id만 쿠키에 저장하기 때문에 비교적 안전하다. 반대로 속도를 보면 쿠키는 클라이언트에 저장되기 때문에 처리속도가 빠르지만, 세션은 실제 저장된 정보가 서버에 있으므로 따로 처리가 필요해 쿠키보다 느리다. 마지막으로 생명주기를 보면 쿠키는 브라우저를 종료해도 계속해서 남아있을 수 있지만 세션 같은 경우는 만료시간에 상관없이 브라우저를 종료하면 삭제된다. 그림 참고: https://www.inflearn.com/course/%EC%8A%A4%ED%94%84%EB%A7%81-mvc-2 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "NETWORK Cookie Session study http web",
    "url": "/study/http%20web/2022-10-01-Cookie-Session/"
  },{
    "title": "상속관계 매핑(Inheritance Relationship Mapping)",
    "text": "상속관계 매핑 관계형 데이터베이스는 상속 관계라는 것이 없지만 그와 유사한 슈퍼타입 서브타입 관계가 있다. 그래서 이를 이용해 객체의 상속 구조와 DB의 슈퍼타입 서브타입 관계를 매핑 해보자. 슈퍼타입 서브타입 논리 모델을 실제 물리 모델로 구현하는 방법에는 3가지가 있다. 조인 전략으로 각각의 테이블로 변환 단일 테이블 전략으로 통합 테이블로 변환 구현 클래스마다 테이블 전략으로 서브타입 테이블로 변환 조인 전략 조인 전략은 각 객체마다 테이블을 생성하는 전략으로 테이블 구조는 아래 그림처럼 된다. @Inheritance(strategy = InheritanceType.JOINED)를 이용하여 조인 전략을 선택가능, 만약 안 써주면 기본전략인 단일 테이블 전략으로 된다. @Entity @Inheritance(strategy = InheritanceType.JOINED) public class Item { @Id @GenerateValue private Long id; private String name; private int price; } @Entity public class Album extends Item { private String artist; } @Entity public class Movie extends Item { private String director; private String actor; } @Entity public class Book extends Item { private String author; private String isbn; } // DB // -&gt; Item, ID: 1, NAME: 아저씨, PRICE:10000 // -&gt; ID: 2, NAME: LOVE DIVE, PRICE:20000 // -&gt; ID: 3, NAME: 어린왕자, PRICE:15000 // -&gt; Album, ARTIST: IVE, ID: 2 // -&gt; Book, AUTHOR: Antoine de Saint-Exupéry, ISBN: 1231423, ID:3 // -&gt; Movie, ACTOR: 원빈, DIRECTOR: 이정범, ID: 1 조인 전략 같은 경우 DTYPE(구분타입)을 안 써도 되지만 써주는게 좋다. 없으면 DB상에서 ITEM만 SELECT해봤을 때 이 ITEM이 ALBUM인지 MOVIE인지 BOOK인지 구별할 수 없음. @DiscriminatorColumn(name=“DTYPE”), @DiscriminatorValue(“XXX”)을 사용해서 편하게 구분 가능 @Entity @Inheritance(strategy = InheritanceType.JOINED) @DiscriminatorColumn //기본설정이 DTYPE이라 name은 생략 public class Item { @Id @GenerateValue private Long id; private String name; private int price; } @Entity @DiscriminatorValue(“A”) public class Album extends Item { private String artist; } @Entity @DiscriminatorValue(“M”) public class Movie extends Item { private String director; private String actor; } @Entity @DiscriminatorValue(“B”) public class Book extends Item { private String author; private String isbn; } // DB // -&gt; Item, DTYPE: M, ID: 1, NAME: 아저씨, PRICE:10000 // -&gt; DTYPE: A, ID: 2, NAME: LOVE DIVE, PRICE:20000 // -&gt; DTYPE: B, ID: 3, NAME: 어린왕자, PRICE:15000 // -&gt; Album, ARTIST: IVE, ID: 2 // -&gt; Book, AUTHOR: Antoine de Saint-Exupéry, ISBN: 1231423, ID:3 // -&gt; Movie, ACTOR: 원빈, DIRECTOR: 이정범, ID: 1 위처럼 DTYPE을 해주면 Item만 봤을 때도 구분가능하게 된다. 조인 전략의 장단점 조인 전략같은 경우는 테이블이 정규화 되있기 때문에 저장공간이 효율적으로 저장될 수 있고 외래키 참조 무결성 제약조건을 활용가능하다. 하지만 조회시 조인을 많이 사용해서 성능이 저하될 수 있다. 그리고 데이터 저장시 INSERT SQL(ITEM + 상속 객체)이 2번 호출될 수 있다. 단일 테이블 전략 단일 테이블은 이름 그대로 하나의 테이블에 상속받은 객체(Album, Movie, Book)의 필드까지 한번에 다 넣는 전략이다. 이 전략 같은 경우 DTYPE(구분 타입)이 없으면 구분이 불가능하기 때문에 @DiscriminatorColumn을 안 적더라도 기본적으로 DTYPE이 적용되어 있다. @Entity @Inheritance(strategy = InheritanceType.SINGLE_TABLE) public class Item { @Id @GenerateValue private Long id; private String name; private int price; } @Entity public class Album extends Item { private String artist; } @Entity public class Movie extends Item { private String director; private String actor; } @Entity public class Book extends Item { private String author; private String isbn; } //DTYPE 같은 경우 따로 @DiscriminatorValue(“XXX”)을 사용 안 했기 때문에 객체의 기본값이 채워짐(Movie -&gt; Movie) // DB // Item: DTYPE: Moive, ID: 1, NAME: 아저씨, PRICE:10000, ARTIST: null, DIRECTOR: 이정범, ACTOR: 원빈, AUTHOR: null, ISBN: null // Item: DTYPE: Album, ID: 2, NAME: Love Dive, PRICE:20000, ARTIST: IVE, DIRECTOR: null, ACTOR, null, AUTHOR: null, ISBN: null // Item: DTYPE: Book, ID: 3, NAME: 어린 왕자, PRICE:15000, ARTIST: null, DIRECTOR: null, ACTOR, null, AUTHOR: Antoine de Saint-Exupéry, ISBN: 1231423 단일 테이블 전략 장단점 하나의 테이블에 있으므로 조인이 필요 없어 일반적으로 조회 성능이 빠르고 조회 쿼리가 단순하다. 하지만 자신과 관련없는 필드에 null을 허용할 수 있는 단점이 있고 단일 테이블에 모든 것을 저장하므로 테이블이 커질 수 있어 상황에 따라서는 조회성능이 오히려 느려질 수도 있다. 구현 클래스마다 테이블 전략 이 전략은 ITEM 테이블을 없애고 그 필드들을 아래 객체(Movie, Album, Book)들로 다 내리는 것인데 권장하지 않는 전략이다. 왜냐하면 어떤 item을 찾을려면 Movie, Album, Book 모든 테이블을 다 뒤져봐야 되기 때문에 매우 비효율적 @Entity @Inheritance(strategy = InheritanceType.TABLE_PER_CLASS) public abstract class Item { @Id @GenerateValue private Long id; private String name; private int price; } @Entity public class Album extends Item { private String artist; } @Entity public class Movie extends Item { private String director; private String actor; } @Entity public class Book extends Item { private String author; private String isbn; } // DB // -&gt; Album, ID: 2, NAME: LOVE DIVE, PRICE:20000, ARTIST: IVE // -&gt; Book, ID: 3, NAME: 어린왕자, PRICE:15000, AUTHOR: Antoine de Saint-Exupéry, ISBN: 1231423 // -&gt; Movie, ID: 1, NAME: 아저씨, PRICE:10000, ACTOR: 원빈, DIRECTOR: 이정범 구현 클래스마다 테이블 전략 장단점 서브 타입을 명확하게 구분해서 처리할 때는 효과적이고 필드가 다 사용하기 떄문에 NOT NULL 제약조건 사용이 가능하다. 하지만 여러 테이블을 함께 조회할 때 성능이 느리고 자식 테이블을 통합해서 쿼리하기가 어렵다. 그리고 변경이라는 관점에서 매우 좋지 않은게 새로운 타입이 추가되면 굉장히 고쳐야 될게 많다. 어떤 전략이 좋을까? 그래서 어떤 전략을 사용하는게 좋을까? 기본적으로 조인 전략을 생각하고 가는게 좋고 거기서 더 생각해볼게 조인 전략과 단일 테이블을 사용했을 때 서로의 Trade Off를 비교해서 선택을 하면 되겠다. 추가적으로 @MappedSuperclass를 설명하려고 하는데 이 매핑은 상속관계 매핑과는 상관없고 편의를 제공해주는 어노테이션이다. @MappedSuperclass @MappedSuperclass는 공통적인 매핑 정보가 필요할 때 사용해볼 수 있다. 우리가 객체마다 주로 등록일이나 수정일, 등록자, 수정자 같은 공통 정보를 귀찮게 반복해서 넣어준적이 있을 것이다. 이런 공통 정보를 반복적으로 넣을 필요 없이 상속받아 처리 해줄 수 있다. @MappedSuperclass public abstract class BaseEntity { private String createBy; private LocalDateTime createDate; private String lastModifiedBy; private LocalDateTime lastModifiedDate; ... } @Entity public class Member extends BaseEntity { ... } @Entity public class Team extends BaseEntity { ... } //DB // Member, ... + createBy, createDate, lastModifiedBy, lastModifiedDate // Team, ... + createBy, createDate, lastModifiedBy, lastModifiedDate 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA Inheritance Mapping study jpa",
    "url": "/study/jpa/2022-09-26-Inheritance/"
  },{
    "title": "운영체제 - 교착 상태(Deadlock)",
    "text": "교착 상태(Deadlock)란? 교착 상태란 둘 이상의 프로세스가 자원을 점유한 상태에서 서로 다른 프로세스에서 점유하고 있는 자원을 요구하며, 서로가 작업을 끝나기만을 기다리며, 무한 대기 상태에 빠진 상황을 말한다. 예를들어, P1은 P2가 들고 있는 자원의 락이 풀리기를 기다리고 있고 P2는 P1이 들고 있는 자원의 락이 풀리기를 기다리고 있는 상황이다. 교착 상태 발생조건 아래 4가지 조건을 모두 만족하는 경우 교착 상태가 발생할 가능성이 있다. 상호 배제(Mutual Exclusion): 한 번에 한 프로세스만 공유 자원을 사용할 수 있다. 점유 대기(Hold and Wait): 자원을 최소한 하나 가진상태에서 다른 자원을 기다린다. 비선점(Non-Preemption): 한 프로세스가 다른 프로세스의 자원 접근 권한을 강제로 취소할 수 없다. 순환 대기(Circular Wait): 두 개 이상의 프로세스가 자원 접근을 기다리는데, 관계에 사이클이 존재한다. 교착 상태 해결법 교착 상태 해결법은 크게 3가지로 나눌 수 있다. 데드락 발생하지 않도록 예방(Prevention)하기 데드락이 발생할 수 있지만 적절히 회피(Avoidance)하기 데드락이 발생하면 탐지(Detection)하여, 데드락에서 회복(Recovery)하기 교착 상태 예방(Prevention)하기 교착 상태가 발생하지 않도록 사전에 예방하는 방법으로 교착상태 발생조건 4가지 조건중 하나를 예방(부정)함으로 써 교착상태를 예방하는 방법입니다. 상호 배제 부정: 한 번에 여러 프로세스 공유 자원을 사용할 수 있다. 점유 대기 부정: 프로세스가 실행되기 전 필요한 모든 자원을 할당하여 프로세스 대기를 없애거나, 자원이 점유되지 않은 상태에서만 자원 요청을 받도록 한다. 비선점 부정: 다른 프로세스에게 자원 접근을 강제로 취소하게 한다. 순환 대기 부정: 사이클이 존재하지 않게 일정한 방향으로 자원을 요구할 수 있게 한다. 하지만 이렇게 각 조건을 하나씩 부정하여 예방하게 되면 시스템의 처리량이나 자원의 효율성을 떨어뜨릴 수 있다. 교착 상태 회피(Avoidance)하기 교착상태가 발생할 수 있는 가능성이 있는 불완전 상태(Unsafe Sate)에서는 자원 할당을 하지 않고 안전한 상태(Safe State)에서만 자원 요청을 허락하는 방법 안정 상태(Safe State): 프로세스들이 요청하는 모든 자원을 데드락이 일어나지 않게 할당할 수 있는 상황, 안전 순서(Safe Sequence)가 있는 경우 안전 순서(Safe Sequence): 교착상태를 일으키지 않고 자원을 할당하는 순서 불안정 상태(Unsafe State): 안정 상태가 아닌 상황으로, 교착상태 발생할 가능성이 있는 상황 회피 알고리즘으로 가장 유명한 은행원 알고리즘을 알아보자 은행원 알고리즘 다익스트라가 제안한 방법으로, 자원을 할당하기 전에 모든 자원의 최대 할당량을 이용해 시뮬레이션하여 안정 상태(Safe State)인지 여부를 검사하여 교착 상태 가능성을 미리 알아보는 것이다. 예를 들어 처음에 시스템이 총 15개의 자원을 가지고 있다고 가정해보자.   최대 자원 요청량 할당 중인 자원량 남은 필요한 자원량 P0 11 7 4 P1 6 3 3 P2 10 2 8 현재 할당 중인 자원량은 7+3+3 으로 12개이다. 그러면 이제 사용 가능한 자원량은 15 - 12 = 3이 된다. 여기서 어떤 순서로 할당하냐에 따라 Safe Sequence인지 아닌지 결정된다. 먼저 P1에 사용 가능한 자원 3개를 할당 (사용 가능한 자원: 3 - 3 = 0) P1의 작업이 끝나면 할당된 자원 6개 반납 (사용 가능한 자원: 0 + 6 = 6) P0에 사용 가능한 자원을 4개 할당 (사용 가능한 자원: 6 - 4 = 2) P0의 작업이 끝나고 할당된 자원 11개 반납 (사용 가능한 자원: 2 + 11 = 13) P2에 사용 가능한 자원 8개 할당 (사용 가능한 자원: 13 - 8 = 5) p2 작업이 끝나고 나면 할당된 자원 10개 반납 (사용 가능한 자원: 5 + 10 = 15) 위 순서 처럼 P1 -&gt; P0 -&gt; P2 순서로 할당하게 되면 Safe Sequence를 만족하게 된다. 그러나 은행원 알고리즘 같은 경우 미리 자원의 최대 요청량을 알아야 되고, 할당할 수 있는 자원수가 일정해야 되는 등 제약조건이 많아 실제로 복잡한 환경에서 사용하기 어렵다. 교착 상태 탐지(Detection), 회복(Recovery)하기 교착 상태 탐지 알고리즘이나 자원 할당 그래프를 통해 교착 상태가 발생했는지 탐지하고, 교착 상태가 탐지되었다면 회복 기법을 통해 복구합니다. 회복(Recovery) 기법 프로세스 종료: 교착 상태에 있는 프로세스를 종료하는 방법으로 모든 프로세스를 종료 시키는 방법과, 하나씩 종료해가는 방법이 있다. 자원 선점: 프로세스에 할당된 자원을 선점해서, 교착 상태가 해결될 때까지 그 자원을 다른 프로세스에 할당해 주는 방법 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "os Deadlock etc operating system",
    "url": "/etc/operating%20system/2022-09-23-Deadlock/"
  },{
    "title": "IPC - 프로세스 간의 통신",
    "text": "IPC(Inter Process Communication)이란? IPC란 내부 프로세스 간의 통신을 하는 것이다. 프로세스 혼자 작업을 할 수도 있지만 프로세스끼리 통신을 하여 데이터를 주고 받어 작업 속도를 높이거나 편의를 향상 시킬 수 있다. 하지만 각 프로세스는 독립적인 메모리 공간을 가지고 있기 때문에 별도의 매커니즘이 필요하며 이를 위해 커널 영역에서 IPC라는 내부 프로세스간 통신을 제공하고 프로세스들은 IPC를 이용해 통신을 하게 된다. 프로세스 통신 스레드는 메모리 공간과 자원을 공유하지만 프로세스는 fork같은 함수로 생성되면서 PC를 포함하여 메모리 공간을 복사하여 별도의 자원을 할당하기 때문에 프로세스 같은 경우는 통신할 공간이 없어 통신을 위한 별도의 공간을 만들어주어야 한다. IPC 종류 IPC에는 기본적으로 메시지 패싱(Message Passing), 데이터 공유(Shared Memory) 두 가지 방법이 있다. Messaging Passing (메시지 패싱) 커널이 메시지를 대신 전달해주는 방법으로 Direct Communication과 Indirect Communication방식이 있다. Direct Communication(오른쪽 그림)은 Process A가 커널에 메시지를 직접 주고 그 메시지를 커널이 Process B에게 메시지를 직접 전달하는 것이고 Indirect Communication(왼쪽 그림)은 Process A가 메시지를 메시지 큐라는 곳에 넣어두고 Prcoess B가 그 메시지큐에 가서 읽어오는 방식이다. Message Passing같은 경우 커널이 알아서 동기화해주기 때문에 안전하고 동기화 문제가 없다는 장점이 있지만 오버헤드가 커서 성능이 떨어진다는 단점이 있다. Shared Memory (데이터 공유) 두 프로세스 간의 공유할 수 있는 메모리를 생성 후 데이터를 교환하는 방법으로 중개자 없이 곧바로 메모리에 접근할 수 있기 때문에 성능이 좋지만 동기화 문제가 있을 수 있다. 참고: https://jhnyang.tistory.com/24 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "os IPC Message-Queue Shared-Memory etc operating system",
    "url": "/etc/operating%20system/2022-09-22-IPC/"
  },{
    "title": "다양한 연관관계 매핑(Various Association Mapping)",
    "text": "이전글에서 연관관계가 왜 필요한지, 무엇인지 자세하게 알아보았다. 또한 양방향, 연관관계의 주인에 대해서도 설명했으니 이해가 안가면 읽고오자. 이번에는 어떤 연관관계가 있는지 알아보자. 다대일 (N:1) 가장 많이 사용하는 연관관계로 다대일의 반대는 일대다 관계이다. 다대일 단방향 @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; @ManyToOne @JoinColumn(name = \"TEAM_ID\") private Team team; … } @Entity public class Team { @Id @GeneratedValue private Long id; private String name; … } 다대일 양방향 외래키가 있는 쪽을 연관관계의 주인으로 하면 되고 양쪽을 서로 참조하도록 반대에도 참조를 추가해주면 된다. @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; @ManyToOne @JoinColumn(name = \"TEAM_ID\") private Team team; … } @Entity public class Team { @Id @GeneratedValue private Long id; private String name; @OneToMany(mappedBy = \"team\") //양방향 추가 List&lt;Member&gt; members = new ArrayList&lt;Member&gt;(); … } 일대다 (1:N) 우선 설명하기전에 말하자면 일대다 같은경우는 권장하지 않는다. 그 이유는 밑에서 설명하겠다. 일대다 보단 다대일 관계를 사용하자. 일대다 단방향 @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; … } @Entity public class Team { @Id @GeneratedValue private Long id; private String name; @OneToMany @JoinColumn(name = \"TEAM_ID\") List&lt;Member&gt; members = new ArrayList&lt;&gt;(); … 그림을 보면 다른 테이블에 연관관계가 매핑되있는걸 알 수 있다. 이게 어쩔 수 없는게 DB입장에서 보면 N쪽에 외래키가 있을 수 밖에 없어서 일(1)쪽에서 단방향을 주게 되면 이렇게 된다. 객체와 테이블의 차이 때문에 반대편 테이블의 외래 키를 관리하는 특이한 구조가 된다. 이렇게 되면 Team을 업데이트 칠때 Member의 외래키를 업데이트 해야되서 자신의 테이블이 아니고 다른 테이블(Member)에 쿼리가 나가게된다. 그러면 쿼리 같은걸 추적할 때 매우 헷갈릴 수 있을 뿐아니라 쿼리가 추가로 나가게 되기 때문에 권장 하지 않는다. 그리고 일대다 같은 경우 @JoinColumn을 꼭 사용해야 되는데 그렇지 않으면 조인테이블 방식을 사용하여 중간에 테이블이 새로 추가된다. 일대다 양방향 사실 일대다 양방향은 공식적으로 존재하지는 않고 꼼수를 써서 사용할 수 있다. @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; //양방향 추가 @ManyToOne @JoinColumn(name = \"TEAM_ID\", insertable = false, updatable = false) private Team team; … } @Entity public class Team { @Id @GeneratedValue private Long id; private String name; @OneToMany @JoinColumn(name = \"TEAM_ID\") List&lt;Member&gt; members = new ArrayList&lt;&gt;(); … @JoinColumn(insertable=false, updatable=false)으로 읽기 전용필드를 사용해서 양방향 처럼 사용하는 방법이다. 어쨋든 이런 방법이 있지만 가급적 일대다 보단 다대일을 사용하자 일대일 (1:1) 일대일 관계같은 경우 주 테이블이나 대상 테이블 중에 외래 키 선택이 가능하고 외래 키에 데이터베이스 유니크(UNI) 제약조건을 추가해야 한다. 일대일: 주 테이블에 외래 키 단방향 @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; //일대일 추가 @OneToOne @JoinColumn(name = \"LOCKER_ID\") private Locker locker; … } @Entity public class Locker { @Id @GeneratedValue private Long id; private String name; } 일대일: 주 테이블에 외래 키 양방향 다대일 같이 단뱡향에서 반대편에 참조(+mappedBy) 추가해주면 된다. @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; @OneToOne @JoinColumn(name = \"LOCKER_ID\") private Locker locker; … } @Entity public class Locker { @Id @GeneratedValue private Long id; private String name; // 양방향 추가 @OneToOne(mappedBy = \"locker\") private Member member; } 일대일: 대상 테이블에 외래 키 단방향 대상 테이블에 외래키가 있는 경우 단뱡향은 지원하지 않고 양방향만 지원한다. 일대일: 대상 테이블에 외래 키 양방향 사실 이 경우는 아까 주 테이블에 외래 키 양방향을 뒤집은 것이다. 일대일 정리 그래서 어디에 외래 키가 있는게 좋을까? 각각 trade off가 있어서 어느게 더 좋다 할 수 없고 DBA와 개발자간의 협의가 필요하다. 각각의 장단점을 보자. 주 테이블에 외래 키 객체지향 개발자 선호 JPA 매핑 편리 주 테이블만 조회해도 대상 테이블에 데이터가 있는지 확인 가능하다 하지만 값이 없으면 외래키에 null을 허용해서 DBA가 싫어할 수 있음 대상 테이블에 외래 키 DBA가 선호하는 방법으로 이렇게 하는 경우 주 테이블과 대상 테이블을 일대일에서 일대다 관계로 변경할 때 그냥 유니크 제약조건만 없애면 되기 때문에 테이블 구조 유지 가능 하지만 지연 로딩으로 설정해도 항상 즉시 로딩 된다. 프록시 할 때 값이 있는지 없는지 알아야 되기 때문에 그 때 Locker 테이블을 뒤져, 즉시 로딩이 됨 다대다 (N:M) 관계형 데이터베이스 같은경우 두 개의 테이블로 다대다 관계를 표현할 수 없고 중간에 연결 테이블을 하나 추가해서 일대다, 다대일로 풀어내야 한다. 하지만 객체같은 경우 컬렉션을 사용해 객체 두 개로 다대다를 나타낼 수 있다. @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; @ManyToMany @JoinTable(name = \"MEMBER_PRODUCT\") private List&lt;Product&gt; products = new ArrayList&lt;&gt;(); … } @Entity public class Product { @Id @GeneratedValue private Long id; private String name; /* 양방향인 경우 추가 @ManyToMany(mappedBy = \"products\") private List&lt;Member&gt; members = new ArrayList&lt;&gt;(); */ … } @ManyToMany를 사용하여 다대다를 나타낼 수 있으며 @JoinTable로 연결 테이블을 지정할 수 있고 단방향, 양방향 둘다 가능하다. 하지만 편리해보이지만 실무에서는 사용하지 않는다. 왜냐하면 테이블이 숨겨져있기 때문에 중간에 예상치 못한 쿼리가 발생할 수 있으며, 중간에 매핑 정보만 들어가기 때문에 주문시간, 수량 같은 추가 데이터를 넣을 수 없다. 다대다 한계 극복 연결 테이블을 엔티티로 만들어서 @ManyToMany를 @OneToMany, @ManyToOne으로 나누어서 사용 @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; @OneToMany(mappedby = \"member\") private List&lt;MemberProduct&gt; memberProducts = new ArrayList&lt;&gt;(); … } @Entity public class MemberProduct { @Id @GeneratedValue private Long id; @ManyToOne @JoinColumn(name = \"MEMBER_ID\") private Member member; @ManyToOne @JoinColumn(name = \"PRODUCT_ID\") private Product product; private int count; private int price; private LocalDateTime orderDateTime; } @Entity public class Product { @Id @GeneratedValue private Long id; private String name; @OneToMany(mappedby = \"product\") private List&lt;MemberProduct&gt; memberProducts = new ArrayList&lt;&gt;(); … } 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA Association study jpa",
    "url": "/study/jpa/2022-09-20-Various-Associations/"
  },{
    "title": "연관관계 매핑(Association Mapping)",
    "text": "객체의 참조와 테이블의 외래 키를 어떻게 매핑하는지 알아보자 연관관계가 필요한 이유 만약 연관관계가 없어 객체를 테이블에 맞추어 모델링하는 경우를 생각해보자. @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; //참조 대신에 외래 키를 그대로 사용 @Column(name = \"TEAM_ID\") private Long teamId; … } @Entity public class Team { @Id @GeneratedValue private Long id; private String name; … } 이렇게 되면 객체를 다루는 것이 아닌 외래 키 식별자를 직접 다루게 된다. 식별자로 저장하고 조회하게 되는데 계속 DB한테 물어보며 꺼내게 된다. 이 경우 객체 지향적이지 않다. //팀 저장 Team team = new Team(); team.setName(\"TeamA\"); em.persist(team); //회원 저장 Member member = new Member(); member.setName(\"member1\"); member.setTeamId(team.getId()); em.persist(member); //조회 Member findMember = em.find(Member.class, member.getId()); //연관관계가 없음 Team findTeam = em.find(Team.class, team.getId()); 이렇게 객체를 테이블에 맞추어 데이터 중심으로 모델링 하게되면 협력 관계를 만들 수 없다. 자, 이제 연관관계를 사용해서 해보자. 단방향 연관관계 @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; private int age; // @Column(name = \"TEAM_ID\") // private Long teamId; //객체의 참조와 테이블의 외래 키를 매핑 @ManyToOne @JoinColumn(name = \"TEAM_ID\") private Team team; … } 이렇게되면 참조로 연관관계를 조회할 수 있게 되어 객체끼리 탐색이 가능해진다. //팀 저장 Team team = new Team(); team.setName(\"TeamA\"); em.persist(team); //회원 저장 Member member = new Member(); member.setName(\"member1\"); member.setTeam(team); //단방향 연관관계 설정, 참조 저장 em.persist(member); //조회 Member findMember = em.find(Member.class, member.getId()); //참조를 사용해서 연관관계 조회 Team findTeam = findMember.getTeam(); //수정 Team teamB = new Team(); teamB.setName(\"TeamB\"); em.persist(teamB); // 회원1에 새로운 팀B 설정 member.setTeam(teamB); 양방향 연관관계와 연관관계의 주인(*매우 중요) 양방향같은 경우 처음 보면 어려울 수 있다. 양방향 연관관계를 이해하려면 객체와 테이블이 관계를 맺는 차이를 알아야 한다. 양방향에서 테이블 연관관계는 위에서 설명한 단방향 테이블에서 변하지 않는다. 즉, 테이블은 방향자체가 없다고 할 수 있다. 굳이 표현하자면 하나의 외래키로 양쪽을 조인하면 되기때문에 양방향이라 할 수 있겠다. 하지만 객체는 방향이 존재하는데 단방향이면 반대방향에서 조회할 수 없다. 그래서 양쪽에서 조회가 필요할 경우 양방향으로 설정해줘야 된다.(각 객체에서 참조값이 있어야 함) @Entity public class Member { @Id @GeneratedValue private Long id; @Column(name = \"USERNAME\") private String name; private int age; @ManyToOne @JoinColumn(name = \"TEAM_ID\") private Team team; … } @Entity public class Team { @Id @GeneratedValue private Long id; private String name; @OneToMany(mappedBy = \"team\") //mappedBy는 뒤에서 설명하겠다. List&lt;Member&gt; members = new ArrayList&lt;Member&gt;(); … } 위처럼 양방향 관계를 설정해주게 되면 반대 방향으로도 객체 그래프 탐색이 가능하다. //조회 Team findTeam = em.find(Team.class, team.getId()); //역방향 조회 int memberSize = findTeam.getMembers().size(); 객체와 테이블의 양방향 관계 사실 객체의 양방향 관계는 양방향 관계가 아니라 서로 다른 단방향 관계 2개이다. 객체를 양방향으로 참조하려면 단방향 연관관계를 2개 만들어야 한다. class A{ B b; } class B { A a; } 하지만 테이블은 외래 키 하나로 두 테이블의 연관관계를 관리한다. 즉, MEMBER.TEAM_ID 외래키 하나로 양쪽을 조인할 수 있다 SELECT * FROM MEMBER M JOIN TEAM T ON M.TEAM_ID = T.TEAM_ID SELECT * FROM TEAM T JOIN MEMBER M ON T.TEAM_ID = M.TEAM_ID 테이블 외래 키는 한 갠데 객체에는 두 개기 때문에 혼란이 오게 된다. 그래서 여기서 연관관계의 주인이라는 규칙이 생기게 되었다. 연관관계의 주인 양방향 매핑 규칙 객체의 두 관계중 하나를 연관관계의 주인으로 지정 연관관계의 주인만이 외래키를 관리(등록, 수정) 주인이 아닌쪽은 읽기만 가능 주인은 mappedBy 속성을 사용하지 않고, 주인이 아니면 mappedBy 속성으로 주인을 지정 위에서 @OneToMany(mappedBy = “team”) 사용 그래서 연관관계의 주인을 지정해줘야 되는 건 알겠는데 누구를 주인으로 해야될까? 외래키가 있는 곳(N인곳)을 주인으로 정하면 된다. 반대로 하게 되면 헷갈릴 수 있다. 양방향 매핑시 주의 연관관계의 주인에 값을 입력하지 않는 경우 Team team = new Team(); team.setName(\"TeamA\"); em.persist(team); Member member = new Member(); member.setName(\"member1\"); //역방향(주인이 아닌 방향)만 연관관계 설정 team.getMembers().add(member); em.persist(member); //=&gt; Member //=&gt; ID: 1, USERNAME: member1, TEAM_ID: null team 같은 경우 mappedBy가 설정된 곳이기 때문에 넣어줘도 결국 member의 외래키에는 아무것도 설정되지 않는다. Team team = new Team(); team.setName(\"TeamA\"); em.persist(team); Member member = new Member(); member.setName(\"member1\"); team.getMembers().add(member); //연관관계의 주인에 값 설정 member.setTeam(team); em.persist(member); //=&gt; Member //=&gt; ID: 1, USERNAME: member1, TEAM_ID: 2 위처럼 연관관계의 주인인 Member를 이용하여 설정해 주어야 된다. 그럼 그 위의 코드 team.getMembers().add(member);는 필요가 없을까? 아니다. 순수한 객체 관계를 고려하면 항상 양쪽 다 값을 입력해야 된다. 안 넣어주면 나중에 문제가 생긴다. 그래서 양방향 매핑은 항상 좋을까? 오히려 양방향을 쓰게 되면 신경 쓸게 많아진다. 그렇기 때문에 처음에 설계할 때 일단 단방향 매핑만으로만 완성하고 뒤에 양방향이 필요할 때 그때 추가하면 된다. 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA Association-Mapping study jpa",
    "url": "/study/jpa/2022-09-18-Association-Mapping/"
  },{
    "title": "의식적인 연습으로 TDD, 리팩토링 연습하기 세미나 정리 및 후기",
    "text": "이 글은 우아한 테크 세미나, OKKYCON에서 강연을 하신 박재성님의 의식적인 연습으로 TDD, 리팩토링 연습하기에 대한 정리 및 후기입니다. 참고 링크: 우아한 테크 세미나 -youtube.com/watch?v=bIeqAlmNRrA, OKKYCON - https://www.youtube.com/watch?v=cVxqrGHxutU 전에 코드 리뷰에 관한 글을 올렸는데, 그래서 서로 의논해서 코드의 품질을 향상시키겠다는 취지인 건 알겠는데 어떻게 향상시키는 거야? 그 Base(방법)를 어떻게 잡아야 되지라는 생각을 할 수 있다. 그래서 오늘의 주제를 포스트하게 되었다. 사실 이 주제 같은 경우 예전에 해당 세미나를 접하고 공부를 해봤지만 그때 당시엔 어려워서 나중에 다시 공부해서 정리하자 마음먹었는데 최근에 다시 정리하게 됐다. TDD? 리팩토링? TDD, 리팩토링 말만 들으면 굉장히 멋져보이지만 막상 해볼려고 하면 절대 쉽지 않다. 심지어 TDD, 리팩토링을 무작정 많이 연습 한다고 해서 느는것도 아니다. 생각만큼 쉽게 연습할 수 있는 놈이 아니고 의식적인 연습이 필요하다. 의식적인 연습의 7가지 원칙 효과적인 훈련 기법이 수립되어 있는 기술 연마 개인의 컴포트 존(Comfort Zone)을 벗어난 지점에서 진행, 자신의 현재 능력을 살짝 넘어가는 작업을 지속적으로 시도 명확하고 구체적인 목표를 가지고 진행 신중하고, 계획적이다. 즉, 개인이 온전히 집중하고 ‘의식적’으로 행동할 것을 요구 피드백과 피드백에 따른 행동 변경을 수반 효과적인 심적 표상을 만들어내는 한편으로 심적 표상에 의존 기존에 습득한 기술의 특정 부분을 집중적으로 개선함으로써 발전시키고, 수정하는 과정을 수반 -&gt; 무작정하지말고 의식적인 연습으로 효과적으로 연습하자 그래서 뭐부터 해야될까? TDD랑 일단 단위 테스트는 다른데 TDD는 어렵기 때문에 단위테스트 부터 먼저 연습하자. 1단계 - 단위테스트 연습 내가 사용하는 API 사용법을 익히기 위한 학습 테스트에서 시작 예를 들어 자바 String 클래스의 다양한 메소드(함수) 사용법 자바 ArrayList에 데이터를 추가, 수정, 삭제하는 방법 -&gt; 내가 만든 API가 입력을 넣고 결과가 잘 나오는지부터 부터 연습 2단계 - TDD 연습 처음부터 어려운 프로젝트를 하지말자. 어려운 문제를 해결하는 것이 목적이 아니라 TDD 연습이 목적, 난이도가 낮거나 자신에게 익숙한 문제로 시작하는 것을 추천. 먼저 장난감 프로젝트를 활용해 연습하면 좋음 (웹, 모바일 UI나 DB에 의존관계를 가지지 않는 요구사항으로 연습) TDD Circle of life Test Fail -&gt; Test Pass -&gt; refactoring인데 처음부터 다하기 힘들다. 그래서 처음에 2단계까지 하는거 추천. Test Fail -&gt; Test Pass 2단계까지 되면 refactoring 추가 3단계 Refactoring 연습 테스트 코드는 변경하지 말고 테스트 대상 코드(프로덕션 코드)를 개선하는 연습을 한다. 처음에 클래스는 한가지 역활만 해야 한다고 클래스를 분리하라고 하면 너무 추상적일 수 있다. 그러면 처음하는 사람은 뭘해야될지 막막할 수도 있고 멘붕이 올 것이다. 그래서 우선 측정 가능한 방법으로 작은 것 부터 차근차근 연습을 시작해보자. 예를들어 아래의 과정을 한번 보자. 약간 길 수 있지만 제일 중요한 부분이니 시간내서 봐보자. 한 메서드에 오직 한 단계의 들여쓰기(indent)만 한다. public class StringCalcculator { public static int splitAndSum(String text){ int result = 0; if (text == null || text.isEmpty()){ result = 0; }else { String[] values = text.split(\",|:\"); for (String value : values){ //이 부분이 들여쓰기가 2인 부분 result += Integer.parseInt(value); } } return result; } } 여기서 들여쓰기가 2인 부분을 메소드를 생성하여 1로 줄여 준다. public class StringCalcculator { public static int splitAndSum(String text){ int result = 0; if (text == null || text.isEmpty()){ result = 0; }else { String[] values = text.split(\",|:\"); result = sum(values); //변경된 부분 } return result; } } private static int sum(String[] values){ int result = 0; for (String value : values){ result += Integer.parseInt(value); } return result; } else 예약어를 쓰지 않는다. public class StringCalcculator { public static int splitAndSum(String text){ int result = 0; if (text == null || text.isEmpty()){ result = 0; }else { //else 부분 String[] values = text.split(\",|:\"); result = sum(values); } return result; } } private static int sum(String[] values){ int result = 0; for (String value : values){ result += Integer.parseInt(value); } return result; } 다음 처럼 else를 없애고 사용할 수 있다. public class StringCalcculator { public static int splitAndSum(String text){ if (text == null || text.isEmpty()){ return 0; } String[] values = text.split(\",|:\"); return sum(values); } } private static int sum(String[] values){ int result = 0; for (String value : values){ result += Integer.parseInt(value); } return result; } 메소드가 한 가지 일만 하도록 구현하기 public class StringCalcculator { public static int splitAndSum(String text){ if (text == null || text.isEmpty()){ return 0; } String[] values = text.split(\",|:\"); return sum(values); } } //현재 더하기를 할 뿐아니라 String을 숫자로 바꾸는 역활도 하고 있음. private static int sum(String[] values){ int result = 0; for (String value : values){ result += Integer.parseInt(value); } return result; } 숫자로 바꿔주는 메소드와 덧셈만 하도록 하는 메소드로 나눠 주자 public class StringCalcculator { public static int splitAndSum(String text){ if (text == null || text.isEmpty()){ return 0; } String[] values = text.split(\",|:\"); int[] numbers = toInts(values); return sum(numbers); } } //숫자로 바꿔주는 메소드 private static int[] toInts(String[] values){ int[] numbers = new int[values.length]; for (int i = 0; i &lt; values.length; i++) { numbers[i] = Integer.parseInt(values[i]); } return numbers; } //값을 더하는 메소드 private static int sum(int[] numbers){ int result = 0; for (int number : numbers){ result += number; } return result; } 로컬 변수가 정말 필요한가? (다른 곳에서 사용하지 않는다면 굳이 필요없다.) public class StringCalcculator { public static int splitAndSum(String text){ if (text == null || text.isEmpty()){ return 0; } return sum(toInts(text.split(\",|:\"))); // 로컬 변수들 삭제 } } private static int[] toInts(String[] values){ int[] numbers = new int[values.length]; for (int i = 0; i &lt; values.length; i++) { numbers[i] = Integer.parseInt(values[i]); } return numbers; } private static int sum(int[] numbers){ int result = 0; for (int number : numbers){ result += number; } return result; } compose method 패턴 적용 compose method: 메소드(함수)의 의도가 잘 드러나도록 동등한 수준의 작업을 하는 여러단계로 나눈다. public class StringCalcculator { public static int add(String text){ if (isBlank(text)){ //같은 단계로 만들기 위해 isBlank return 0; } return sum(toInts(split(text))); // 같은 단계로 만들기 위해 split } } private static boolean isBlank(String text){ return text == null || text.isEmpty(); } private static String[] split(String text) { return text.split(\",|:\"); } private static int[] toInts(String[] values){ int[] numbers = new int[values.length]; for (int i = 0; i &lt; values.length; i++) { numbers[i] = Integer.parseInt(values[i]); } return numbers; } private static int sum(int[] numbers){ int result = 0; for (int number : numbers){ result += number; } return result; } 자 이렇게 하고나면 add() 메소드를 처음 읽는 사람에게 어느 코드가 더 읽기 좋을까? Refactoring 하기 전 public class StringCalcculator { public static int splitAndSum(String text){ int result = 0; if (text == null || text.isEmpty()){ result = 0; }else { String[] values = text.split(\",|:\"); for (String value : values){ result += Integer.parseInt(value); } } return result; } } Refactoring 하고난 후 public class StringCalcculator { public static int add(String text){ if (isBlank(text)){ return 0; } return sum(toInts(split(text))); } } private static boolean(String text){ ... } private static String[] split(String text) { ... } private static int[] toInts(String[] values){ ... } private static int sum(int[] numbers){ ... } 후자가 더 좋을 것이다. 주의할점은 한 번에 모든 원칙을 지키면서 리팩토링하려고 연습하면 안되고 한번에 한가지 명확하고 구체적인 목표를 가지고 연습해야한다. 그리고 연습할 땐 극단적인 방법으로 하는 것도 더 좋다. 예를 들어 한 메소드의 라인 수 제한을 15라인에서 10라인으로 줄여서 하는 것이다. 이렇게 극단적으로 하게 되면 기존과 다른 방법이 나오게 되기 때문에 도움이 된다. 이렇게 할 수 있는 것부터 하고 나면 이제 아까 처음에 말했던 클래스 분리를 연습하면 된다. 클래스 분리 부분은 여기서 더 설명하면 너무 길어지니 생략하겠다. 이것도 코드로 보면 도움이 많이 되니 직접 영상에서 찾아서 보자. OKKYCON 기준 29:21초에 나온다. 4단계 - 장난감 프로젝트 난이도 높이기 앞에서 간단하게 해봤으면 이제 점진적으로 요구사항이 복잡한 프로그램 구현해보자. 앞에서 지켰던 기준을 지키면서 프로그래밍 연습을 해야한다. TDD, 리팩토링 연습하기 좋은 프로그램 요구사항 게임과 같이 요구사항이 명확한 프로그램으로 연습 의존관계(모바일 UI, 웹 UI, 데이터베이스, 외부 API와 같은 의존관계)가 없이 연습 약간은 복잡한 로직이 있는 프로그램 연습하기 좋은 예(단, UI는 콘솔) 로또 사다리 타기 볼링 게임 점수판 체스 게임 지뢰 찾기 게임 5단계 - 의존관계 추가를 통한 난이도 높이기 이제 웹, 모바일 UI, 데이터베이스와 같은 의존관계를 추가. 이때 필요한 역량은 테스트하기 쉬운 코드와 테스트하기 어려운 코드를 보는 눈 테스트하기 어려운 코드를 테스트 하기 쉬운 코드로 설계하는 감(sense)이 필요하다. 이렇게 해서 자신감이 생기면 현장에 적용할 수 있다. 한 단계 더 나아간 연습을 하고 싶다면 컴파일 에러를 최소화하면서 리팩토링하기 ATDD 기반으로 응용 애플리케이션 개발하기 레거시 애플리케이션에 테스트 코드 추가해 리팩토링하기 결국 TDD, 리팩토링 연습을 위해 필요한것은? 조급함 대신 마음의 여유 나만의 장난감 프로젝트 같은 과제를 반복적으로 구현할 수 있는 인내력 정리 결국 간단하게 안된다는 것이다. 의식을 가지고 오랜 시간을 들여 연습을 해야 실력이 느는 것이므로 조급해지면 안 될 것 같다. 나도 예전에 조금 공부해 보고 바로 프로젝트에 적용해 보려고 했었는데 당연히 실패했던 기억이 있다. 천천히 작은 것부터 시작해 실력을 길러야겠다. 이번에도 역시 세미나가 정말 뜻깊었고 의미 있는 시간이었다. 강연을 들을 때마다 강연자분들이 정말 대단하시고 멋져보인다. 나도 얼른 성장해서 지식을 나눌 수 있는 개발자가 될 수 있길.. 앞으로의 계획은 일단 첫 번째 규칙처럼 효과적인 훈련 기법이 수립되어 있는 기술을 연마하기 위해 우선 https://edu.nextstep.camp/c/9WPRB0ys/ 을 따라가며 천천히 연습해 보려고 한다. 앞으로 할게 너무 많아서 두려운 한편, 또 성장할 생각에 뭔가 설렌다. *오타가 있거나 피드백 주실 부분이 있으면 편하게 말씀해 주세요.",
    "tags": "seminar TDD Refactoring etc",
    "url": "/etc/seminar/2022-09-16-tdd-refactoring/"
  },{
    "title": "엔티티 매핑(Entity Mapping)",
    "text": "객체와 테이블 매핑 @Entity 테이블과 매핑할 클래스는 @Entity가 필수로 @Entity가 붙은 클래스는 JPA가 관리한다. 기본 생성자 필수 final 클래스, enum, interface, inner 클래스 사용x 저장할 필드에 final 사용x 속성: name JPA에서 사용할 엔티티 이름을 지정 기본값: 클래스 이름을 그대로 사용(ex: Member) @Entity(name = \"Member\") public class Member { ... } @Table 엔티티와 매핑할 테이블 지정 name: 매핑할 테이블 이름 catalog: 데이터베이스 catalog 매핑 schema: 데이터베이스 schema 매핑 uniqueConstraints(DDL): DDL 생성시에 유니크 제약 조건 생성 @Entity @Table(name = \"MBR) public class Member { ... } 데이터베이스 스키마 자동 생성 필드와 컬럼 매핑을 알아보기전에 데이터베이스 스키마 자동 생성에 대해 알아보자 DDL을 애플리케이션 실행 시점에 자동 생성으로 해줄 수 있는데 이렇게 생성된 DDL은 개발 장비에서만 사용, 생성된 DDL은 운영서버에서는 사용하지 않거나, 적절히 다듬은 후 사용 데이터베이스 스키마 자동 생성 - 속성 hibernate.hbm2ddl.auto 추가 create: 기존테이블 삭제 후 다시 생성 (DROP + CREATE) create-drop: create와 같으나 종료시점에 테이블 DROP update: 변경분만 반영 validate: 엔티티와 테이블이 정상 매핑되었는지만 확인 none: 사용하지 않음 운영 장비에는 절대 create, create-drop, update를 사용하면 안되고 validate 또는 none만 사용 DDL 생성 기능 제약조건 추가: 회원이름은 필수, 10자 초과X @Column(nullable = false, length = 10) DDL 생성 기능은 DDL을 자동 생성할 때만 사용되고 JPA의 실행 로직에는 영향을 주지 않음. @Entity public class Member { @Id private Long id; @Column(unique = true, length = 10) private String name; ... } 필드와 컬럼 매핑 @Column: 컬럼 매핑 @Enumerated: enum 타입 매핑 @Temporal: 날짜 타입 매핑 @Lob: BLOB, CLOB 매핑 @Transient: 특정 필드를 컬럼에 매핑하지 않음 @Entity public class Member { @Id private Long id; @Column(name = \"name\") private String username; private Integer age; @Enumerated(EnumType.STRING) private RoleType roleType; @Temporal(TemporalType.TIMESTAMP) private Date createdDate; @Temporal(TemporalType.TIMESTAMP) private Date lastModifiedDate; @Lob private String description; @Transient private int temp; ... } @Column 속성 name: 필드와 매핑할 테이블의 컬럼 이름, 기본값: 객체의 필드 이름 insertable, updatable: 등록, 변경 가능 여부, 기본값: TRUE nullable(DDL): null값의 허용 여부를 설정. False로 설정하면 DDL 생성시에 not null 제약 조건이 붙음 unique(DDL): @Table의 uniqueConstraints와 같지만 한 컬럼에 간단히 유니크 제약조건을 걸 때 사용 length(DDL): 문자 길이 제약조건으로 String 타입에만 사용, 기본값: 255 columnDefinition(DDL): 데이터베이스 컬럼 정보를 직접 입력 가능 @Enumerated 속성 value: Enumerated 방식으로 2가지 방법이 있음, 기본값: EnumType.ORDINAL EnumType.ORDINAL: enum 순서를 데이터베이스에 저장 EnumType.STRING: enum 이름을 데이터베이에 저장 ORDINAL을 사용하게되면 숫자를 기반으로 하기 때문에 순서가 바뀌게 되면 데이터가 엉킬 수 있으므로 사용X @Temporal 속성 날짜 타입(java.utill, Date, java.util.Calendar)을 매핑할 때 사용할 수 있는데, 최근에는 LocalDate, LocalDateTime이 나와서 생략 가능 value: 3가지 방법이 있음 TemporalType.DATE: 날짜, 데이터베이스 date 타입과 매핑(2022-09-14) TemporalType.TIME: 시간, 데이터베이스 time 타입과 매핑(18:27:34) TemporalType.TIMESTAMP: 날짜와 시간, 데이터베이스 timestamp 타입과 매핑(2022-09-14 18:27:34) @Lob 데이터베이스 BLOB, CLOB 타입과 매핑되고 @Lob 같은 경우 속성이 없다. 매핑하는 필드 타입이 문자면 CLOB이 매핑되고 나머지는 BLOB으로 매핑 @Transient 데이터베이스에 저장되지않으며 주로 메모리상에서만 임시로 어떤 값을 보관하고 싶을 때 사용 기본키 매핑 @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; 직접 할당: @Id만 사용 자동 생성(@GeneratedValue) IDENTITY: 데이터베이스에 위임, MYSQL SEQUENCE: 데이터베이스 시퀀스 오브젝트 사용, ORACLE @SequenceGenerator 필요 TABLE: 키 생성용 테이블 사용, 모든 DB에서 사용 @TableGenerator 필요 AUTO: 방언에 따라 자동 지정, 기본값 IDENTITY 전략 @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; 기본 키 생성을 데이터베이스에 위임하며 주로 MYSQL, PostgreSQL, SQL Server, DB2에서 사용한다. 단점으로 JPA의 버퍼링 기능을 사용못한다는 것인데 JPA는 보통 트랜잭션 커밋 시점에 INSERT SQL을 한번에 실행하는데 IDENTITY 전략은 데이터베이스에 INSERT SQL을 실행 한 이후에 ID값을 알 수 있기 때문에 즉시 INSERT SQL를 실행하게 됨. SEQUENCE 전략 @Entity public class Member { @Id @GeneratedValue(strategy = GenerationType.SEQUENCE) private Long id; ... } 유일한 값을 순서대로 생성하는 특별한 데이터베이스 오브젝트로 오라클, PostgreSQL, DB2, H2에서 사용한다. SEQUENCE 방식은 다음 ID값을 DB에있는 시퀀스에서 가져오기 때문에 INSERT 쿼리를 나중에 보낼 수 있기 때문에 버퍼링이 가능하다. *테이블마다 따로 시퀀스를 관리하고 싶다면 @SequenceGenerator 사용 @Entity @SequenceGenerator( name = \"MEMBER_SEQ_GENERATOR\", sequenceName = \"MEMBER_SEQ\" //매핑할 데이터베이스 시퀀스 이름 initialValue = 1, allocationSize = 1) public class Member { @Id @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = \"MEMBER_SEQ_GENERATOR\") private Long id; ... } @SequenceGenerator 속성 name: 식별자 생성기 이름, 필수 값 sequenceName: 데이터베이스에 등록되어 있는 시퀀스 이름, 기본 값: hibernate_sequence initialValue: DDL 생성 시에만 사용됨, 시퀀스 DDL을 생성할 때 처음 시작하는 수를 지정, 기본값: 1 allocationSize: 시퀀스 한 번 호출에 증가하는 수로 allocationSize를 이용하여 성능 최적화가 가능, 기본값: 50 위에서 다음 ID를 DB에서 가져온다 했는데 매번 한개씩 가져오는 것 보다 allocationSize를 높여 미리 그만큼 사용할 ID를 떙겨와 성능을 높일 수 있다. TABLE 전략 키 생성 전용 테이블을 하나 만들어서 데이터베이스 시퀀스를 흉내내는 전략으로 모든 데이터베이스에 적용 가능하다. 하지만 테이블을 직접 사용하다보니 성능이 조금 떨어질 수 있다. TABLE 전략 같은 경우도 SEQUENCE 전략처럼 ALLOCATION으로 성능 최적화가 가능하다. @Entity @TableGenerator( name = \"MEMBER_SEQ_GENERATOR\", table = \"MY_SEQUENCES\", pkColumnValue = \"MEMBER_SEQ\", allocationSize = 1) public class Member { @Id @GeneratedValue(strategy = GenerationType.TABLE, generator = \"MEMBER_SEQ_GENERATOR\") private Long id; ... } 위처럼 적게되면 데이터베이스에 아래와 같이 생성 되게 됨 create table MY_SEQUENCES ( sequence_name varchar(255) not null, next_val bigint, primary key ( sequence_name ) ) 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA Mapping study jpa",
    "url": "/study/jpa/2022-09-14-Entity-Mapping/"
  },{
    "title": "영속성 관리(Persistence Management)",
    "text": "영속성 컨텍스트(Persistence Context) 엔티티를 영구 저장하는 환경으로 엔티티 매니저를 통해 영속성 컨텍스트에 접근할 수 있다. 엔티티의 생명주기 비영속(new/transient) 영속성 컨텍스트와 전혀 관계가 없는 새로운 상태 //객체를 생성한 상태(비영속) Member member = new Member(); member.setId(\"member1\"); member.setUsername(\"회원1\"); 영속(managed) 영속성 컨텍스트에 관리되는 상태 //객체를 생성한 상태(비영속) Member member = new Member(); member.setId(\"member1\"); member.setUsername(\"회원1\"); EntityManager em = emf.createEntityManager(); em.getTranscation().begin(); //객체를 저장한 상태(영속) -&gt; 아직 DB에 저장된 것은 아님 em.persist(member); 준영속 (detached) 영속성 컨텍스트에 저장되었다가 분리된 상태 //회원 엔티티를 영속성 컨텍스트에서 분리, 준영속 상태 em.detach(member); 삭제(removed) 삭제된 상태 //객체를 삭제한 상태(삭제) e.mremove(member); 영속성 컨텍스트의 이점 1차 캐시(엔티티 조회) em.persist를 이용해 영속화 하게 되면 먼저 1차 캐시에 저장하게 된다. 그리고 조회를 하면 DB를 거치지 않고 캐시에서 가져오게 됨 Member member = new Member(); member.setId(\"member1\"); member.setUsername(\"회원1\"); //1차 캐시에 저장 em.persist(member); //1차 캐시에서 조회 em.find(Member.class, \"member1\"); 아래 같은 경우 member2가 영속성 컨텍스트에 없기 때문에 DB에서 조회하게 된다. Member findMember2 = em.find(Member.class, \"member2\"); 사실상 1차 캐시는 큰 도움이 안될 수 있다. 엔티티매니저 같은 경우 영속성 컨텍스트를 트랜잭션 단위로 만들고 끝날 때(ex)고객의 요청 하나 들어오고 끝날 때) 종료 시키기 때문에 그때 1차 캐시도 날라가게 된다. 즉, 애플리케이션 전체에서 사용하는 캐시가 아니라 그 짧은 트랜잭션 순간에서만 이득 볼 수 있기 때문에 큰 도움이 안될 수 있다. 하지만 비즈니스 로직이 굉장히 복잡한 경우에는 도움이 되긴 한다. 영속 엔티티의 동일성 보장 Member a = em.find(Member.class, \"member1\"); Member b = em.find(Member.class, \"member1\"); System.out.println(a == b);// =&gt; true 반환 트랜잭션을 지원하는 쓰기 지연(엔티티 등록) 쓰기 지연 SQL 저장소를 이용해 쿼리들을 모아놨다가 commit 하기 전에 한번에 보낼 수 있음(버퍼링 기능) EntityManager em = emf.createEntityManager(); EntityTransaction transaction = em.getTransaction(); //엔티티 매니저는 데이터 변경시 트랜잭션을 시작해야 한다. transaction.begin(); // [트랜잭션] 시작 em.persist(memberA); em.persist(memberB); //여기까지 INSERT SQL을 데이터베이스에 보내지 않는다. //커밋하는 순간 데이터베이스에 INSERT SQL을 보낸다. transaction.commit(); // [트랜잭션] 커밋 변경 감지(엔티티 수정) 영속 엔티티 같은 경우 데이터를 수정할 때 업데이트 코드를 따로 작성하지 않아도 된다. 스냅샷을 이용해 엔티티와 비교 후 마지막에 update를 한다. *스냅샷: 1차캐시에 제일 처음에 들어온 엔티티를 따로 저장해 놓은 것 EntityManager em = emf.createEntityManager(); EntityTransaction transaction = em.getTransaction(); transaction.begin(); Member memberA = em.find(Member.class, \"memberA\"); // 영속 엔티티 데이터 수정 memberA.setUsername(\"hi\"); memberA.setAge(10); //em.update(member) &lt;- 이런 업데이트 코드가 없어도 JPA가 자동으로 해줌 transaction.commit(); // [트랜잭션] 커밋 플러시 영속성 컨텍스트의 변경내용을 데이터베이스에 반영하는 것으로 반영하고나서 영속성 컨텍스트를 비우지는 않음. 플러시 발생하는 경우 변경 감지 수정된 엔티티 쓰기 지연 SQL 저장소에 등록 쓰기 지연 SQL 저장소의 쿼리를 데이터베이스에 전송(등록, 수정, 삭제 쿼리) 영속성 컨텍스트를 플러시하는 방법 em.flush(): 직접 쓸일 거의 없지만 커밋하기전 query를 보고 싶거나 테스트하고 싶은 경우 직접 호출 트랜잭션 커밋 - 플러시 자동 호출 JPQL 쿼리 실행 - 플러시 자동 호출 준영속 상태 영속 상태의 엔티티가 영속성 컨텍스트에서 분리(detached)된 상태로 영속성 컨텍스트가 제공하는 기능을 사용하지 못한다. 준영속 상태로 만드는 방법 em.detach(entity): 특정 엔티티만 준영속 상태로 전환 em.clear(): 영속성 컨텍스트를 완전히 초기화 em.close(): 영속성 컨텍스트를 종료 //참고로 em.find으로 조회 했을 떄도 그 객체는 영속성 상태가 된다. Member member = em.find(Member.class, 150L); member.setName(\"AAAAA\"); em.detach(member); tx.commit(); 이 경우 영속성 객체인 member는 준영속 상태가 되어 값을 변경하고 커밋을 하여도 변경감지가 일어나지 않게된다. 준영속 상태로 만드는건 직접 쓸일은 거의 없고 나중에 웹 애플리케이션을 개발할 때 복잡한 상황이 오면 준영속 상태를 처리할 때(변경 감지 기능 사용, 병합(merge)사용)가 있다. 후에 기회가 있으면 자세하게 설명하겠다. 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA study jpa",
    "url": "/study/jpa/2022-09-11-Persistence-Management/"
  },{
    "title": "왜 JPA를 사용할까?",
    "text": "왜 JPA를 사용하게 됐을까? 객체를 영구 보관하는 다양한 저장소에는 RDB, NoSQL, File, OODBn 등이 있지만 현실적인 대안은 관계형 데이터베이스(RDB)이다. 즉, SQL을 이용해서 객체를 RDB에 저장할 수 밖에 없다. 관계형 DB를 쓰는 상황에서는 SQL에 의존적인 개발을 피하기 어렵다. 하지만 SQL 중심적인 개발에는 여러 문제가 있다. 무한 반복, 지루한 코드 CRUD (INSERT, SELECT, UPDATE, DELETE)을 테이블마다 생성해야 함. 객체 지향과 관계형 데이터베이스 간의 패러다임 불일치 객체 지향 객체 지향 프로그래밍은 추상화, 캡슐화, 정보은닉, 상속, 다형성 등 시스템의 복잡성을 제어할 수 있는 다양한 장치들을 제공 추상화, 캡슐화, 정보은닉, 상속, 다형성 등을 잘 사용하자 관계형 데이터베이스 데이터를 잘 정규화해서 보관하자 패러다임이 불일치하는 두 가지를 매핑하기 떄문에 문제가 생김 결국 RDB가 인식할 수 있는 것은 SQL뿐이기 때문 객체를 SQL로 짜야된다. 개발자 == SQL매퍼 라고 할만큼 SQL 작업을 너무 많이 하고 있다. 객체와 관계형 데이터베이스의 차이 상속 연관관계 데이터 타입 데이터 식별 방법 모델링 과정에서의 문제 객체를 테이블에 맞추어 모델링 객체 그래프 탐색 객체는 자유롭게 객체 그래프(연관 관계가 있는 객체 사이)를 탐색할 수 있어야 된다. 그러나 처음 실행하는 SQL에 따라 탐색 범위가 결정되기 때문에 탐색할 수 없음. ⇒ 객체를 자바 컬렉션에 저장 하듯이 DB에 저장할 수 는 없을까? JPA 도입 JPA(Java Persistent API) 자바 진영의 ORM 기술표준으로 인터페이스의 모음이다. 즉, 실제로 구현된것은 아니고 구현한 클래스와 매핑을 해준다. JPA를 구현한 ORM 프레임워크로는 Hibernate, EclipseLink, DataNucleus가 있다. ORM(Object-relational mapping) Object-realational mapping(객체 관계 매핑) 객체는 객체대로 설계 관계형 데이터베이스는 관계형 데이터베이스대로 설계 ORM 프레임워크가 중간에서 매핑 대중적인 언어에는 대부분 ORM 기술이 존재 그래서 JPA를 도입해서 얻는 장점? SQL 중심적인 개발에서 객체 중심으로 개발 생산성 기존: 테이블마다 CRUD 생성 유지보수 기존: 원래 필드 변경시 모든 SQL 수정 JPA: 필드만 추가, SQL은 JPA가 처리 패러다임의 불일치 해결 (상속, 연관관계, 객체 그래프 탐색) 성능 JPA의 성능 최적화 기능 중간 계층이 있는 경우 다음과 같은 방법으로 성능을 개선할 수 있는 방법이 있다. 버퍼링(모아놨다가 한번에 보내는) 기능 캐싱(미리 데이터 저장) 기능 JPA도 JDBC API와 DB 사이에 존재하기 때문에 위의 두 기능이 존재한다. 1차 캐시와 동일성 보장 - 캐싱 기능 트랜 잭션을 지원하는 쓰기 지연 - 버퍼링 기능 지연 로딩 객체가 실제 사용될 때 로딩하는 기능 여기까지 JPA를 왜 사용하게 되었는지 알아보았고 다음부터는 JPA에 대해 자세히 알아보자. 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA study jpa",
    "url": "/study/jpa/2022-09-09-JPA/"
  },{
    "title": "경쟁 상태(Race Condition)",
    "text": "경쟁 상태(Race Condition)이란? 공유 자원에 대해 여러 쓰레드가 프로세스가 동시에 접근할 때, 데이터의 불일치를 일으킬 수 있다. 예를들어 원래 User1와 User2의 목적은 계좌에 있던 30만원을 각각 5만원, 4만원을 넣어 39만원을 만들려는 목적인데 동시에 접근해 사용하게 되면 아래처럼 불일치가 일어나는 걸 볼 수 있다. Race Condition이 발생하는 경우 발생하는 경우를 보기전에 먼저 유저모드와 커널모드에 대해 알아보자 유저모드: 사용자가 접근할 수 있는 영역을 제한적으로 두고, 프로그램의 자원에 함부로 접근하지 못하는 모드. 코드를 작성하고, 프로세스를 실행하는 행동을 할 수 있다. 커널모드: 모든 자원(CPU, 메모리 등) 접근, 명령 가능한 모드 프로세스간에는 주소 공간이 독립적이지만 커널에 있는 자원은 여러 프로세스가 공유할 수 있기 때문에 주로 유저모드보다 커널모드일 때 Race Condition이 발생한다. 1. 커널 작업을 수행하는 중에 인터럽트 발생 커널 모드에서 데이터를 불러 작업을 수행하다 인터럽트가 발생하면 같은 데이터를 조작하게 된다. 그래서 커널모드에서 작업을 수행할 떄는 인터럽트가 발생되지 않도록 설정하면 된다. 2. 프로세스가 시스템 콜(System Call)을 하여 커널모드로 진입하여 작업을 수행하던 중 문맥 교환(Context Switch)이 발생할 때 프로세스 A가 커널모드에서 데이터를 작업(count값 읽고 증가)하는 도중 문맥교환이 일어나면 같은 프로세스 B에서 같은 데이터를 조작하게 된다. 이때, 프로세스2의 작업은 반영되지 않게 된다. 이 경우는 프로세스가 커널모드에서 작업하는 경우 CPU 제어권을 뺏지 않도록 하면 된다. 3. 멀티 프로세서에서 공유 메모리 내의 커널 데이터에 접근할 때 멀티 프로세스 환경에서 2개의 CPU가 동시에 커널 내부의 공유 데이터에 접근하는 경우도 문제가 생길 수 있는데 커널 내부에 있는 공유 데이터에 접근할 때마다 lock/unlock을 해서 해결할 수 있다. Thread-safe 멀티 스레드 환경에서 여러 스레드가 동시에 하나의 객체 및 변수(공유자원)에 접근할 때, 의도한 대로 동작하는 것을 의미한다. Thread-safe하기 위해서는 공유 자원에 접근하는 임계 영역(Critical Section)을 동기화 기법으로 잘 제어해줘야 된다. 동기화 기법에는 Mutex나 Semaphore 등이 있는데 자세하게 알고싶으면 이 글을 참고하면 되겠다. 참고: https://github.com/gyoogle/tech-interview-for-developer https://hibee.tistory.com/297 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "os Race-Condition etc operating system",
    "url": "/etc/operating%20system/2022-09-08-Race-Condition/"
  },{
    "title": "JWT를 선택한 이유(+Redis)",
    "text": "HTTP의 Stateless한 특징 때문에 인증을 관리하기 위한 방안(Session, JWT)이 필요하다. Session과 JWT를 간단히 알아보고 프로젝트에 왜 JWT를 적용하게 되었는지 알아보자. Session Session 같은 경우 토큰방식보다 보안에 강하다는 장점이 있지만 단점으로 서버의 확장성이 떨어지고 서버의 자원(세션을 저장할 공간)이 많이 필요하다. 그리고 멀티 디바이스 환경에서 처리에 신경써줘야 할 부분이 생긴다. JWT(Json Web Token) JWT는 인증에 필요한 정보들을 암호화 시킨 토큰을 뜻하며 Header, Payload, Signature 3부분으로 이루어지게 된다. Json 형태인 각 부분은 Base64Url로 인코딩되어 표현된다. 각 부분을 이어 주기 위해 .구분자를 사용하여 구분한다. Base64Url은 암호화된 문자열이 아니고 같은 문자열에 대해 항상 같은 문자열을 반환 Header(헤더) 헤더는 알고리즘 방식인 alg와 토큰의 타입인 typ으로 구성된다. { \"alg\": \"HS256\", \"typ\": \"JWT } Payload(페이로드) 서버에서 보낼 데이터(sub, name, iat) Signature(서명) 서명은 Base64방식으로 인코딩한 Header(Encoded Header), Payload(Encoded Payload)를 SECRET KEY를 이용해 Header에서 정의한 알고리즘으로 해싱 하고, 이 값을 다시 Base64Url로 인코딩하여 생성 장점 세션/쿠키는 별도의 저장소 관리가 필요하지만 JWT는 발급한 후 검증만 하면 되기 때문에 따로 저장소가 필요하지 않다. 이는 Stateless한 서버를 만드는 입장에서 큰 강점인데 서버를 확장하거나 유지, 보수하는데 유리하다. 단점 이미 발급된 JWT에 대해 돌이킬 수 없다. 세션/쿠키 같은 경우 만일 쿠키가 악의적으로 사용되면 해당 세션을 지워도 된다. 하지만 JWT는 한번 발급되면 유효기간이 만료될 때 까지 계속 사용가능 하기 때문에 위험하다. 또한 세션/쿠키 방식에 비해 JWT의 길이는 길기 때문에 인증이 필요한 요청이 많을 수록 서버의 자원낭비가 발생할 수 있다. 우리는 JWT를 왜 선택하게 되었을까 우리는 Session 방식대신 JWT를 선택하게 되었다. REST API의 Stateless한 특성에 JWT가 좀 더 부합하는 것 같았고 웹뿐 아니라 모바일도 생각하고 있었기 때문에 처리를 더 간소화하고 앞으로의 확장성을 생각해 선택하게 되었다. 보안같은 경우 RefreshToken을 추가하여 추가적으로 보완하기로 생각했다. Refresh Token: Access Token을 재발급해주는 용도의 토큰. Access Token으로만 진행할 경우의 문제점은 유효기간을 길게할 경우는 탈취당하면 그 긴 시간동안 정보 탈취, 유효기간을 짧게하면 사용자가 로그인을 계속해야되는 번거로움이 있다. 그래서 나온게 Refresh Token으로 Access Token의 경우 유효기간을 짧게 잡고 Refresh Token을 길게 잡아서 Access Token의 기간이 다 되어도 Refresh Token으로 사용자는 로그인 없이 다시 Access Token 재발급 가능 RefreshToken에 대한 고찰 JWT의 구성방식이나 저장장소에 대해 정말 많이 고민하고 찾아본거 같은데 정말 다양했던 것 같고 그에 따른 의견도 정말 다 달랐다. 어느 것이 제일 뛰어나다고 할 수 없었고 다들 trade off가 있었던 것 같다. 우리 같은 경우 RefreshToken은 쿠키에 저장하는걸 선택했고 HTTPOnly와 Secure 옵션을 사용해 CSRF 공격에 대비하고 추가적으로 리프레쉬 토큰값은 노출시키고 싶지 않았기 때문에 실제 리프레쉬 토큰값에 해당하는 id(random uuid)값을 쿠키에 저장하게 하였다. +Redis 서버에서는 RefreshToken을 Redis라는 In Memory에 저장하게 되었다. 결국 API를 사용하는데 Access Token이 가장 중요하기 때문에 이 Access Token을 재발급 해주는 토큰(RefreshToken)을 읽고 쓰는 데이터베이스가 조금 더 빠르면 좋을 것 같아 생각하게 되었고, Refresh Token 같은 경우 일정 시간 뒤에 삭제를 해야 하는데 일정 시간 뒤 자동으로 삭제되는 TTL(TimeToLive) 기능이 지원되어 잘 맞을 것 같았다. 휘발성이라는 단점은 Refresh Token 같은 경우 휘발되어도 큰 문제가 되지 않는 정보라 상관없을 것 같다. 참고: https://jwt.io/ *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JWT SESSION Redis project",
    "url": "/project/2022-09-07-Jwt/"
  },{
    "title": "서블릿(Servlet)",
    "text": "옛날에 처음 해당 내용을 공부했을 때는 내용 자체가 어렵고 상세한 이해를 하지 않은 상태에서도 개발하는데 큰 문제가 없어서 신경을 못 썻던 기억이 있다. 하지만 프로젝트를 진행할 때 Controller나 interceptor 같은 곳에서 자주 HttpServletRequest나 HttpServletReponse를 사용하게 되었는데 해당 기술에 대해 매우 궁금해졌고 다시 공부하고 싶어졌다. 어떤 기술인지 한번 알아보자 서블릿(Servlet)이란? 서블릿(Servlet)이란 동적인 웹페이지를 생성하기 위한 자바 기반의 웹 프로그래밍 기술이다. 만약 클라이언트쪽에서 회원 저장이라는 요청을 했을 때 웹 애플리케이션 서버(WAS)를 직접 구현하게 되면 이 아래의 과정(연결, 파싱, 확인, 실행, 생성 등)을 개발자가 직접 다 구현해야되는데 반복적이고 비효율적인 작업이다. 하지만 서블릿을 지원하는 WAS를 사용하게 되면 우리는 간단하게 핵심 비즈니스 로직만 작성하면 된다. 아래처럼 특정 URL(/hello)이 호출되면 서블릿 코드가 실행되는데 HttpServletRequest를 이용하여 요청 정보를 편리하게 사용하거나 HttpServletResponse를 이용하여 응답 정보를 편리하게 제공할 수 있다. 이처럼 서블릿을 사용하면 개발자는 HTTP 스펙을 매우 편리하게 사용가능하다. 서블릿 HTTP 요청, 응답 흐름 클라이언트에서 HTTP 요청 시 WAS는 Request, Response 객체를 새로 만들어서 서블릿 객체를 호출한다. Request 객체에서 HTTP 요청 정보를 편리하게 꺼내서 사용 Response 객체에 HTTP 응답 정보를 편리하게 입력 WAS는 Reponse 객체에 담겨있는 내용으로 HTTP 응답 정보를 생성 서블릿 컨테이너(Servlet Container)란? 서블릿 컨테이너란 서블릿을 담고 관리해주는 컨테이너로 톰캣처럼 서블릿을 지원하는 WAS를 서블릿 컨테이너라고 한다. 서블릿 컨테이너의 역활을 보자면 웹 서버와의 통신 지원 서블릿 컨테이너는 서블릿 객체를 생성, 초기화, 호출, 종료하는 생명주기 관리 서블릿 객체는 싱글톤으로 관리 동시 요청을 위한 멀티 쓰레드 처리 지원 동시 요청 - 멀티 쓰레드 추가적으로 동시 요청 - 멀티 쓰레드에 대해 알아보자 쓰레드(Thread) 쓰레드는 애플리케이션 코드를 하나하나 순차적으로 실행한다. 자바 메인 메서드를 처음 실행하면 main이라는 이름의 쓰레드가 실행되는데 쓰레드가 없다면 자바 애플리케이션 실행이 불가능하다. 쓰레드는 한번에 하나의 코드 라인만 수행하는데 동시 처리가 필요하면 쓰레드를 추가로 생성해야 한다. 단일 요청 - 쓰레드 하나 사용 하나의 쓰레드가 클라이언트 쪽에서 요청이 들어오면 아래처럼 쓰레드가 할당이 되어 servlet을 호출하고 응답한다. 하지만 만약에 다중으로 요청이 들어오게 된다면 하나의 요청이 끝날때 까지 다른 요청은 기다리게 된다. 요청 마다 쓰레드 생성 클라이언트쪽에서 요청이 올 때마다 쓰레드를 생성하는 방법으로 앞의 요청이 지연이 되어도 다른 요청에 지장이 없다. 장점 동시 요청 처리 가능 하나의 쓰레드가 지연 되어도, 나머지 쓰레드 정상 동작 단점 쓰레드 생성 비용은 매우 비싸고 응답 속도가 늦어짐 컨텍스트 스위칭 비용이 발생 쓰레드 생성에 제한이 없다. -&gt; 고객 요청이 너무 많아지면 서버가 죽어버림 쓰레드 풀 요청 마다 쓰레드 생성의 단점을 보완한 방식으로 필요한 쓰레드를 쓰레드 풀에 미리 생성하여 보관한다. 쓰레드가 필요하면, 이미 생성되어 있는 쓰레드를 쓰레드 풀에서 꺼내서 사용하고 사용을 종료하면 쓰레드 풀에 반납한다. 만약 모두 사용중이여서 쓰레드 풀에 쓰레드가 없으면 기다리는 요청은 거절하거나 특정 숫자만큼 대기시킬 수 있다. 쓰레드가 미리 생성되어 있으므로, 쓰레드를 생성하고 종료하는 비용을 절약할 수 있고, 응답 속도도 빠르다. 그리고 생성 가능한 쓰레드의 최대치가 있으므로 너무 많은 요청이 들어와도 기존 요청은 안전하게 처리 가능하다. 참고: https://www.inflearn.com/course/%EC%8A%A4%ED%94%84%EB%A7%81-mvc-1 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring Servlet Servlet-Container study spring",
    "url": "/study/spring/2022-09-03-Servlet/"
  },{
    "title": "스프링(Spring) vs 스프링 부트(Spring Boot)",
    "text": "스프링 부트를 사용하고 있어도 스프링과 스프링부트가 뭐가 다른지 어떤 차이점이 있는지 헷갈릴 수 있다. 스프링과 스프링 부트에 대해 알아보자 스프링(Spring) 스프링은 개발자가 개발에만 집중할 수 있도록 설계된 프레임워크로 다양한 방법으로 편리성을 제공한다. DI(Dependency Injection), IoC(Inversion of Control) 스프링 프레임워크의 가장 핵심적 기술로 DI와 IOC가 있는데 이 둘을 적절히 사용함으로 써, 우리는 느슨하게 결합된 애플리케이션을 개발할 수 있게 된다. 느슨하게 결합되면 코드의 재사용성이 증가하고 단위테스트가 용이해진다. 간단하게 예를 들면, 아래처럼 의존성 주입(DI)이 없는 경우 example을 리턴하기 위해 ExampleService에 의존적이게 된다. 즉, 강하게 결합되는 것을 의미하고 만약에 단위테스트에서 ExampleService에 대한 mock을 생성해야 된다면 어려울 것이다. @Controller public class ExampleController{ private ExampleService service = new ExampleService(); @RequestMapping(\"/example) public String example() { return service.returnExample(); } } 하지만 의존성 주입(DI)을 이용하면 느슨하게 결합할 수 있다. 우리는 @Component와 @Autowired란 애노테이션을 이용하여 간단하게 외부에서 주입시켜줄 수 있다. 이렇게 되면 단위테스트를 할 때 그냥 ExampleService의 mock을 주입하기만 하면 된다. @Component public class ExampleService { ... } @Controller public class ExampleController{ @Autowired private ExampleService service; @RequestMapping(\"/example) public String example() { return service.returnExample(); } } IoC(Inversion of Control): 기존 프로그램은 클라이언트 구현 객체가 스스로 필요한 서버 구현 객체를 생성하고, 연결하고, 실행. 즉, 구현 객체가 프로그램의 제어 흐름을 제어했다. 하지만 이렇게 프로그램의 제어 흐름을 직접 제어하는 것이 아니라 외부에서 관리하는 것을 제어의 역전(IoC)라고 한다. 자세한건 이 게시글 참고 중복코드 제거 스프링 프레임워크는 의존성 주입뿐 아니라 다음과 같은 스프링 모듈들을 이용해서 의존성 주입의 핵심을 구성 Spring JDBC Spring MVC Spring AOP 예를들어 우리가 스프링 프레임워크를 사용하여 DB를 연결할 때는 몇줄 안치고 간단하게 연결했던 기억이 있을거다. 하지만 스프링을 사용하지 않고 순수 JDBC를 연결하려면 매번 길고 중복된 코드를 쳐야되는데 이 불편한 작업을 스프링이 대신해준다고 볼 수 있다. 다른 프레임워크와의 통합 Hibernate, JUnit, Mocktio 같은 프레임워크와 통합이 간단해서 이를 통해 개발하는 프로그램 품질이 향상된다. 그래서 스프링은 알겠는데 스프링 부트는 뭐야? 스프링 부트(Spring Boot) 위에서 처럼 스프링 프레임워크는 다양한 기능이 있는만큼 환경설정 또한 복잡하다. 이를 또 자동화해주기 위해 도와주는 프레임워크가 스프링 부트이다 쉬운 의존성 관리 테스트가 필요할 때 스프링 같은경우 Spring Test, JUnit, Hamcrest 및 Mockito 등 필요한 모든 라이브러리를 종속성으로 추가해야되지만 SpingBoot에선 이러한 라이브러리를 자동으로 포함하기 위해 spring-boot-start-test만 추가하면 된다. 그리고 버전 관리도 자동으로 해준다. 내장 서버 스프링 부트 프레임워크는 내장 WAS(Web Application Server)를 가지고 있기 때문에 따로 WAS를 설치하지 않아도 실행이 가능하고 jar파일로 간편하게 배포같은 것도 가능하다. 편리한 Configuration 설정 및 AutoConfiguration JSP 웹 애플리케이션을 생성하는데 스프링 같은 경우 아래 코드들을 정의 해야 한다. public class MyWebAppInitializer implements WebApplicationInitializer { @Override public void onStartup(ServletContext container) { AnnotationConfigWebApplicationContext context = new AnnotationConfigWebApplicationContext(); context.setConfigLocation(\"com.baeldung\"); container.addListener(new ContextLoaderListener(context)); ServletRegistration.Dynamic dispatcher = container .addServlet(\"dispatcher\", new DispatcherServlet(context)); dispatcher.setLoadOnStartup(1); dispatcher.addMapping(\"/\"); } } @EnableWebMvc @Configuration public class ClientWebConfig implements WebMvcConfigurer { @Bean public ViewResolver viewResolver() { InternalResourceViewResolver bean = new InternalResourceViewResolver(); bean.setViewClass(JstlView.class); bean.setPrefix(\"/WEB-INF/view/\"); bean.setSuffix(\".jsp\"); return bean; } } 하지만 Spring Boot 같은 경우 web starter를 추가하면 다음과 같이 두줄만 추가하면 된다. spring.mvc.view.prefix=/WEB-INF/jsp/ spring.mvc.view.suffix=.jsp 그리고 위의 모든 Spring 구성은 Boot web starter를 포함함으로 써 AutoConfiguration에 의해 자동으로 포함된다. 이것이 의미하는 것은 Spring Boot는 애플리케이션에 존재하는 종속성, 속성, 빈을 살펴보고 이를 기반으로 구성을 활성화한다고 볼 수 있다. 참고 https://www.baeldung.com/spring-vs-spring-boot https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&amp;blogId=sthwin&amp;logNo=221271008423&amp;parentCategoryNo=&amp;categoryNo=50&amp;viewDate=&amp;isShowPopularPosts=true&amp;from=search *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring Spring-Boot study spring",
    "url": "/study/spring/2022-08-30-Spring-vs-Boot/"
  },{
    "title": "인터럽트(Interrupt)",
    "text": "CPU가 입력을 받아들이는 방법 폴링 방식(polling) 사용자가 명령어를 사용하여 입력 핀의 값을 계속 읽어서 변화를 알아내는 방식으로 하드웨어의 지원 없이도 언제든지 가능하다. 인터럽트 방식(interrupt) MCU 자체가 하드웨어적으로 그 변화를 체크하여 변화 시에만 일정한 동작을 하는 방식으로 하드웨어적으로 지원되는 몇개의 입력 또는 값의 변화에만 대응처리가 가능하다는 제약이 있으나 폴링에 비해 빠르고 특히 실시간 대응에는 필수적인 기능이다. MCU(Micro Controller Unit): 마이크로프로세서(CPU)와 입출력 모듈을 하나의 칩으로 만들어 정해진 기능을 수행하는 컴퓨터 만약 인터럽트 기능이 없었다면 마이크로컨트롤러는 특정한 일을 할 시기를 알기 위해 계속 체크(pollling)해야 된다. 근데 폴링을 하는 시간에는 마이크로컨트롤러는 자기 원래 할 일에 집중할 수 없게 돼서 비효율적 인터럽트(Interrupt)란? 인터럽트는 프로그램을 실행하는 도중에 예기치 않은 상황이 발생할경우 현재 실행하는 프로그램을 잠깐 중단하고 먼저 필요한 상황을 우선 처리한 후 실행하던 프로그램으로 다시 복귀하여 작업을 처리하는 것이다. 인터럽트 종류 외부 인터럽트: 입출력장치, 타이밍 장치, 전원 장치 등 외적이 요인으로 발생하는 인터럽트 ex) 전원 이상, 기계 착오, 외부 신호, 입출력 주로 하드웨어에서 발생해서 하드웨어 인터럽트라고도 함 내부 인터럽트: 잘못된 명령이나 데이터를 사용할 때 발생 ex) 0으로 나누기, OverFlow, UnderFlow, 명령어 잘못 사용한 경우 주로 프로그램 내부에서 발생해서 소프트웨어 인터럽트라고도 함 인터럽트 처리과정 현재 주 프로그램을 실행중 (①) 인터럽트가 발생 (②) 현재 진행중인 프로그램을 중단하고 현재 프로그램 상태를 저장한다. (③) 인터럽트 서비스 루틴으로 점프 후 처리(④, ⑤, ⑥) 인터럽트 작업이 끝나면 원래 하던 프로그램 상태 로드 (⑦, ⑧) 다시 주 프로그램 실행(원래 하던 곳 이어서 진행) (⑨) 4번 과정을 추가적으로 설명하자면 인터럽트 벡터는 인터럽트가 발생했을 때, 그 인터럽트를 처리할 수 있는 서비스 루틴(ISR)들의 주소를 가지고 있는 공간이다. 운영체제는 각종 인터럽트별로 인터럽트 발생 시 처리해야할 내용이 이미 프로그램되어 있으며 이를 인터럽트 서비스 루틴 또는 인터럽트 핸들러이다. 참고: https://m.blog.naver.com/PostView.nhn?blogId=scw0531&amp;logNo=220650635893&amp;proxyReferer=https:%2F%2Fwww.google.com%2F https://www.theengineeringprojects.com/2021/12/esp32-interrupts.html *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "os polling interrupt etc operating system",
    "url": "/etc/operating%20system/2022-08-28-Interrupt/"
  },{
    "title": "왜 도커(docker)를 사용할까?",
    "text": "요즘 프로젝트들을 보면 그냥 배포하는 것이 아닌 아래처럼 도커를 통해 배포하는 것을 많이 볼 수 있다. 우리 프로젝트도 도커를 이용해 배포를 하려고 하는데 도커가 뭔지, 왜 사용하는지 알아보자 도커(Docker)란? 컨테이너를 사용하여 응용프로그램을 더 쉽게 만들고 배포하고 실행할 수 있도록 설계된 도구이다. 아래 그림처럼 다양한 프로그램, 실행환경을 컨테이너로 추상하고 인터페이스를 제공하여 프로그램의 배포 및 관리를 단순하게 해준다. 도커를 왜 사용할까? 어떠한 프로그램을 설치하는 과정에는 갖고 있는 서버, 패키지 버전, 운영체제에 따라 많은 에러들이 발생할 뿐 아니라 설치 과정 또한 복잡하다. 도커는 어떠한 프로그램을 다운 받는 과정을 굉장히 간단하게 만들어준다. 도커없이 일반적으로 Redis를 설치하는 경우 Redis 홈페이지에 접속한 후 설치방법에 따라 명령어를 입력하게 되는데 정상적으로 한 번에 되면 정말 다행이지만 항상 어느 곳에서 나 잘될 수 가 없다. 그 예로, 위에선 wget이 없다고 에러가 발생한 모습이다. 그래서 wget을 받은 후 다시 Redis를 받아야되는데 이런식으로 어떠한 특정 프로그램을 받을 때 부수적인 것(wget 같은)들도 받으면서 과정이 복잡해지고 에러도 많이 생기게 된다. 도커로 Redis를 받는 설치하는 경우 그냥 docker run -it redis라는 명령어 한 줄이면 끝난다. 이처럼 도커를 이용하여 프로그램을 설치하면 예상치 못한 에러도 덜 발생할 뿐 아니라 설치하는 과정도 훨씬 간단해진다. 도커 이미지 그래서 도커 컨테이너는 어떻게 생성 될까? 도커 이미지라는 것이 필요한데 도커 이미지는 프로그램을 실행하는데 필요한 설정이나 종속성을 갖고 있으며 도커 이미지를 통해 컨테이너를 생성하며 도커 컨테이너를 이용하여 프로그램을 실행한다. 즉, 이미지는 응용 프로그램을 실행하는데 필요한 모든 것을 포함하고 있다는 것인데 그 필요한 것이 뭘까? 컨테이너가 시작 될 때 필요한 명령어 ex) run kakaotalk 파일 스냅샷 ex)컨테이너에서 카카오톡을 실행하고 싶다면 카카오톡 파일(카카오톡 실행하는데 필요한 파일) 스냅샷 이미지가 컨테이너를 만드는 순서 Docker run 도커 이미지에 있는 파일 스냅샷을 컨테이너 하드 디스크에 옮김 이미지에서 가지고 있는 명령어를 이용해서 카카오톡을 실행 그래서 우리 프로젝트에는 왜? 이렇게 도커가 뭔지, 왜 쓰는지에 대해 간단히 알아 봤는데 그래서 우리 프로젝트에서는 왜 쓰고 쓴다면 어떤게 좋을지 생각해봤다. 현재 우리 프로젝트는 소규모의 팀으로 당장 큰 여유가 없을 뿐더러 미리 미래를 위해 대규모의 환경을 구성할 필요도 없다. 그렇기 때문에 현재 배포는 최대한 간단한 구성을 할 필요가 있다고 판단했다. 하지만 나중에는 우리가 더 성장해 규모가 커져 구성을 어떻게 변경할지 알 수가 없기 때문에 나중에 확장성을 위해 도커를 도입하기로 결정했다. 참고: https://www.inflearn.com/course/%EB%94%B0%EB%9D%BC%ED%95%98%EB%A9%B0-%EB%B0%B0%EC%9A%B0%EB%8A%94-%EB%8F%84%EC%BB%A4-ci *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "docker container image project",
    "url": "/project/2022-08-23-docker/"
  },{
    "title": "데이터베이스 - 정규화(Normalization)",
    "text": "정규화(Normalization)란? 데이터베이스 정규화란 관계형 데이터베이스의 설계에서 중복을 최소화하게 데이터를 구성하는 프로세스이다. 데이터베이스 정규화의 목표는 이상이 있는 관계를 재구성하여 작고 잘 조직된 관계를 생성하는 것이다. 정규화를 통해 불필요한 데이터를 없앨 수 있고, 삽입/갱신/삭제 시 발생할 수 있는 각종 이상현상을 방지할 수 있다. 예를들어 정규화를 하지 않았을 때 문제점을 보면 위의 그림과 같이 정규화가 되지 않은 테이블의 경우 중복(Adam이라는 학생이 두번)뿐 아니라, 데이터를 처리할 때 다양한 이상현상이 발생 삽입: 어떤 학생이 아무 과목도 수강하지 않으면, Subject_opted 에는 Null 갱신: 위의 테이블에서 Adam의 Address가 변경되면 여러줄 데이터를 갱신해야함 -&gt; 데이터 불일치 발생 가능 삭제: Alex학생이 과목 수강을 취소한다고 삭제해버리면 Alex라는 레코드가 아예 삭제됩니다. 이렇게 다양한 문제점이 발생할 수 있기 때문에 정규화가 필요하다. 정규화에는 1, 2, 3, BCNF, 4, 5차 정규화까지 있는데 보통 3차까지 해도 충분하다고 하며 4, 5차 까진 비용의 문제로 잘 하지 않는다고 한다. 1차 정규화 제1 정규화란 테이블의 컬럼이 원자값(Atomic Value, 하나의 값)을 갖도록 테이블을 나누는 것이다. 예를 들어, 아래와 같은 경우 Adam이 Subject를 두 개 가지고 있기 때문에 1차 정규형을 만족하지 못한다. 위의 테이블을 1정규화를 하게 되면 아래처럼 한 개의 행을 더 만들게 된다. 만약 하나의 열에 여러 개의 데이터가 들어가 있으면 어떤 학생이 같은 Subject를 듣는가? 같은 쿼리를 검색하기 힘들다. 물론 데이터를 추출해서 검색할 수 있겠지만 매우 불편할 것이다. 그리고 수정한다고 해도 전체 내용을 수정해 줘야 되고 해주는 과정에서도 혹시 빠트리기도 하면 큰일이 날 수 있다. 2차 정규화 제2 정규화란 제1 정규화를 진행한 테이블에 대해 완전 함수 종속을 만족하도록 테이블을 나누는 것을 의미한다. 즉, 기본키중에 특정 컬럼에만 종속된 컬럼(부분적 종속)이 없어야 한다. 아래와 같은 테이블이 좋지 않은 이유는 위에서도 말했지만 갱신이상이 발생한다. 예를들어 자료구조에 따라 강의실이 결정되는데 502번의 자료구조 강의실만 공학관113으로 변경된 경우 자료구조의 강의실은 공학관 113, 공학관 111로 데이터가 불일치하게 됩니다. 위 테이블에서는 기본키가 (학생번호, 강좌이름)으로 복합키인데 (학생번호, 강좌이름)인 기본키가 성적을 결정하고 있다. ((학생번호, 강좌이름) -&gt; (성적)) 그런데 강의실이라는 컬럼은 기본키 부분집합중 강좌이름에 의해 결정될 수 있다. ((강좌이름) -&gt; (강의실)) 즉, 기본키(학생번호, 강좌이름)의 부분키인 강좌이름에 종속되는 컬럼이 있기 때문에 제2 정규형을 만족하지 못하고 아래처럼 나눠 제2 정규형을 만족시킬 수 있다. 3차 정규화 제3 정규화란 제2 정규화를 진행한 테이블에 대해 이행적 종속을 없애도록 테이블을 분해하는 것이다. 여기서 이행적 종속이란 A -&gt; B, B -&gt; C 일 때, A -&gt; C가 성립되는 것을 의미한다. 위 테이블을 보면 학생 번호는 강좌 이름을 결정하고 있고 강좌 이름은 수강료를 결정하고 있다. (학생번호 -&gt; 강좌이름, 강좌이름 -&gt; 수강료) 그렇기 떄문에 (학생 번호, 강좌 이름) 테이블과 (강좌 이름, 수강료) 테이블로 분해할 필요가 있다. 이런 이행적 종속을 제거하는 이유는 예를 들어 501번 학생이 수강하는 강좌가 스포츠경영학으로 변경된 경우 501번 학생은 스포츠경영학이라는 수업을 20000원으로 듣게 된다. 수강료를 15000원으로 맞게 고칠 수 있지만 변경을 한번 더 해야된다는 번거로움이 있는데 이걸 해결하기 위해 제3 정규화를 한다. BCNF 정규화 BCNF 정규화란 제3 정규화를 진행한 테이블에 대해 모든 결정자가 후보키가 되도록 테이블을 나누는 것이다. 후보키: 유일성과 최소성을 만족하는 속성 또는 속성들의 집합 위 테이블에서 기본키는 (학생번호, 특강이름)이다. 그리고 기본키 (학생번호, 특강이름)는 교수를 결정하고 있는데 교수 또한 특강이름을 결정하고 있다. 그런데 교수는 특강이름을 결정하는 결정자이지만 후보키가 아니기 때문에 BCNF를 만족하지 못하며 만족시키기 위해선 아래처럼 테이블을 분해되야 된다. 참고 https://mangkyu.tistory.com/110 https://3months.tistory.com/193 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "database normalizaiton 1NF 2NF 3NF BCNF etc",
    "url": "/etc/database/2022-08-22-Normalization/"
  },{
    "title": "네트워크 - OSI 7계층(OSI 7 Layer)",
    "text": "OSI 7계층이란? OSI 7 계층은 네트워크에서 통신이 일어나는 과정을 7단계로 나눈 것인데 왜 이렇게 나누게 되었을까? 우선 흐름을 한 눈에 알아보기 쉽고 각 계층은 독립적으로 존재함으로 써 문제 발생 시 어느 계층에 문제가 생겼는지 쉽게 파악 가능하다. 1계층 - 물리 계층(Physical Layer) 1계층에서는 비트 0, 1의 통신 단위로 기계적, 전기적 신호를 주고받는 역활을 하게 되는데 이 계층에서는 단지 데이터를 전송할 뿐 전송하려는 데이터가 무엇인지, 어떤 에러가 있는지는 전혀 신경쓰지 않는다. 대표적인 장비로 케이블, 허브, 리피터가 있다. 리피터(repeater): 신호가 약해졌을 때 신호를 멀리 보내기 위한 증폭 장치. 현재는 다른 네트워크 장비에 기본적으로 리피터 기능이 탑재되어 있어 잘 사용하지 않음. 허브(hub): 리피터 역활을 하며, 여러 네트워크 기기를 연결하는 역활을 한다. 요즘은 스위치가 등장하여 거의 사용되지 않음. 2계층 - 데이터링크 계층(DataLink Layer) 2계층에서는 물리계층을 통해 송수신 되는 정보의 오류와 흐름을 관리하여 정보를 좀더 안전하게 전달할 수 있게 도와주는 역활을 한다. 물리 계층에서 생길 수 있는 오류를 감지하고 수정(CRC 기반 오류 제어)하고 흐름을 제어(송신자와 수신자 사이 데이터 처리 속도 조절)한다. 여러가지 프로토콜이 존재하는데 가장 대표적인 프로토콜로는 이더넷(Ethernet)이 있다. 이 계층에서는 맥(MAC) 주소를 가지고 통신하며 전송되는 단위는 프레임(Frame)이다. 대표적인 장비로 브리지, 스위치등이 있다. 이더넷(Ethernet): 네트워크에 연결된 기기들이 MAC 주소를 가지고 상호간에 데이터를 주고 받을 수 있도록 만들어진 통신 규약 브리지, 스위치는 2 이상의 링크 계층 네트워크를 결합하여 LAN을 확장 구성하는 장비 브리지(bridge): 브리지 당 수개의 포트, 한번에 하나의 프레임 전달. Store And Forward 방식 사용 Store And Forward: 프레임 전체를 내부 버퍼에 잠시 저장했다가 에러검출과 같은 처리를 완전히 수행한 후에야 목적지로 전송 스위치(switch): 브리지 보다 훨신 많은 포트, 한번에 여러 프레임을 병렬로 전달할 수 있어서 브리지보다 빠르다. Store And Forward 방식 뿐 아니라 Cut Through Method 방식도 사용. 허브(1계층 장비)가 특정 데이터를 모든 포트에 전달했다면 스위치는 포트마다 회선이 독립적이기 때문에 원하는 포트에만 선별적으로 데이터 전달이 가능 Cut Through Method: 프레임 전체가 수신 수신되기 전에 헤더 내의 목적지 주소만 보고서 판단해 즉시 전송, 속도는 빠르지만 에러복구 능력이 약함 3계층 - 네트워크 계층(Network Layer) 3계층에서 가장 중요한 기능은 데이터를 목적지까지 가장 안전하고 빠르게 전달하는 기능(라우팅)이다. 경로를 선택하고 주소를 정하여 패킷(3계층 통신 단위)을 전달해주는 것이 이 계층의 역활이다. 대표적인 장비로는 라우터(Router)가 있고, 요즘은 스위치(2계층 장비)에 라우팅 기능을 장착한 Layer 3 스위치도 있다. 라우터: 라우팅과 포워딩을 담당하는 장치, 서로 다른 네트워크를 연결 4계층 - 전송 계층(Transport Layer) 4계층은 양 끝단 사용자들이 데이터를 주고 받을 수 있게 해주며, 포트를 열어서 응용프로그램들이 전송을 할 수 있게 해준다. 대표적으로 TCP, UDP 프로토콜이 있으며 통신 단위는 TCP는 Segment, UDP는 Datagram TCP(Transmission Control Protocol): 신뢰성(패킷 손실, 중복, 순서바뀜 x)있고 연결 지향적인 프로토콜 UDP(User Datagram Protocol): 신뢰성이 없으며 비연결성인 프로토콜, 대신 빨라서 빠른 요청이 필요한 실시간에 적합 5계층 - 세션 계층(Session Layer) 5계층은 양 끝단의 응용 프로세스가 통신을 관리하기 위한 방법(반이중 방식(half-duplex), 전이중 방식(Full Duplex))을 제공하며 이 계층은 TCP/IP 세션을 만들고 없애는 역활을 하며 통신 단위는 data 반이중 방식(half-duplex): Sender와 Reciver는 한번에 하나씩 통신 가능 전이중 방식(Full Duplex): Sender와 Receiver는 동시에 정보를 전송하고 수신 가능 6계층 - 표현 계층(Presentation Layer) 6계층은 애플리케이션 계층(7계층)이 다양한 데이터 타입을 다루는 부담을 줄여준다. 인코딩이나 암호화 등을 담당하며 통신 단위는 data 7계층 - 응용 계층(Applicatoin Layer) 7계층은 응용 프로세스와 관계하여 일반적인 응용 서비스를 수행한다. 즉, 우리가 사용하는 사용자 인터페이스를 제공하는 프로그램을 말한다. 대표적으로 HTTP, FTP 프로토콜이 이 계층에 속하며 통신 단위는 data 참고 http://www.ktword.co.kr/test/view/view.php?m_temp1=4842 https://shlee0882.tistory.com/110 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "NETWORK OSI7 study http web",
    "url": "/study/http%20web/2022-08-19-OSI7/"
  },{
    "title": "PCB와 Context Switching",
    "text": "우리는 인터넷을 하고 있는 동시에 음악도 들을 수 있고 채팅도 할 수 있다. 어떻게 컴퓨터는 동시에 처리할까? 컴퓨터는 사실 동시에 처리하는 것이 아니라 각 프로그램을 일정시간 동안 번갈아가면서 실행(TIME SHARING)하고 있는데 그 속도가 매우 빨라서 우리가 동시에하고 있는 것처럼 느낄뿐이다. 프로세스들이 교체되어 수행되고 나면 다시 원래 하고 있던 프로세스를 불러와야 되는데 그때 원래 하던 작업을 기억하고 있어야 겠죠. 이때 프로세스 단위로 정보를 저장하는 구조가 바로 PCB(Process Control Block)입니다. PCB(Process Control Block) 란? 운영체제가 프로세스를 제어하기 위한 정보를 저장해놓는 곳으로, 프로세스 상태 정보를 저장하고 있는 구조체 프로세스 상태 관리와 문맥교환(Context Swiching)을 위해 필요 PCB는 프로세스 생성 시 만들어지며 주기억장치에 유지 다시 한번 예로 들면, 기존에 어떤 프로세스를 작업하고 있는데 급한 프로세스가 처리해달라고 요청이 오면 원래 하던 프로세스를 어딘가에 임시 저장을 해놓아야 급한 프로세스를 처리하고 나서도 다시 원래하던 애를 이어서 할 수 있겠죠. 즉, 프로세스에 대한 상태 정보를 저장할 공간이 필요한데 그게 바로 PCB입니다. PCB 저장 정보 Process ID: 프로세스 고유 식별번호로, PID(Process Identification Number)라고도 한다. Process State(프로세스 상태): 프로세스의 현재 상태(준비, 실행, 대기 등의 상태)를 저장 Program Counter(계수기): 다음에 실행되는 명령어의 주소를 저장 Process Priority(스케줄링 정보): 프로세스 우선순위 등과 같은 스케줄링 관련 정보를 저장 CPU Registers: 프로세스의 레지스터 상태를 저장하는 공간 Account(계정 정보): CPU 사용시간, 각종 스케줄러에 필요한 정보 저장 입출력 정보: 프로세스 수행 시 필요한 주변 장치, 파일등의 정보를 저장 이렇게 프로세스에 대한 상태를 저장한 PCB에 의존하여 프로세스를 변경하는데, 다른 프로세스로 변경하는 것을 Context Switching이라 한다. Context Switching 멀티 프로세스 환경에서 CPU가 하나의 프로세스를 실행하고 있는 상태에서 인터럽트 요청에 의해 다음 우선 순위의 프로세스가 실행되어야 할 때 기존의 프로세스 상태 값을 PCB에 저장하고 CPU가 다음 프로세스를 실행할 수 있도록 다음 PCB를 읽어 교체하는 작업을 Context Switching이라 한다. Context Switching 과정 현재 CPU는 process P0을 수행하고 있다가 인터럽트(interrupt)가 걸리게되면 현재 수행하고 있는 것을 먼저 PCB0에 저장을 한다. Waiting 상태로 변하게 되면 CPU는 다른 프로세스(process P1)을 Running으로 바꿔 올린다. CPU가 앞으로 수행할 프로세스(process P1)에 관한 정보로 교체 이번에는 CPU가 process P1을 수행하다 다시 인터럽트(interrupt)가 걸리면 현재 수행하고 있는 것을 PCB1에 저장한다. Waiting 상태로 변하게 되면 CPU는 다른 프로세스(process P0)을 Running으로 바꿔 올린다. CPU가 앞으로 수행할 프로세스(process P0)에 관한 정보로 교체 왜 Context Swiching이 필요? 만약 컴퓨터 매번 하나의 Task만 처리한다면 다음 Task를 처리하기 위해선 현재 Task가 끝날 때까지 기다려야 되기 때문에 매우 불편하다. 그래서 동시에 사용하는 것처럼 하기 위해 Context Switching이 필요하게 되었다. Context Switching 오버헤드 현재 Context Switching 과정을 보면 P0이 수행되다가 P1이 되고 P1을 수행하다 다시 P0이 수행이 실행이 된다. 이렇게 계속 교체가 되면 현재 수행되던 프로세스를 메모리에 저장하고, 다음 수행할 프로세스를 CPU에 넣어야 되는 이런 번거로운 일(overhead)이 추가가 되는데 왜 이렇게 하는 걸까? 그냥 P0을 한번에 한 다음 P1을 하면 되지 않을까? 프로세스 수행 중 입출력 이벤트가 발생했을 때 CPU를 사용하지 않게 되는데 이 CPU가 낭비되는 시간 동안 차라리 overhead가 발생하더라도 Context Switching을 통해 다른 프로세스를 실행시키는 게 전체적으로 봤을 때 더 효율적이기 때문에 overhead를 감수하고 Context Switching을 하는 것이고 그래서 운영체제가 CPU를 관리하는 것이다. 참고: https://jhnyang.tistory.com/33 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "os pcb context-switching etc operating system",
    "url": "/etc/operating%20system/2022-08-15-PCB-Context-Switching/"
  },{
    "title": "HTTP와 HTTPS의 동작 과정",
    "text": "HTTP와 HTTPS에 대한 설명은 이전글에 있습니다. HTTP의 동작 과정 사용자가 url 주소 입력 DNS 서버에 의해 서버의 ip 주소 찾음 웹서버와의 TCP 연결 시도 3 way handshake 웹서버와 http메시지 주고받음 클라이언트가 서버에게 요청(request)을 보내면, 서버가 클라이언트에게 응답(response) 전송 웹서버와 TCP연결 해제 대칭키 암호키와 비대칭키 암호화 먼저 HTTPS의 동작 과정을 알아보기전에 대칭키 암호화와 비대칭키 암호화에 대해 간단히 알아보자 대칭키 암호화 클라이언트와 서버가 동일한 키를 사용해 암호화/복호화 진행 하나의 키를 사용하기 때문에 연산 속도가 빠르지만 키가 노출되면 매우 위험 비대칭키 암호화 1개의 쌍으로 구성된 공개키와 개인키를 암호화/복호화 하는데 사용 키가 노출되어도 비교적 안전하지만 연산 속도가 느리다. 공개키: 모두에게 공개가능한 키 개인키: 나만 알고 있는 키 HTTPS의 동작 과정 HTTPS는 대칭키 암호화와 비대칭키 암호화를 모두 사용하여 빠른 연산 속도와 안정성을 모두 가지고 있다. HTTPS 연결 과정(Hand-Shaking)에서는 먼저 서버와 클라이언트 간의 세션키를 교환해야 하는데 여기서 세션키는 주고 받는 데이터를 암호화하기 위해 사용되는 대칭키이며, 데이터 간의 교환에는 빠른 연산 속도가 필요하므로 세션키는 대칭키로 생성한다. 이 세션키를 서버와 클라이언트가 안전하게 교환하는 과정에서 비대칭키가 사용된다. 클라이언트가 서버로 최초 연결 시도 서버는 공개키를 브라우저에게 넘겨줌 클라이언트는 세션키를 생성한 후 서버의 공개키로 암호화하여 서버로 전송 서버는 개인키로 암호화된 세션키를 복호화하여 세션키를 얻음 클라이언트와 서버는 동일한 세션키를 공유하므로 데이터를 서로 암호화/복호화 가능 추가로 봐야되는 부분 HTTPS 동작 과정중 2번을 보면 공개키를 브라우저에게 넘겨주는데 그냥 공개키만 넘겨버리는 것이 아니고 추가적인 과정이 있다. 서버는 클라이언트와 세션키를 공유하기 위한 공개키를 생성해야 하는데, 일반적으로 인증된 기관(Certificate Authority)에 공개키를 전송하여 인증서를 발급받는다. 자세한 과정을 한번 보자 A기업은 HTTP 기반의 애플리케이션에 HTTPS를 적용하기 위해 공개키/개인키를 발급 CA 기업에게 돈을 지불 후, 공개키를 저장하는 인증서 발급 요청 CA기업은 CA기업의 이름, 서버의 공개키, 서버의 정보 등을 기반으로 인증서 생성후, CA 기업의 개인키로 암호화하여 A기업에게 제공 A기업은 클라이언트에게 암호화된 인증서를 제공 클라이언트는 CA기업의 공개키를 미리 다운받아 갖고 있어, 암호화된 인증서를 복호화함 인증서안의 A기업의 공개키로 세션키를 공유 이렇게 그냥 공개키를 넘기는 것이 아닌 인증서를 넘기게 되면 인증서는 CA의 개인키로 암호화되었기 떄문에, 신뢰성을 확보할 수 있다. 또한 브라우저에는 인증된 CA 기관의 정보들이 사전에 등록되어 있어 인증된 CA기관의 인증서가 아닌 경우와 된 경우 다음과 같은 형태로 브라우저에서 보여진다. 그렇다면 언제 HTTP, HTTPS? HTTPS는 HTTP보다 안전하게 데이터를 주고 받을 수 있지만 암호화/복호화 과정이 추가되어있기 때문에 HTTP보다 속도가 느리다. 또한 인증서를 발급하고 유지하기 위한 추가 비용이 발생한다. 그래서 개인 정보와 같은 민감한 데이터를 주고 받아야 한다면 HTTPS를 이용하면 되고, 노출이 되어도 되는 단순한 정보를 주고 받는다면 HTTP를 이용하면 되겠다. 참고: https://mangkyu.tistory.com/98 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP HTTPS study http web",
    "url": "/study/http%20web/2022-08-11-http-https-run/"
  },{
    "title": "HTTP vs HTTPS",
    "text": "HTTP(Hyper Text Transfer Protocol)란? HTTP는 서버/클라이언트 모델에 따라 데이터를 주고받기 위한 프로토콜이다. 즉, 인터넷에서 하이퍼텍스트를 교환하기 위한 통신 규약으로 주로 80번 포트를 이용한다. HTTP는 TCP/IP 위에서 동작하는 프로토콜로 주요 특징으로는 서버가 요청에 응답을 마치면 연결을 끊는 Connectionless와 이전 통신에 대한 정보를 기억하고 있지 않는 Stateless한 특징을 가지고 있다. 하이퍼텍스트(HyperText): 중간에 다른 어떤 무언가를 거치지 않고도 다른 페이지로 접근할 수 있는 텍스트. 가장 대표적으로 HTML이 있다. 하지만 암호화 되지 않은 평문 데이터를 전송하는 프로토콜로 기밀한 정보(비밀번호, 주민등록번호 등)를 주고받기에 적절하지 않았는데 이러한 문제를 해결하기 위해 HTTPS가 등장하게 되었다. HTTP를 사용할 때 문제점? 위장 가능 해커가 자신이 클라이언트나 서버인 것처럼 위장이 가능 변조 가능 내가 받고 있는 데이터가 진짜 서버(클라이언트)가 보낸 것인지 확인을 하지 않기 때문에 중간에 해커가 개입해 변조 가능 HTTPS(Hyper Text Transfer Protocol over Secure Socket Layer)란? HTTPS는 HTTP에 암호화가 추가된 프로토콜이고 HTTP와 다르게 443번 포트를 사용한다. 네트워크 상에서 중간에 제3자가 정보를 볼 수 없도록 암호화를 지원하고 있다. SSL(Secure Socket Layer): 클라이언트와 서버 사이에 전송되는 데이터를 암호화하여 인터넷 연결을 보호하기 위한 표준 기술로 해커가 전송되고 있는 기밀 정보를 보거나 훔치는 것을 방지한다. TLS(Transport Layer Security): SSL와 같은 의미로, TLS가 공식적인 명칭이지만 SSL 이라는 이름이 더 많이 사용되고 있다. 다음글에 HTTP와 HTTPS의 자세한 동작 과정을 알아보도록 하겠습니다! 참고: https://mangkyu.tistory.com/98 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP HTTPS study http web",
    "url": "/study/http%20web/2022-08-10-http-https/"
  },{
    "title": "운영체제란?",
    "text": "운영체제란? 운영체제는 사용자가 컴퓨터를 편리하게 사용할 수 있도록 도와주는 소프트웨어이다. 운영체제 사용의 가장 큰 목적은 하드웨어 관리라고 할 수 있다. 컴퓨터에는 수 많은 하드웨어가 존재하는데 CPU, 메모리, 디스크, 키보드, 마우스, 모니터 등 이를 잘 관리해주어야 컴퓨터를 효율적으로 사용할 수 있다. 운영체제가 없다면 위에서 언급한 하드웨어에 관한 모든 관리를 사용자가 직접해야 되기 때문에 매우 불편할 것이다. 즉, 운영체제는 컴퓨터의 성능을 높이고 사용자에게 편의성을 제공하는 프로그램이라 할 수 있다. 부팅과정 컴퓨터의 전원이 켜지면 프로세서(CPU)는 ROM에 있는 내용을 읽는데 그 중 먼저 POST를 실행시킨다. ROM안에는 현재 컴퓨터의 상태를 검사하는 POST(Power-On Self-Test), 하드디스크에 저장되어 있는 운영체제를 찾아 RAM에 가지고 오는 부트 로더(boot loader)로 이루어져 있다. POST 작업이 끝나면 부트로더가 실행되어 OS를 RAM으로 가져오게 된다. 운영체제 구성 운영체제는 크게 커널(Kernel)과 쉘(Shell)로 나뉘어집니다. 커널(Kernel) 커널은 운영체제의 심장이자 뇌라고 할 수 있는데 대표적으로 여섯가지의 역활이 있다. 시스템 콜 인터페이스: 애플리케이션이 OS를 통해서 어떤 처리를 하고 싶으면 시스템 콜이라고 하는 명령을 이용해서 커널에 명령을 내린다. 이때 명령이 인터페이스를 통해 전달된다. 예를 들어 은행이나 구청 등의 접수 창구와 같다고 생각하면 되겠다. 프로세스 관리: OS는 수백, 수천개의 프로세스를 가동할 수 있는데 이에 비해 물리 서버의 CPU 코어 수는 많아봐야 수십개 정도밖에 안 된다. 그래서 언제, 어떤 프로세스가 어느 정도 CPU 코어를 이용할 수 있는지, 처리 우선순위를 어떻게 결정할 것인지 등을 관리한다. 메모리 관리: 프로세스 관리는 CPU 코어를 고려했지만, 메모리 관리에서는 물리메모리 공간의 최대치를 고려한다. 네트워크 스택: 네트워크에서 발생하는 데이터 처리나 교환에는 다양한 구조가 존재하는데 커널이 TCP/IP를 사용해서 간단히 통신할 수 있는 구조를 제공한다. 파일 시스템 관리: 파일 시스템용 인터페이스를 제공한다. 우리가 사용하는 문서 파일이나 엑셀 파일은 물리 디스크에 기록된 데이터로는 ‘0111…’ 같은 숫자에 불과한데 파일 시스템 덕분에 애플리케이션은 ‘파일’ 단위로 데이터를 작성하거나 삭제할 수 있다. 이 시스템을 관리. 장치 드라이버: 물리 장치용 인터페이스를 제공, 각각의 물리 장치는 제조사가 다양하기 때문에 각각의 대응하는 애플리케이션을 만들기는 어렵다. 그래서 장치 드라이버를 이용해 그 아래에 있는 물리 장치를 은폐한다. 쉘(Shell) 쉘은 명령어 해석기로 사용자가 커널(운영체제)에 요청하는 명령어를 해석하여 커널에 요청하고 그 결과를 출력한다. 사용자는 GUI(Grphical User Interface) 나 CLI(Command Line Interface) 같은 방식으로 운영체제 명령을 요청할 수 있다. 참고: https://velog.io/@codemcd/운영체제OS-1.-운영체제란() *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "os booting kernel shell etc operating system",
    "url": "/etc/operating%20system/2022-08-07-Operating-System/"
  },{
    "title": "프로세스 동기화(Process Synchronization)",
    "text": "Critical Section(임계 영역) 공유하는 자원에 접근하는 코드 영역을 Critical Section이라고 한다. 공유 데이터에 두 개 이상의 프로세스가 동시에 접근하면 데이터 불일치가 발생할 수 있는데 데이터 일관성을 유지하기 위한 매커니즘을 동기화라고 한다. Critical Section으로 인해 발생하는 문제들을 해결하기 위해서는 기본적으로 다음 조건들을 만족해야 한다. Mutual Exclusion (상호 배제) 이미 한 프로세스가 Critical Section에서 작업 중이면 다른 프로세스에서는 Critical Section에 진입하면 안된다. Progress (진행) Critical Section에서 작업 중인 프로세스가 없다면, Critical Section에 들어가고자 하는 프로세스가 존재하는 경우 진입할 수 있어야 한다. Bounded Waiting (한정된 대기) Critical Section에 진입하려는 프로세스가 무한정 기다려서는 안 된다. 동기화 방법 Mutex Lock Critical Section 문제를 해결하기 위한 가장 간단한 방법으로 mutext lock이 있다. Critical Section에 진입하는 프로세스는 들어갈 때 Lock을 걸어 이미 하나의 프로세스가 Critical Section에서 작업중일 때는 다른 프로세스가 Critical Section에 들어갈 수 없도록 한다. 그리고 빠져나올 때 다시 lock을 해제하여 동시에 자원을 접근하는 것을 막는다. 단점으로는 Critical Section에 프로세스가 존재할 때, 다른 프로세스들이 계속해서 진입하려고 시도하기 때문에 CPU를 낭비하게 된다(Busy Waiting). Semaphore Semaphore는 카운터(Counter)를 이용하여 동시에 자원에 접근할 수 있는 프로세스를 제한한다. 주로 s라는 변수로 나타내며, 이는 사용 가능한 자원의 개수를 의미한다. 오직 두개의 atomic한 연산(wait, signal)을 통해서 접근할 수 있다. Busy-Wait 방식 이 방식은 P 자원이 모두 사용 중이라면 wait 하는 방식으로 자원의 여유가 생기면 s–으로 자원을 획득하고 자원을 모두 사용했다면 s++를 통해 자원을 반납. //=wait(s) P(s){ while(s &lt;= 0) do wait s--; } //=signal(s) V(s){ s++; } Block - Wakeup 방식 먼저 세마포어를 아래 처럼 정의해준다. type struct{ int value; struct process *L; }semaphore; 먼저 자원의 값을 감소시키고, 만약 자원의 개수가 부족하다면 현재 프로세스를 wait queue에 추가시킨후, block을 호출해 중단시킨다. 다른 프로세스가 작업을 완료해 자원의 값을 증가시키며 반납을 했을 때, 자원의 수가 없다면(S.value &lt;= 0) 현재 대기하는 프로세스가 있다는 뜻이므로 wait queue에서 프로세스를 꺼내 wakeup을 호출한다. //=wait(S) P(S){ S.value--; if(S.value &lt; 0){ add this.process to S.L; block(); } } //=signal(S) V(S) { S.value++; if(S.value &lt;= 0){ remove a process P from S.L; wakeup(P); } } Busy-wait vs Block-wakeup Critical Section의 길이가 긴 경우는 Block-wakeup 방식이 유리하고 길이가 짧은 경우에는 오히려 잦은 문맥 교환으로 오버헤드가 생기게 되어 Busy-wait가 유리하다. Semaphore vs Mutex mutex와 semaphore의 가장 큰 차이점은 동기화 대상의 개수로 동기화 대상이 오직 하나일 때는 mutex, 동기화 대상이 하나 이상일 때는 semaphore를 사용한다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "os process-synchronization semaphore mutex-lock etc operating system",
    "url": "/etc/operating%20system/2022-07-29-Process-Synchronization/"
  },{
    "title": "가상 메모리(Virtual Memory)",
    "text": "가상 메모리란? 원래는 실행되는 코드의 전부를 물리 메모리에 적재시켜야 하고, 메모리 용량보다 큰 프로그램을 실행시킬 수 없을 뿐 아니라 여러 프로그램을 동시에 메모리에 올리기에는 용량의 한계, 페이지 교체등 성능 이슈가 발생한다. 하지만 가상 메모리는 프로세스 전체가 메모리에 올라 오지 않더라도 실행이 가능하게 하는 기법이고 프로그램이 물리 메모리보다 커도 상관없다. 가상 메모리 역활 가상 메모리는 작은 메모리를 가지고도 얼마든 큰 가상주소 공간을 프로그래머에게 제공한다. 가상 주소 공간은 한 프로세스가 메모리에 저장되는 논리적인 모습을 가상메모리에 구현한 공간이다. 실제로 필요한 부분만 물리 메모리에 올리고 필요하지 않은 부분은 물리 메모리에 올리지 않는 것이다. 예를 들어, 한 프로그램이 실행되며 200KB를 요구하는데 실행까지 필요한 메모리 공간이 50KB이면, 실제 물리 메모리에는 50KB만 올라가고 나머지 150KB는 나중에 필요시에 요구하게 된다. 프로세스간의 페이지 공유 가상 메모리는 프로세스들이 메모리를 공유하는 것을 가능하게 해주고, 프로세스들은 공유 메모리를 통해 통신할 수 있다. Demanding-paging 기법을 사용하여 다른 프로세스의 각각의 페이지가 같은 프레임을 가르키도록 하면 공유 메모리를 사용할 수 있다. Demand Paging 프로그램 실행 시작 시에 프로그램 전체를 물리 메모리에 적재하는게 아니라 초기에 필요한 것들만 적재하는 전략이며 가상 메모리 시스템에서 많이 사용된다. 가상 메모리는 대개 페이지로 관리되고, 프로세스 내의 개별 페이지들은 페이저(pager)에 의해 관리된다. 페이저는 프로세스 실행에 실제로 필요한 페이지들만 메모리로 가져오기 때문에 시간 낭비와 메모리 낭비를 줄일 수 있다. 페이지 교체 위에서 필요할 때 요구한다고 했는데 프로세스 동작에 필요한 페이지를 요청하는 과정에서 페이지 부재가 발생하게 되면, 원하는 페이지를 보조저장장치에서 가져오게 되는데 이 때 물리 메모리가 모두 사용중인 상황이라면 페이지 교체가 필요하다. 이때, 페이지 교체 알고리즘에 의해 희생될 페이지가 정해진다. *페이지 부재: 메모리에 적재된 페이지중에 필요한 페이지가 없는 경우 페이지 교체 알고리즘 FIFO 페이지 교체(FIFO Page Replacement) First In First Out으로 들어온 페이지 순서대로 페이지 교체 시점에 먼저 나가게 된다. 장점: 가장 간단한 방법으로 직관적이고 쉽다. 단점: 처음부터 활발하게 사용되는 페이즈를 교체해서 페이지 부재율을 높이는 부작용 초래 할 수도 있고 Belady Anomaly 현상이 발생할 수도 있다. *Belady Anomaly: 페이지를 저장할 수 있는 페이지 프레임의 갯수를 늘려도 오히려 페이지 부재가 더 많이 발생하는 이상 현상 최적 페이지 교체(Optimal Page Replacement) Belady Anomaly현상이 일어나지 않으며 앞으로 가장 오랫동안 사용되지 않을 페이지를 찾아 교체하는 알고리즘, 실제로 사용하기 보다 연구 목적을 위해 사용 장점: 알고리즘 중 가장 낮은 페이지 부재율 단점: 구현의 어려움 (어떻게 미리 파악할까?) LRU 페이지 교체(Least Recently Used Page Replacement) 가장 오랫동안 사용되지 않은 페이지를 선택하여 교체하는 알고리즘으로 성능이 좋으며 많은 운영체제가 채택하고 있는 알고리즘이다. LFU 페이지 교체(Least Frequently Used) 참조 횟수가 가장 적은 페이지를 교체하는 알고리즘 MFU 페이지 교체 (Most Frequently Used) 참조 횟수가 가장 많은 페이지를 교체하는 알고리즘 참고: https://github.com/JaeYeopHan/Interview_Question_for_Beginner *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "virtual-memory os etc operating system",
    "url": "/etc/operating%20system/2022-07-24-Virtual-Memory/"
  },{
    "title": "멀티 프로세스(Multi Process) vs 멀티 스레드(Multi Thread)",
    "text": "프로세스와 스레드에 관한 내용은 이전글을 참조해주세요 멀티 프로세스(Multi Process) 두개 이상 다수의 프로세서(CPU)가 각자 독립된 메모리 영역을 가지고 동시에 하나 이상의 작업을 처리하는 것이다. 멀티 프로세스의 장점 독립된 구조로 안정성이 높다. 하나의 프로세스가 죽더라도 다른 프로세스에 영향을 끼치지 않고 정상적으로 수행 멀티 프로세스의 문제점 프로세스 간의 Context Switching시 단순히 CPU 레지스터 교체 뿐만 아니라 RAM과 CPU사이의 캐시 메모리에 대한 데이터까지 초기화되므로 오버헤드가 크고 느리다. 멀티 스레드(Multi Thread) 하나의 프로세스에 여러 스레드로 자원을 공유하며 작업을 나누어 수행하는 것이다. 쉽게 말하면, 하나의 프로그램안에서 여러 작업을 해결하는 것 멀티 스레드의 장점 자원의 효율성 증대 멀티 프로세스로 실행되는 작업을 멀티 스레드로 실행할 경우, 프로세스를 생성하여 자원을 할당하는 시스템 콜이 줄어들어 자원을 효율적으로 관리할 수 있다. 멀티 스레드는 프로세스 내의 메모리를 공유하기 때문에 데이터를 주고 받는 것이 간단해져 시스템 자원 소모가 적다. 처리 비용 감소 및 응답 시간 단축 프로세스 간의 통신(IPC) 보다 스레드 간의 통신의 비용이 적어서 작업들 간의 통신 비용이 줄어듬(Stack 영역빼고 다 공유하기 때문) 프로세스간 전환 속도보다 스레드 간의 전환 속도가 빠름 멀티 스레드의 문제점 자원을 공유하기 때문에 동기화 문제가 생길 수 있다. 그래서 동기화 작업을 통한 작업 처리 순서를 컨트롤 해야된다. 또한 하나의 스레드에 문제가 생기면 전체 프로세스가 영향을 받을 수 있다. 멀티 프로세스 vs 멀티 스레드 둘다 동시에 여러 작업을 수행한다는 점에서는 같다. 그러나 멀티 프로세스 같은 경우 하나의 프로세스가 죽더라도 다른 프로세스에는 영향을 끼치지 않아 안정성이 높지만, 멀티 스레드보다 시간과 자원을 많이 잡아먹는 단점이 있다. 반대로 멀티 스레드 같은 경우는 하나의 스레드가 죽으면 전체 스레드가 종료될 수 있으며 동기화 문제를 갖고 있지만, 멀티 프로세스보다 적은 공간 차지와 빠른 처리의 장점이 있다. 따라서 적용할 시스템의 특징에 따라 적합한 방식을 선택해야한다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "multi-process multi-thread etc operating system",
    "url": "/etc/operating%20system/2022-07-17-Multi-Process-vs-Thread/"
  },{
    "title": "프로세스(Process) vs 스레드(Thread)",
    "text": "프로세스(Process) 프로세스는 실행 중인 프로그램으로 디스크로부터 메모리에 적재되어 CPU의 할당을 받을 수 있는 것을 말한다. 여기서 프로그램과 프로세스의 차이는 뭘까? 프로그램은 보조 기억장치에 존재하며 실행되기를 기다리는 명령어와 정적인 데이터의 묶음이다. 이 명령어와 정적 데이터가 메모리에 적재되면 프로세스가 된다. 프로세스 구조는 지역 변수같은 임시 자료를 가지는 스택과 전역 변수들을 가지는 데이터 영역, 그리고 동적으로 할당되는 메모리인 힙영역을 포함한다. 프로세스 제어 블록(Process Control Bolock, PCB) 어떻게 프로세스간 교체가 일어나면서 작업이 수행되는 걸까? 프로세스는 CPU를 할당받아 작업을 처리하다가 프로세스 전환이 발생하면 진행하던 작업을 저장하고 CPU를 반환해야 하는데, 이때 작업의 진행 상황을 PCB에 저장하고 나중에 다시 작업을 수행할 때 PCB에 저장되어있던 내용을 불러와 이어서 작업을 수행한다. PCB는 이런 특정 프로세스에 대한 중요한 정보를 저장하고 있는 운영체제의 자료구조이다. PCB 구성 요소 PID : 운영체제가 각 프로세스를 식별하기 위해 부여된 프로세스 식별번호(PID, Process Identification) 프로세스 상태: 프로세스는 빠르게 교체되면서 실행되기 때문에 다양한 상태들이 있다. new, ready, running, waiting, terminated 등 프로그램 카운터: CPU가 다음으로 실행할 명령어 스케쥴링 우선순위: 운영체제는 여러개의 프로세스를 동시에 실행하는 환경을 제공한다. 이 중 높은 우선순위를 가진 프로세스가 먼저 실행되는데 이를 스케줄링 우선순위라고 한다. 권한: 프로세스가 접근할 수 있는 자원을 결정하는 정보 프로세스의 부모와 자식 프로세스 프로세스의 데이터와 명령어가 있는 메모리 위치를 가리키는 포인터: 프로세스는 실행중인 프로그램이므로 프로그램에 대한 정보를 가지고 있어야한다. 프로세스에 할당된 자원들을 가르키는 포인터 실행 문맥: 프로세스가 저번 실행상태에서 마지막으로 실행한 레지스터 내용 스레드(Thread) 스레드는 프로세스의 실행 단위로 경량화된 프로세스라고 할 수 있다. 한 프로세스 내에서 동작되는 여러 실행 흐름으로 프로세스 내의 주소 공간이나 자원을 공유할 수 있다. 스레드는 같은 프로세스에 속한 다른 스레드와 코드 영역, 데이터 영역, 힙 영역 등 운영체제 자원들을 공유한다. 이렇게 하나의 프로세스를 다수의 실행 단위로 구분하여 자원을 공유하고 자원의 생성과 관리의 중복성을 최소화하여 효율을 높이는 방법을 멀티 스레딩이라고한다. 이 경우 각각의 스레드는 독립적인 작업을 수행해야 하기 때문에 각자의 스택과 PC 레지스터 값을 가지게 된다. 스택영역 같은 경우 지역변수같은 독립적인 값들이 저장되어 있기 때문에 독립된 스택을 할당하고, PC Register 같은 경우 스레드는 CPU를 할당받았다가 다시 스케쥴러에 의해 선점당할 수 있기 때문에 어디까지 수행했었는지 기억할 필요가 있다. 그래서 PC 레지스터는 독립적으로 할당한다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "process thread etc operating system",
    "url": "/etc/operating%20system/2022-07-12-Process-Thread/"
  },{
    "title": "어떤 깃 브랜치 전략을 사용해야 할까?",
    "text": "새로운 팀 프로젝트에 들어가기 앞서 협업을(물론 현재 백엔드는 혼자지만..) 더 효율적으로 하기 위해 어느 깃 브랜치 전략을 사용할지 정하기 위해 찾아봤는데 대표적으로 git flow와 github flow가 있었다. 둘 중에 자기 팀에게 맞는 전략을 택하면 되는데 일단 둘 다 무엇인지 간단하게 알아보자. (tmi: 이 글과는 관계가 없지만, 다른 팀 프로젝트에 지금 코드 리뷰를 적용하고 있는데 조만간 적용 과정과 후기를 적겠다.) 깃 브랜치 전략(git branch strategy)이란? 여러 개발자가 협업하는 환경에서 git 저장소를 효과적으로 활용하기 위한 work-flow. 즉, 브랜치 생성의 규칙을 만들어 협업을 원활하게 하는 방법이다. 브랜치 전략이 없다면? 어느 브랜치가 최신인지, 어느 브랜치가 내가 원하는 브랜치인지 알기 힘들다. 이런 상황을 최소화하기 위한 것이 바로 브랜치 전략이다. GIT FLOW GIT-FLOW 전략은 5개의 브랜치를 이용하는 전략으로써 항상 유지되는 2개의 메인 브랜치와 역활을 완료하면 사라지는 3개의 보조 브랜치로 이루어진다. 메인 브랜치: 항상 유지 master: 제품으로 출시될 수 있는 브랜치 develop: 다음 출시 버전을 개발하는 브랜치 보조 브랜치: merge 되면 사라짐 feature: 기능을 개발하는 브랜치 release: 출시 버전 준비 브랜치 hotfix: 긴급한 버그 수정 브랜치 GIT-FLOW 개발 프로세스 개발자는 develop 브랜치로부터 본인이 개발할 기능을 위한 feature 브랜치를 생성 기능이 완성되면 develop 브랜치에 merge develop 브랜치에 merge 후, QA를 위해 release 브랜치를 생성 release 브랜치에서 오류가 발생한다면 release 브랜치 내에서 수정한다. QA가 끝나면 해당 버전을 배포하기 위해 master 브랜치로 merge, bugfix가 있었다면 해당 내용을 반영하기 위해 develop 브랜치에도 merge 만약 제품(master branch)에서 버그가 발생한다면, hotfix 브랜치를 만든다. hotfix 브랜치에서 버그 픽스가 끝나면, develop과 master 브랜치에 각각 merge GIT FLOW 과정에 대해 더 상세히 알고 싶다면 https://www.youtube.com/watch?v=EzcF6RX8RrQ GIT FLOW의 특징 주기적으로 배포를 하는 서비스에 적합 가장 유명한 전략인만큼 많은 IDE가 지원 GITHUB FLOW 최신버전인 master 브랜치만 존재 GITHUB FLOW 개발 프로세스 기능 개발, 버그 픽스 등 branch 생성 개발(커밋 메시지는 명확하게) 개발 완료 후 pull request 생성 충분한 리뷰와 토의 리뷰가 끝나면 실제 서버(혹은 테스트환경)에 배포 문제가 없다면 master에 merge 후 push 하고, 배포 GITHUB FLOW의 특징 단순해서 처음 접하는 사람에게도 유용 CI, CD가 자연스럽게 이루어짐 어떤 전략 사용? GIT FLOW: 한달 이상의 긴 호흡으로 개발하여 주기적으로 배포하고, QA 및 배포, hot fix 등을 수행할 수 있는 여력이 있는 팀이라면 GIT FLOW GITHUB FLOW: 항상 릴리즈되어야 할 필요가 있는 서비스와 지속적으로 테스트하고 배포하는 팀이라면 GITHUB FLOW 같은 간단한 workflow 어떤 깃 브랜치 전략을 쓸지 정말 고민을 많이 했다. 현재 백엔드는 혼자인 상황으로 간단하고 빠르게 GITHUB FLOW로 가져갈까 생각했으나 추후에 팀원이 늘어날 걸 고려, 미리 GIT FLOW 경험, 주마다 배포할 계획으로 GIT FLOW를 선택하기로 생각했다. 하지만 기본적으로 GIT FLOW 모델을 유지하되 현재 상황에 맞게 변형해서 사용하기로 했다. 현재 상황은 혼자로 모든 브랜치를 운영하며 개발하기 힘들기 때문이다. 추후에 필요하면 더 추가하면 될 것 같다. 그래서 결론적으로 현재 가져갈 GIT FLOW 브랜치 전략으로는 GIT FLOW master - 배포 develop - 개발 feature - 기능 hotfix - 긴급한 버그 수정 feature 브랜치가 완성되면 develop 브랜치로 pull request를 날린다. 그러면 팀원이 코드 리뷰를 하고 난 후 merge 주마다 master 브랜치로 merge 해 배포 가 될것 같다. 현재 코드 리뷰 해줄 백엔드 팀원은 없지만.. 나중에 추가될 때를 위해 지금부터 팀원이 있다고 생각하며 혼자 코드 리뷰를 진행하며 해야겠다. 참고 https://techblog.woowahan.com/2553/ https://www.youtube.com/watch?v=jeaf8OXYO1g *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "git-branch-strategy git-flow github-flow project",
    "url": "/project/2022-07-07-git-branch-strategy/"
  },{
    "title": "RDB vs NoSQL",
    "text": "RDB(Relational Database) 관계형 데이터베이스인 RDB(Relational Database)는 관계형 모델을 기반으로 하는 데이터베이스이다. 이를 관리하기 위한 시스템을 RDBMS(Relational Database Management System)라 하고 이러한 RDBMS는 주로 SQL을 이용해 데이터를 조작하게 된다. 관계형 데이터베이스 특징 2차원 데이터로 표현(행/열) 상호 관련성을 가진 테이블의 집합으로 구성 테이블 사이의 관계를 외래키로 나타냄 스키마 변경이 어렵다. 수직 확장(Scale-up)은 가능하지만, 수평 확장(Scale-out)은 어렵다. 수직 확장 같은 경우 하드웨어 확장을 의미, 수평 확장의 경우 양적 확장을 의미한다. 회원 Table 회원 번호(PK) 회원 이름 나이 1 회원A 20 2 회원B 25 주문 Table 주문 번호(PK) 주문 회원 번호(FK) 주문 가격 20220704bsad 1 50000 20220703sdsd 2 100000 NOSQL (Not only SQL) NoSQL 데이터베이스는 전통적인 관계형 데이터베이스 보다 덜 제한적인 일관성 모델을 이용하여 데이터의 저장 및 검색을 제공하는 데이터베이스이다. RDB와는 달리 테이블 간 관계를 정의하지 않는다. NoSQL은 빅데이터의 등장으로 인해 데이터와 트래픽이 기하급수적으로 증가함에 따라 RDBMS에서 처리하려면은 비용이 기하급수적으로 증가(RDB에서는 성능 향상하려면 장비를 추가하는 수직 확장인 Scale-up)하기 때문에 데이터 일관성은 포기하되 비용을 고려하여 여러 대의 데이터에 분산하여 저장하는 수평 확장 방식인 Scale-out을 위해 등장했다. 저장 방식에 따른 NoSQL 분류 Key-Value Model Key와 Value 쌍으로 저장되는 데이터 저장 방식으로 대표적으로 Redis가 있다. 단순한 저장구조로 인하여 복잡한 조회 연산을 지원하지 않는다. 고속 읽기와 쓰기에 최적화된 경우가 많다. Document Model Key-Value 모델을 확장한 구조로 하나의 키에 하나의 구조화된 문서를 저장하고 조회한다. 또한 검색에 최적화 되어 있다. Key-Value Model과 다른 점은 Document Model 같은 경우 Value가 계층적인 형태인 문서로 저장된다. 대표적으로 MongoDB가 있다. Wide Column Model 하나의 키에 여러 개의 컬럼 이름과 컬럼 값의 쌍으로 이루어진 데이터를 저장하고 조회한다. 키는 Row(키 값)와 Column-family, Column-name을 가진다. 연관된 데이터들은 같은 Column-family 안에 속해 있으며, 각자의 Column-name을 가진다. 대표적으로 HBase가 있다. Graph Model 노드와 엣지로 그래프에 데이터를 표현하는 데이터베이스로 개체와 관계를 그래프 형태로 표현한 것이므로 관계형 모델이라 할 수 있으며, 데이터 간의 관게가 탐색의 키일 경우 적합하다. 대표적으로 Neo4J가 있다. 그래서 RDB, NoSQL 어떨 때 사용해야 될까? RDB는 데이터 구조가 명확하며 변경 될 여지가 없으며 명확한 스키마가 중요한 경우 사용하는 것이 좋다. 또한 중복된 데이터가 없어 변경이 용이하기 때문에 관계를 맺고 있는 데이터가 자주 변경이 이루어지는 시스템에 적합. NoSQL은 정확한 데이터 구조를 알 수 없고 데이터가 변경/확장이 될 수 있는 경우에 사용하는 것이 좋다. 하지만 데이터 중복이 발생할 수 있어 변경이 많이 이루어지지 않는 시스템이 좋고 또한 막대한 데이터를 저장하기 위해 수평확장(Scale-out) 해야 되는 시스템에 적합하다. 참고 https://docs.microsoft.com/ko-kr/dotnet/architecture/cloud-native/relational-vs-nosql-data https://www.scylladb.com/glossary/wide-column-database/ https://khj93.tistory.com/entry/Database-RDBMS%EC%99%80-NOSQL-%EC%B0%A8%EC%9D%B4%EC%A0%90 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "database RDB NOSQL etc",
    "url": "/etc/database/2022-07-04-RDB-NoSQL/"
  },{
    "title": "데이터베이스 - 교착상태(Deadlock)",
    "text": "교착상태(Deadlock)이란? 데이터베이스에서는 기본적으로 트랜잭션들의 동시성을 제어하기 위해 잠금(Locking)을 사용한다. 이러한 잠금은 데이터의 무결성을 지켜주지만 그 부작용으로 교착상태가 발생할 수 있다. 교착상태란 여러개의 트랜잭션들이 실행을 하지 못하고 서로 무한정 기다리는 상태를 의미한다. 트랜잭션 1이 테이블 B의 첫번째 행의 잠금을 얻고 트랜잭션 2도 테이블 A의 첫번째 행의 잠금을 얻고나서 Transaction 1&gt; create table B (i1 int not null primary key) engine = innodb; Transaction 2&gt; create table A (i1 int not null primary key) engine = innodb; Transaction 1&gt; start transaction; insert into B values(1); Transaction 2&gt; start transaction; insert into A values(1); 트랜잭션을 commit 하지 않고 서로의 첫번째 행에 잠금을 요청하면 Deadlock 발생 Transaction 1&gt; insert into A values(1); Transaction 2&gt; insert into B values(1); ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction 교착 상태 해결방법 예방 기법 각 트랜잭션이 실행되기전에 필요한 데이터를 모두 잠금(Locking)하는 것. 하지만 데이터가 많이 필요할 경우 모든 데이터를 잠금해야 되기 때문에 트랜잭션의 병행성을 보장하지 못한다. 뿐만 아니라 몇몇 트랜잭션들은 계속해서 처리를 못하게 되는 기아 상태가 발생할 수 있다. 회피 기법 위의 단점 때문에 실제로 교착상태를 해결하기 위한 방법으로 회피 기법이 많이 사용된다. 회피 기법은 자원을 할당할 때 시간 스탬프(Time Stamp)를 사용하여 교착상태가 일어나지 않도록 회피하는 방법으로 Wait-Die 방식과 Wound-Wait 방식이 있다. Wait-Die 방식 다른 트랜잭션이 데이터를 점유하고 있을 때 기다리거나(Wait) 포기(Die)하는 방식으로 선행 트랜잭션이 접근하면 대기(wait), 후행 트랜잭션이 접근하면 포기(die)한다. 즉, 오래된 프로세스에게 대기의 기회를 제공하고, 최신 프로세스는 자주 복귀함으로써 오버헤드의 가능성이 있다. Wound-Wait 방식 다른 트랜잭션이 데이터를 점유하고 있을 때 빼앗거나(Wound) 기다리는(Wait) 방식으로 선행 트랜잭션이 접근하면 선점(wound), 후행 트랜잭션이 접근하면 대기(wait)한다. 즉, 오래된 프로세스에게 선점의 기회를 제공하고 최신 프로세스는 대기함에 따라 복귀를 최소화한다. 낙관적 병행 기법 낙관적 병행 기법은 트랜잭션이 실행되는 동안에는 아무런 검사를 하지 않고, 트랜잭션이 다 실행된 이후에 검사 후 문제가 있다면 되돌리는 방법 빈도 낮추기 교착 상태의 빈도를 낮추는 방법으로는 트랜잭션을 자주 커밋 트랜잭션들이 동일한 테이블 순으로 접근하게 한다. 읽기 잠금 획득( SELECT ~ FOR UPDATE)의 사용을 피한다. 테이블 단위의 잠금을 획득해 갱신을 직렬화 참고: 회피 기법 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "database deadlock etc",
    "url": "/etc/database/2022-07-03-Deadlock/"
  },{
    "title": "3계층형 시스템의 흐름(웹 데이터 흐름)",
    "text": "3계층형 시스템의 구성도 먼저 3계층형 시스템의 전체 구성을 보면 아래 그림과 같다. 간단하게 살펴보면 가장 아래 세 대의 서버가 스위치를 경유해서 연결돼 있다. 서버 내부에는 CPU, 메모리, 디스크, NIC/HBA 같은 하드웨어 부품이 있다. 그 위에가 CPU와 메모리 영역을 확대한 것으로 이번 글에서 주로 다루는 부분이다. 웹 데이터 흐름 클라이언트 PC부터 웹 서버까지 웹 브라우저가 요청을 발행 이름 해석 웹 서버가 요청 접수 웹 서버가 정적 콘텐츠인지 동적 콘텐츠인지 판단 필요한 경로로 데이터 액세스 여기서 이름해석이 뭘까? 원하는 웹서버로 요청을 보내기 위해서는 웹서버의 IP 주소를 알아야 되는데 우리는 URL을 입력했기 때문에 그 URL의 맞는 IP를 반환받는 과정이 있어야 한다. 인터넷상의 주소는 ‘IP’라는 숫자로 표현돼 있어서 문자열인 URL과 IP가 연결되어있어야 한다. 이렇게 웹 서버까지 도착하였으면 4번과정을 수행한다. 웹 서버의 역활은 HTTP 요청에 대해 적절한 파일이나 콘텐츠를 반환하는 것이다. 요청에 대한 대답 내용은 HTML 파일이라는 텍스트 데이터나 이미지, 동영상 등의 바이너리 데이터로 구성된다. 이 데이터들은 ‘정적 콘텐츠’와 ‘동적 콘텐츠’로 분류할 수 있다. ‘정적 콘텐츠’란 실시간으로 변경할 필요가 없는 데이터를 가리킨다. 예를들어 회사로고 같은 것이 있다. 이런 데이터 갱신 빈도가 낮은 것은 디스크에 저장해 요청이 있으면 웹 브라우저로 반환한다. ‘동적 콘텐츠’란, 높은 빈도로 변경되는 데이터를 가리킨다. 예를들어 은행 잔고 정보나 쇼핑몰 장바구니 등이 있다. 이런 데이터는 서버 내부 디스크에 저장하면 갱신 빈도가 높아 병목현상의 원인이 될 수 있다. 또한, 파일이라는 형태로 저장하는 것이 비효율적일 수 있기 때문에 이런 동적 컨텐츠는 AP 서버로 요청을 던지고 결과를 기다린다. 웹 서버부터 AP 서버까지 AP 서버는 ‘동적 콘텐츠’에 대한 요청을 처리한다. 웹 서버로부터 요청 스레드가 요청을 받으면 자신이 계산할 수 있는지, DB 접속이 필요한지 판단 DB 접속이 필요하면 연결 풀에 액세스 DB 서버에 요청 2번을 조금 더 설명하자면 자신이 계산할 수 있다는 것은 ‘1 + 1’ 같은 단순한 요청이고, 데이터 접속이 필요한 요청은 ‘사용자 잔금 정보’ 같은 AP서버가 가지고 있지 않은 정보이다. 데이터가 필요하면 DB 서버에 접속하는 것이 일반적이지만 항상 효율적이라 할 수 없다. 규모가 작고 갱신 빈도가 낮은 정보는 캐시로 저장해 두었다가 반환하는 것이 좋다. AP 서버부터 DB 서버까지 DB 서버에서 요청을 접수한다. 요청은 SQL이라는 언어 형태로 이루어지고 이 SQL을 해석해서 데이터 액세스 방식을 결정하고 필요한 데이터만 가져오는것이 데이터베이스의 역활이다. AP서버로부터 요청 프로세스가 요청을 접수하고 캐시가 존재하는지 확인 캐시에 없으면 디스크에 액세스 디스크가 데이터를 반환 데이터를 캐시 형태로 저장 결과를 AP 서버에 반환 앞의 그림들에서는 DB 서버의 디스크 액세스 부분이 갼략화되어 있어서 많은 기업형 시스템의 실정이 잘 반영돼 있지 않았다. 실제로는 DB 서버 내부 디스크는 이중화 관점에서 뒤떨어져 직접 사용하는 경우는 드물고 대부분 아래 그림처럼 별도 저장 장치를 이용한다. 저장 장치에는 다수의 디스크가 설치돼 있다. 하지만 본질적인 구조는 지금까지 등장한 웹 서버, AP 서버, DB서버와 큰 차이가 없다. 대량의 데이터에 고속 액세스하기 위한 전용 서버라고 생각하면 된다. AP서버부터 웹 서버까지 이번에는 다시 같은 경로를 이용해 반환되는 과정을 보자. DB 서버에서 AP 서버의 요청 스레드로 결과가 반한된다. 이 데이터를 가공한 후에 웹 서버로 데이터를 반환한다. DB 서버로부터 데이터가 도착 스레드가 데이터를 가공한 후 파일 데이터 생성 결과를 웹 서버로 반환 가공 결과가 텍스트 데이터라면 HTML이나 XML 파일을 사용하는 것이 일반적이고 이외에 바이너리 데이터를 생성해서 반환하는 경우도 있다. HTTP로 전송 가능한 데이터라면 어떤 형태이든 상관없다. 웹 서버부터 클라이언트 PC까지 AP 서버에서 반환된 데이터를 받아 httpd 프로세스가 PC의 웹 브라우저로 그대로 반환 AP 서버로부터 데이터 도착 프로세스는 받은 데이터를 그대로 반환 결과가 웹 브라우저로 반환되고 화면에 표시 웹 데이터의 흐름 정리 각 서버의 동작은 다르지만 몇가지의 공통점이 있다. 프로세스나 스레드가 요청을 받고 도착한 요청을 파악해서 필요에 따라 별도 서버로 요청을 보낸다. 그리고 도착한 요청에 대한 응답을 한다. 3계층 시스템에서는 사용자 요청이 시작점이 돼서 해당 요청이 다양한 서버로 전달된다. 자신이 할 수 없는 처리는 다음 서버로 역활을 넘긴다는 것이다. 3계층이라 하고 있지만, 실제로는 대부분 더 많은 계층을 사용하고 있다. 참고: 그림으로 공부하는 IT 인프라 구조 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "infra three-tier-system web-flow study it infra",
    "url": "/study/it%20infra/2022-06-30-three-tier-system/"
  },{
    "title": "트랜잭션(Transaction)",
    "text": "트랜잭션이란? 데이터베이스의 상태를 변환시키는 하나의 논리적인 작업 단위를 구성하는 연산들의 집합 예를들어 A계좌에서 B계좌에서 이체하는 상황일 때 A 계좌의 잔액을 확인 A 계좌의 금액에서 이체할 금액을 빼고 저장 B 계좌의 잔액을 확인 B 계좌의 금액에서 이체할 금액을 더하고 저장 이러한 과정이 모두 합쳐져 계좌 이체라는 하나의 작업단위 구성 하나의 트랜잭션은 Commit되거나 Rollback 될 수 있다. Commit 트랜잭션이 정상적으로 완료된 상태 Rollback 하나의 트랜잭션 처리가 비정상으로 종료되었을 때 이 트랜잭션이 행한 모든 연산을 취소 트랜잭션의 특성 원자성(Atomicity) 트랜잭션을 처리하는 도중 문제가 발생하면 트랜잭션에 해당하는 어떠한 작업 내용도 수행되지 않고, 아무런 문제가 발생되지 않을 경우에만 정상적으로 모든 작업이 수행되어야 한다. 일관성(Consistency) 트랜잭션이 완료된 후에도 트랜잭션이 일어나기 전의 상황과 동일하게 데이터의 일관성을 보장해야한다. 고립성(Isolation) 각각의 트랜잭션은 서로 간섭없이 독립적으로 수행되어야 한다. 지속성(Durability) 트랜잭션이 정상적으로 종료된 후에는 영구적으로 반영되어야 한다. 트랜잭션의 상태 활동(Active) 트랜잭션이 실행 중인 상태 부분 완료(Partially Commited) 트랜잭션의 마지막 연산까지 실행했고, Commit 연산만 남은 상태 실패(Failed) 트랜잭션 실행에 오류가 발생해 중단된 상태 완료(Committed) Commit 연산까지 실행해 트랜잭션이 정상적으로 완료된 상태 철회(Aborted) 트랜잭션이 비정상적으로 종료된 후 Rollback 연산으로 인해 트랜잭션 실행 전으로 돌아간 상태 Commit(커밋): 하나의 트랜잭션 작업이 성공적으로 끝났고 데이터베이스가 일관성인 상태에 있을 때, 이 성공적인 작업을 트랜잭션 관리자에게 알려주는 것을 Commit 연산이라고 한다. Rollback(롤백): 하나의 트랜잭션 처리가 비정상적으로 종료되어 데이터베이스의 일관성이 깨진경우, 이 트랜잭션의 일부가 정상적으로 처리되었더라도 원자성에 근거해 이 트랜잭션이 행한 모든 연산을 취소하는 연산으로 Rollback시에 변경한 내용을 전부 원래대로 되돌린다. Partially Committed 와 Committed 차이점 Commit 요청이 들어오면 Partially Committed 상태가 된다. 이후 Commit을 정상적으로 수행할 수 있으면 Committed 상태가 되고, 만약 오류가 발생하면 Failed 상태가 된다. 트랜잭션을 사용할 때 주의할 점 일반적으로 데이터베이스의 커넥션의 개수가 제한적이기 때문에 트랜잭션의 범위를 최소화하는 것이 좋다. 많아지면 커넥션을 가져가기 위해 기다려야하는 상황이 발생한다. 참고: https://github.com/WeareSoft/tech-interview *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "database transaction etc",
    "url": "/etc/database/2022-06-29-Transaction/"
  },{
    "title": "데이터베이스 - 조인(Join)",
    "text": "Join이란? 한 데이터베이스 내의 여러 테이블의 레코드를 조합하여 하나의 열로 나타낸 것이다. 관계형 데이터베이스는 구조적 특징으로 정규화를 수행해 의미있는 데이터의 집합으로 테이블을 나누게 되고 각 테이블은 관계를 가진다. 여러 테이블로 나뉘어 저장되므로 각 테이블에 저장된 데이터를 효과적으로 검색하기위해 조인이 필요하다. Join의 종류 기존의 db 내부 조인(INNER JOIN) 가장 흔한 결합 방식이며, 기본 조인 형식으로 간주된다. 2개의 테이블의 컬럼 값을 조합함으로써 새로운 테이블을 생성 명시적 조인 표현(explicit)과 암시적 조인 표현(implicit)이 있다. 명시적 조인 표현 &lt;!-- 명시적으로 JOIN, ON 키워드 사용--&gt; SELECT * FROM employee INNER JOIN department ON employee.DepartmentID = department.DepartmentID; 암시적 조인 표현 &lt;!-- 단순히 FROM에서 컴마로 구분 --&gt; SELECT * FROM employee, department WHERE employee.DepartmentID = department.DepartmentID; 결과 a. 자연 조인(NATURAL JOIN) INNER JOIN의 한 유형으로 동일한 컬럼명을 가진 컬럼의 각 쌍에 대해 단 하나의 컬럼만 포함 SQL SELECT * FROM employee NATURAL JOIN department; 결과 b. 교차 조인(CROSS JOIN) INNER JOIN의 한 유형으로 조인되는 두 테이블에서 곱집합을 반환한다. SQL &lt;!-- 명시적 조인 표현 --&gt; SELECT * FROM employee CROSS JOIN department; &lt;!-- 암시적 조인 표현 --&gt; SELECT * FROM employee, department; 결과 외부조인(OUTER JOIN) 조인 대상 테이블에서 특정한 테이블의 데이터가 모두 필요한 상황에서 외부조인을 활용하여 효과적으로 결과 집합 생성 가능 a.왼쪽 외부 조인(LEFT OUTER JOIN) 왼쪽 테이블의 모든 데이터를 포함하는 결과를 생성 SQL SELECT * FROM employee LEFT OUTER JOIN department ON employee.DepartmentID = department.DepartmentID; 결과 b. 오른쪽 외부 조인(RIGHT OUTER JOIN) 오른쪽 테이블의 모든 데이터를 포함하는 결과를 생성 SQL SELECT * FROM employee RIGHT OUTER JOIN department ON employee.DepartmentID = department.DepartmentID; 결과 c. 완전 외부 조인(FULL OUTER JOIN) 양쪽 테이블 모든 데이터 포함하는 결과 생성 SQL SELECT * FROM employee FULL OUTER JOIN department ON employee.DepartmentID = department.DepartmentID;; 결과 조인 사용할 때 주의사항 및 고려사항 SQL을 어떻게 작성하냐에 따라 성능이 크게 차이나기 때문에 명확하게 정의해야 한다. 조인 조건을 명확하게 제공하지 않을 경우 의도치 않게 CROSS JOIN이 수행될 수 있다. 조건을 먼저 적용하여 관계를 맺을 집합을 최소화한 후, 조인을 맺으면 효율적이다. INNER JOIN vs LEFT JOIN 그래서 INNER JOIN과 LEFT JOIN의 차이점은 뭘까? INNER JOIN은 겹치지 않는 행이 존재할 경우 결과에서 제외된다. LEFT JOIN은 왼쪽의 모든 행을 조회하고 조건과 맞지 않는 행은 NULL로 채워진다. 참고: https://github.com/WeareSoft/tech-interview *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "database join etc",
    "url": "/etc/database/2022-06-27-Join/"
  },{
    "title": "인프라 아키텍처(Infra Architecture)",
    "text": "인프라(Infra)란? 인프라는 우리말로 하면 ‘기반’이란 뜻으로, 우리 생활을 지탱하는 바탕이나 토대라는 의미다. IT 인프라도 마찬가지로 IT의 기반이 되는 것으로 우리 생활을 지탱하고 있다. 예를 들어 인터넷 검색 엔진에서 검색을 하면 많은 검색 결과를 얻을 수 있다. 이런 방대한 데이터는 어떻게 관리될까? 이것을 지탱하고 있는 것이 IT 인프라다. 그러면 인프라 아키텍처는 무엇일까? IT 인프라의 구조를 의미한다. 우선 아키텍처에는 어떤 방식이 있는지 간단하게 그림으로 한번 보자. 자세한건 뒤로가며 설명한다. 궁극의 아키텍처는 존재할까? 결론부터 말하자면 없다. 각 아키텍처 설계 요소에는 반드시 장점과 단점이 공존하기 때문에 자기 상황에 맞게 설계하는것이 중요하다. 즉, 시스템의 가장 중요한 장점은 살리고 단점을 최소화하도록 설계하는 것이 중요 집약형과 분할형 아키텍처 IT 인프라는 컴퓨터로 구성되는데 기본적인 구성 방식에는 ‘집약형’과 분할형’이 존재한다. 집약형 아키텍처 하나의 대형 컴퓨터를 이용해서 모든 업무를 처리하는 형태. 컴퓨터를 구성하는 주요 부품은 모두 다중화돼 있어서 하나가 고장 나더라도 업무를 계속할 수 있음. 또한, 복수의 서로 다른 업무 처리를 동시에 실행할 수 있도록 유한 리소스 관리도 함. 장점 구성이 간단 리소스 관리나 이중화에 의해 안정성이 높고 고성능 단점 도입 비용과 유지 비용이 비쌈 확장성의 한계 분할형 아키텍처 여러 대의 컴퓨터를 조합해서 하나의 시스템을 구축하는 구조. 분할형 아키텍처는 표준 OS나 개발 언어를 이용하기 때문에 ‘오픈 시스템’이라고도 부르고 여러 대의 컴퓨터를 연결해서 이용하기 떄문에 ‘분산 시스템’이라 부르는 경우도 있다. 장점 낮은 비용으로 시스템 구축 가능 확장성이 높다. 단점 관리 구조가 복잡 한 대가 망가지면 영향 범위를 최소화하기 위한 구조 검토 필요 수직 분할형 아키텍처 분할형에는 서버의 역활 분담을 고려해야 하는데 각각의 서버가 다른 역활을 하는지 아니면 비슷한 작업을하는지에 대한 관점이다. 수직 분할형은 서버별로 다른 역활을 담당하는 관점의 분할이다. *수직형이라고 표현하는 것은, 특정 서버 측면에서 봤을때 역활에 따라 ‘위’ 또는 ‘아래’ 계층으로 나뉘기 때문 클라이언트-서버형 아키텍처 클라이언트-서버형은 수직 분할형의 한 예로 서버에 클라이언트가 접속하는 형태 장점 클라이언트 측에서 많은 처리를 실행할 수 있어 소수의 서버로 다수의 클라이언트 처리 가능 단점 클라이언트 측의 소프트웨어 정기 업데이트가 필요하다. 확장성의 한계 3계층형 아키텍처 클라이언트-서버형 아키텍처의 단점을 개선하려고 한 것이 3계층형이다. 3계층형도 수직 분할형의 예로 ‘프레젠테이션 계층’, ‘애플리케이션 계층’, ‘데이터 계층’의 3층 구조로 분활돼있다. 우리가 주로 사용하고 있는 사이트의 대부분이 이 3계층 구조를 사용하고 있다. 장점 서버 부하 집중 개선 클라이언트 단말의 정기 업데이트 불필요 단점 구성이 복잡하다. 수평 분할형 아키텍처 더 높은 확장성을 실현하려면 다른 하나의 축으로 분할하는 것이 필요하다. ‘수평 분할형 아키텍처’는 용도가 같은 서버를 늘려나가는 방식. 서버 대수가 늘어나면 한 대가 시스템에 주는 영향력이 낮아져서 안정성이 향상된다. 그리고 전체적인 성능 향상도 가능하다. 수직 분할형과 수평 분할형은 독립적인 관계가 아니고 대부분의 시스템이 두 가지 방식을 함께 채택 단순 수평 분할형 아키텍처 같은 기능을 가진 복수의 시스템으로 단순 분할한다. 수평 분할을 샤딩(Sharding)이나 파티셔닝(Partitioning)이라 부르기도 함 장점 확장성이 향상 독립적으로 운영되므로 서로 영향을 주지 않음 단점 데이터를 동시에(일원화) 이용 불가 데이터를 양쪽 따로 보유하고 있기 때문 업데이트 양쪽을 동시에 해 줘야 함 서버별 처리량에 치우침 발생 가능 이용자 수가 한쪽에 대부분이 몰리는 경우 공유형 아키텍처 공유형에서는 단순 분할형과 달리 일부 계층에서 상호 접속이 이루어짐 장점 확장성이 향상 서로 다른 시스템의 데이터를 참조 가능 단점 독립성이 낮아짐 공유한 계층의 확장성이 낮아짐 지리 분할형 아키텍처 서버를 수직 또는 수평으로 분할한 아키텍처를 조합함으로 목적에 적합한 구성을 만들 수 있다. 업무 연속성 및 시스템 가용성을 높이기 위한 방식으로 지리적으로 분할하는 아키텍처 스탠바이형 아키텍처 물리 서버를 최소 두대 준비하여 한 대 고장나면 대기하고 있는 다른 한대로 옮겨 운영. 이 때 소프트웨어 재시작을 자동으로 하는 구조를 ‘페일오버(Failover)’라고 한다. 한쪽이 계속 놀고 있을 수 있기 때문에 양쪽 서버를 교차 이용하는 경우도 많다. 재해대책형 아키텍처 특정 데이터 사이트에 있는 상용 환경에 고장이 발생하면 다른 사이트에 있는 재해 대책 환경에서 업무 처리를 재개 참고: 그림으로 공부하는 IT 인프라 구조 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "infra architecture study it infra",
    "url": "/study/it%20infra/2022-06-26-IT-Infra-Architecture/"
  },{
    "title": "데이터베이스 - 인덱스(Index)",
    "text": "인덱스(Index)란? 인덱스는 각 데이터의 색인으로 데이터가 저장된 레코드의 주소이다. DBMS도 데이터베이스 테이블의 모든 데이터를 검색해서 원하는 원하는 결과를 가져오려면 시간이 오래 걸린다. 그래서 칼럼의 값과 해당 레코드가 저장된 주소를 키와 값의 쌍으로 인덱스를 만들어 둔다. DBMS의 인덱스는 항상 정렬된 상태를 유지하기 때문에 원하는 값을 탐색하는데는 빠르지만 새로운 값을 추가, 삭제, 수정하는 경우에는 쿼리문 실행속도가 더 느려진다. 즉, 저장 성능을 희생해 읽기 속도를 높이는 것이다. 하지만 모든 컬럼(column)에 인덱스를 생성하게 되면 오히려 저장 성능이 떨어지고 인덱스의 크기가 커져서 역효과가 날 수 있다. Index 자료구조 B+-Tree 인덱스 알고리즘 일반적으로 사용되는 인덱스 알고리즘은 B+-Tree 알고리즘이다. B-tree는 데이터가 항상 정렬된상태로 유지되어 있고 가장 상단의 노드인 ‘루트 노드(Root Node)’, 중간 노드인 ‘브랜치 노드(Branch Node)’, 가장 아래 노드인 ‘리프노드(Leaf Node)’ 로 이루어져 있다. Binary search tree와 비슷하지만, B-tree는 한 노드 당 자식 노드가 2개 이상 가능하다. key값을 이용해 찾고자 하는 데이터를 트리 구조를 이용해 찾는 것이다. Hash 인덱스 알고리즘 칼럼의 값을 해시 값으로 계산해서 인덱싱하는 알고리즘으로 O(1)의 매우 빠른 검색을 지원한다. 하지만 값을 변형해서 인덱싱하므로 값의 일부만으로 검색하고자 할 때는 해시 인덱스를 사용할 수 없다. 왜 B-tree를 쓸까? 시간복잡도 O(1)인 hash 인덱스 알고리즘이 더 좋을거 같은데 왜 쓰지 않을까? SELECT 질의 조건에는 부등호(&lt;&gt;) 연산도 포함되는데 동등 연산(=)에 특화된 hash table을 사용하게 된다면 문제가 발생한다. Index의 성능과 고려해야 할 사항 검색 쿼리의 성능을 향상시키는 INDEX는 항상 좋을까? 모든 컬럼에 INDEX를 생성해두면 더 빨라지지 않을까 생각할 수 있지만 그렇지 않다. INDEX를 생성하게 되면 INSERT, DELETE, UPDATE 쿼리문을 실행할 때 별도의 과정이 추가적으로 발생한다. INSERT의 경우 INDEX에 대한 데이터도 추가해야 하므로 그만큼 성능에 손실이 발생하고, DELETE의 경우 INDEX에 존재하는 값은 삭제하지 않고 사용하지 않는다는 표시로 남게 된다. 즉, row의 수는 그대로인 것이다. 이 작업이 반복되게 되면 실제 데이터는 1만건이고 총 데이터가 10만건인 상황이 발생할 수 있다. UPDATE의 경우는 INSERT, DELETE 문제점을 동시에 수반해 변경 전 데이터는 삭제되지 않고, INSERT로 인한 분리도 발생하게 된다. 이렇게 되면 인덱스를 쓰는 의미가 없다. 그렇다면 인덱스를 어느 컬럼에 사용하는 것이 좋을까? 인덱스는 자주 조회하고 수정 빈도가 낮으며 데이터 중복이 적은 컬럼을 선택하는 것이 좋다. 그러나 한 테이블에 인덱스가 너무 많으면 데이터를 수정할 때 시간이 너무 커지고, 나이(age)나 성별(gender)같은 데이터 중복이 높은 컬럼은 인덱스를 생성하는 것이 비효율적이다. 인덱스로 얻는 이점보다 인덱스 추가 저장공간, 데이터 수정으로 인덱스 수정에 의한 성능 저하가 더 클 수 있기 때문이다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "database index etc",
    "url": "/etc/database/2022-06-24-Index/"
  },{
    "title": "데이터베이스를 사용하는 이유",
    "text": "왜 데이터베이스를 사용하게 됐을까? 데이터베이스가 존재하기 전에는 파일 시스템을 이용하여 데이터를 관리하였다.(현재도 부분적으로 사용) 데이터를 각각의 파일 단위로 저장하며 이러한 일들을 처리하기 위한 독립적인 애플리케이션과 상호 연동이 되어야 한다. 이 때 발생할 수 있는 문제점은 데이터 종속성, 중복성, 데이터 무결성이 있다. 데이터의 종속성(Data Dependency) 데이터의 종속성은 프로그램의 구조가 데이터의 구조에 영향을 받는 것을 의미한다. 즉, 데이터의 구조가 변경되면 프로그램까지 같이 바뀌어야 되기 때문에 개발과 유지 보수가 힘들어진다. 데이터의 중복성(Data Redundancy) 파일 시스템은 프로그램마다 데이터 종속성으로 인해서 공유가 안되는 경우가 많아서 프로그램 마다 같은 데이터를 중복해서 저장하는 경우가 많다. 이는 저장공간 낭비뿐만 아니라 수정시에도 모든 데이터를 수정해야 하는 문제가 있다. 데이터의 무결성(Data Integrity) 같은 데이터를 가진 모든 곳에서 수정이 발생되지 않는 경우, 중복된 데이터가 서로 일치하지 않는 경우가 발생한다. 이렇게 무결성이 깨지게 되면 잘못된 정보가 생성될 수 있기 때문에 2차적인 문제가 발생할 수 있다. 데이터베이스 파일 시스템의 단점을 커버하면서도 다수의 사용자들이 공유할 수 있는 데이터베이스가 등장하게 된다. 데이터베이스의 특징 데이터의 독립성 물리적 독립성: 데이터베이스를 수정하더라도 관련된 응용 프로그램을 수정할 필요가 없다. 논리적 독립성: 데이터베이스는 논리적인 구조로 다양한 응용프로그램의 논리적 요구를 만족시킬 수 있다. 데이터의 무결성 데이터의 유효성 검사를 통해 데이터의 무결성을 보장 데이터의 보안성 인가된 사용자들만 데이터베이스 자원에 접근할 수 있도록 보안 구현 데이터의 일관성 연관된 정보를 논리적인 구조로 관리함으로 써 어떤 하나의 데이터만 변경되었을 때 발생할 수 있는 불일치성을 배제 데이터 중복 최소화 데이터를 통합해서 관리함으로써 파일 시스템의 단점인 데이터 중복 문제를 해결 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "database etc",
    "url": "/etc/database/2022-06-22-database/"
  },{
    "title": "OOP의 5가지 원칙(SOLID)",
    "text": "좋은 OOP(객체지향 프로그래밍)를 설계를 하기 위한 5가지 원칙(SOLID)이 있다. S - 단일 책임 원칙(SRP, Single Responsibility Principle) 객체는 단 하나의 책임만 가져야 한다. 변경이 있을 때 파급 효과가 적으면 단일 책임 원칙을 잘 따르는 것. O - 개방 폐쇄의 원칙(OCP, Open Closed Principle) 기존의 코드를 변경하지 않으면서 기능을 확장할 수 있어야 함. 인터페이스(변하지 않는 것)에서 구체적인 출력 클래스(변하는 것)를 캡슐화해 처리하도록 해야한다. 변해야 하는 것은 쉽게 변할 수 있게 하고, 변하지 않아야 할 것은 변하는 것에 영향을 받지 않게 해야한다. L - 리스코프 치환 원칙(LSP, Liskov Substitution Principle) 프로그램 객체는 프로그램의 정확성을 깨뜨리지 않고 하위 인스턴스로 변경이 가능해야 한다. 다형성에서 하위 클래스는 인터페이스의 규약을 다 지켜야 한다는 것. I - 인터페이스 분리 법칙(ISP, Interface Segregatoin Principle) 특정 클라이언트를 위한 인터페이스 여러개가 범용의 인터페이스 하나보다 낫다. SRP와도 연관되어 있는데 하나의 범용 인터페이스는 여러개의 책임을 수행하게 될 가능성이 높다. 그러므로 단일 책임을 갖는 여러개의 분할 인터페이스로 나누는것이 SRP, ISP 둘다 만족할 수 있다. D - 의존관계 역전 원칙(DIP, Dependency Inversion Principle) 프로그래머는 추상화(인터페이스)에 의존해야지 구현체(클래스)에 의존하면 안된다. 인터페이스에 의존해야 유연하게 구현체를 변경할 수 있다. OCP가 되려면 기본적으로 DIP가 만족되어야 하는데 다형성 만으로는 DIP, OCP를 지킬 수 없고 의존성 주입(DI, Dependency Injection) 이라는 기술이 필요하다. DI에 대해 자세하게 더 알고 싶다면 이 게시글로 가보자. 간단한 정의만 보고는 SOLID가 잘 이해되지 않을 수 있다. 그래서 아래 글에 예를 들어가며 SOLID에 대해 설명해놨으니 참고해보자! 예를 들어가며 SOLID에 대해 알아보자(1) 예를 들어가며 SOLID에 대해 알아보자(2) *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java OOP SOLID etc",
    "url": "/etc/java/2022-06-18-OOP/"
  },{
    "title": "지속 가능한 SW 개발을 위한 코드리뷰 세미나 정리 및 후기",
    "text": "이 글은 우아한 테크 세미나에서 강연을 하신 백명석님의 지속 가능한 SW 개발을 위한 코드 리뷰에 대한 정리 및 후기입니다. 참고 링크: https://www.youtube.com/watch?v=ssDMIcPBqUE 계기 이 세미나를 듣게 된 계기는 최근에 같은 학생들끼리 프로젝트를 하고 있는데 각자 자기가 맡은 부분만 관심을 가지고 개발해 진행하다 보니 서로의 코드에 대한 피드백이나 리뷰는 없고 일방적으로 자기 스타일 대로만 개발코드를 짜고 합치고 반복하다보니 뭔가 빠진 것 같고 이게 맞는건가?? 하는 불안한 생각이 들었다. 분명 같이 하고 있는데 왜 혼자하는 것 같은 공허한 느낌도 들었던 것 같다.. 거기다가 중간에 문제가 생기면 그 문제에 관련된 개발을 한 사람이 다 책임을 지고 고치는데 그 사람만 문제에 대해 관심을 가지고 나머지는 무관심했던 것 같다. 듣고나서 생각해보니깐 정말 외롭고 너무했던것 같은 생각이 들었다ㅜㅜ.. 만들 때 리팩터링과 클린코드, 코드리뷰에 대해 관심은 있었지만 자세하게 알지 못했고 내가 다른 사람의 코드를 판별할 수준이 되나?? 라는 생각을 가지고 있어서 프로젝트를 진행할때 적용을 못했던 것 같다. 하지만 어느 정도 진행을 하면서 개발에 대해 자신감이 조금 생기고 프로젝트가 안정화되어 여유가 조금 생기니 제일 먼저 리팩터링과 코드리뷰를 공부하여 적용하고 싶은 생각이 들었던 것 같다. 아마 이대로 가다가는 나중에 터질 것 같다는 직감이 아니였을까 싶다. 그렇게 세미나를 접하게 되었고, 듣고 나니 내가 가려웠던 부분을 시원하게 긁어주었고 자신감을 얻은 것 같다. 이 글을 어서 마무리하고 정리한 세미나 내용을 프로젝트 팀원들에게 빨리 전해서 같이 공부하고 적용하고 싶다. 자 어서 세미나 내용으로 가보자. 세미나 정리 시작전 이런 말씀을 하셨다. 개발은 나 혼자 성장하는게 아니라 다른 사람까지 성장 시키는 일이다. 자기 혼자 잘할 수 있지만 그 결과가 작다. 하지만 나보다 부족한사람, 더 잘하는 사람, 유사한 사람과 함께 협력해서 한다면 더 큰 결과물을 낼 수 있다고 한다. 그래서 공유를 통해서 협력을 이뤄내고 결과를 극대화 할 수 있는 방법들 중 하나인 코드리뷰를 준비하게 되었다고 하셨다. (그 동안의 나를 반성하게 되었다..) 왜 코드 리뷰를 해야하나? 우리가 살고있는 시대는 현재 소프트웨어에 의해 운영되는 제품과 서비들의 영역이 늘어나고 있고 변동성의 시대로 변화의 속도가 빠르고 다양하게 전개될 것이라고 한다. 또 Global GDP에서 Tech의 비율은 2020년에 5%에서 2030년 10%로 증가할 것으로 보이는데 주목할 것은 나머지 90%. 즉, Digtal Transform은 이제 시작인 것이다. 그렇기 떄문에 비즈니스 성공을 위한 개발 조직의 성능(생산성)이 중요해졌다. 그림을 보면 좋은 디자인은 개발 생산성이 꾸준하게 느는걸 볼 수 있다.(좋은 디자인이 나오려면 코드 리뷰나 어떤 좋은 수단이 필요) SW 공학의 특성 공학 활동의 최종 목적 빌드 할 수 있는 어떤 종류의 재생산 가능한(Reproducible) 문서 SW 공학의 설계와 빌드 설계 = 완전한 소스 코드 SW 빌드 = 컴파일 좋은 설계 ~= 클린코드 SW 엔지니어: 설계를 잘하는 사람 -&gt; 코드를 잘 작성하는 사람 클린 코드의 중요성 SW의 진정한 비용 ~= 유지보수(전체의 80% 이상) 한번 작성한 코드는 10번 이상 읽음. 작성 보다 이해에 10배의 노력 소요 90% 이상의 시간을 어떤 코드를 이해하는데 사용함 목적 주목적: 품질 문제 검수(버그, 장애) 더 나은 코드 품질: 아키텍처 속성 개선을 위한 코드 개선(향후 변경 비용 개선) 학습 및 지식 전달: 코드 해결책 등과 관련된 지식 공유에 기여 공유(주고 받는 학습)를 통한 역량 증대 및 성장 대개의 경우 리뷰어들도 리뷰 과정에서 지식을 얻게 됨(하드스킬, 소프트스킬) 동기 부여(잘하는 사람이 하는걸 보면 대부분 사람은 잘하고 싶은 마음이 생김) 상호 책임감 증대 집단 코드 오너십 및 결속 증대 내가 하고 있는 일에 관심을 가져주는 것 팀에서 일어나는 일 공유. 내 동료는 무엇을 하나? 팀웍 설계 개선 제안(좋은 사례 공유, 부족한 부분에 대해 의견을 물어 볼 수 있음) 개발 문화 개선 듣다가 상호 책임감 증대에서 진짜 공감이 되었던 것 같다.. 처음에도 말했듯이 문제가 생기게 되면 관련된 사람에게만 잘못을 묻고 하는 것은 개개인뿐만 아니라 팀에게도 좋지 않은 것 같다. 팀원이 다같이 코드를 검수하는 시간을 가졌다면 내가 작성한 코드가 아니어도 팀원의 일원으로 책임감이 생기게 될꺼고 그렇게 되면 자동으로 집단 코드 오너십 및 결속이 증대되는 것이다. 이렇게 팀웍이 쌓여서 팀이 한층 더 성장하지 않나 싶다. 코드 리뷰의 절차 저자(Author) 코드 작성, 리뷰 요청 리뷰어 코드를 읽고 머지 가능한가 결정 변경 내역(Cahnge List, PR) 리뷰 시작 전에 작성 저자가 머지를 원하는 소스 코드에 대한 일련의 변경(잘한 것, 아쉬운 것, 눈여겨 볼 것)에 대해 기술 좋은 Pull Request 예 왜 코드 리뷰가 어려운가? 기본적으로 저자는 본인 생각에 멋지다고 생각하는 PR을 보내고 리뷰어는 왜 멋지지 않은지에 대한 장황한 이유를 작성하기 때문에 여기서 갈등이 생기게 된다. 저자나 리뷰어나 내가 좀 틀릴 수 있구나 저 사람이 맞을 수 있구나 이런 마음가짐으로 접근해야지 내가 너보다 훌륭한 개발자야 라는 마음으로 접근하면 서로에게 문제가 된다. 그리고 코드에 대한 비판은 그때 작성한 코드에 대한 거지 자신에 대한 비판이 아니므로 분리해서 생각해야 한다. 코드리뷰는 지식을 공유하는 기회이자 공유를 통해 서로의 지식/경험을 나누며 상호 학습을 통한 역량 증대 수단이다. 그러므로 코드 토의를 개인적 공격으로 받아들이면 물거품이 된다. 생각을 글로 전달하는 것은 매우 어렵다. 음성 톤, 표정의 부재로 오해의 위험성이 크다. 그러므로 피드백을 할때 조심스럽게 표현하는 것이 매우 중요하다! 효율적인 PR 방법 지루한 작업은 컴퓨터로 처리 코드를 읽는 것은 인지적 부담이 되는 고수준의 집중이 요구되는 작업 컴퓨터가 할 수 있는 일에 이런 노력을 낭비하지 말라 심지어 기계가 더 잘 할 수 있는 일에(예를들어 build, test, merge conflict 찾기 등등..) Formatting Tool 공백, 들여쓰기 오류 등 별도의 커밋/PR로 분리. 리뷰 불필요를 기술해서 리뷰를 생략할 수 있도록 스타일 가이드를 통해 스타일 논쟁을 해소 스타일에 대한 논쟁은 리뷰에서 시간 낭비 좋은 스타일 가져다 써라! 아니면 쓰고있는 스타일을 점진적으로 개선 아니면 두개를 하이브리드로 PR을 올릴 때 주석 달기 PR을 저자가 먼저 읽어보고 -&gt; 리뷰어들을 위한 설명을 커멘트로 남겨서 -&gt; 리뷰어들의 시간을 절약할 수 있게 하라 리뷰어에 모두를 포함하라 많은 사람들이 볼 수록 버그를 더 잘 찾아낼 수 있다. 많은 사람들이 본다는 것을 알면 사람들은 대개 더 잘 하려는 경향이 있다.(코드 / PR 작성) 의미 있는 커밋으로 분리 작게 나눠서 커밋을 하면 보는사람이 편함 효율적인 리뷰 방법 리뷰는 즉시 시작 코드 리뷰를 높은 우선순위로 저자는 리뷰 종료 될 때까지 대기함 리뷰를 바로 시작하면 선순환됨 코드를 읽고 피드백을 줄 떄는 시간을 가지고 진행해도 되지만 시작은 바로 해라 이상적으로는 수분 내에 리뷰 라운드의 최대 시간은 하루 우선순위 높은 업무로 1일 내 불가하면 다른 리뷰어 지정 월 1회 이상 재지정을 해야한다면 속도를 줄여서 건강한 개발 관습(Practice)을 유지할 수 있어야 함 남의 것을 리뷰하는 것에 대한 것을 조직내에서 인정해주지 않기 때문에 그거에 대한 따로 시간을 내기 힘든 것이다. 그래서 리뷰를 하는 것에 대한 문제라기 보다는 조직적인 문제라 볼 수 있다. 어떻게든 보장되도록 조직적으로 접근할 필요가 있다. 그리고 Pull Requests와 Pair Programming 중 어느게 더 좋은지 논쟁이 있는데 두 가지는 각각의 트레이드오프가 있다. Pull Requests 같은 경우 지연시간이 있고 Pair Programming 같은 경우는 바로바로 하니깐 지연시간(Latency)이 없다. 근데 처리량(Throughput)의 경우 Pull Requests가 여러명이서 하기 때문에 Pair Programming 보다 좋다고 할 수 있다. 또한 팀의 성향에 따라 달라지는데 팀원들이 좀 내성적이거나 사색적인 경우는 Pull Request가 더 어울리고 팀원이 외향적이거나 친밀한 개인적 상호작용이 있는 경우에는 Pair Programming이 더 좋다고 할 수 있다. 하지만 절대 어느 것이 더 좋다는 답은 없고 아니면 섞어하는 앙상블 방식도 있다. 고수준으로 시작, 저수준으로 내려가라 리뷰 라운드에서 많은 의견을 남길 수록, 저자가 당황할 위험 커짐 하나의 라운드에 20~50개 정도의 의견은 위험의 시작 초기 라운드에서는 고수준 피드백으로 제한 버그, 장애, 성능, 보안 등 고수준의 피드백이 처리된 후에 저수준 이슈를 처리 (선택적인) 설계 개선 예제 코드 제공에 관대해라 저자를 기분 좋게 하기 위한 방법 리뷰 중에 선물 주기(코드 예제) 너무 긴 예제는 관대한 것이 아니라 억압적으로 보임 라운드당 2~3개의 코드 예제로 제한 모든 PR에 예제를 제공하면 저자가 코드를 작성할 수 없다고 무시하는 것 같음 리뷰의 범위를 존중하라 PR에 포함되지 않은 라인은 리뷰 범위가 아님 태그를 활용 [Nit] ‘고치면 좋지만 아니어도 그만’을 의미 그래도 보통 고침 리뷰어는 항상 더 개선할 수 있는 의견을 자유롭게 남길 수 있어야 함 중요치 않다면 “Nit”를 태그로 남겨서 저자가 무시할 수 있도록 할 수 있음 교육적인 목적, 지속적으로 기술을 연마하는 것을 돕는 목적 예) nit: null 대신 Optional을 쓰면 어떨까요? 한두 등급만 코드 레벨을 올리는 것을 목표로 D 등급의 PR을 받으면 저자가 C나 B 등급을 받도록 도와라 (한번에 다 받아들여서 갑자기 A등급이 될 수는 없음) Letter Grade 완전하지는 않아도 충분히 좋은 코드가 되도록 피드백 방법 절대 “너”라고 하지 마라 리뷰의 핵심 “무엇이 코드를 나아지게 하는가” “누가 그런 아이디어(잘못)를 냈는지”가 아님 저자의 방어 유발을 최소화하는 방법으로 피드백 비판의 대상은 코드. 저자가 아님 “너”만 빼라 I Message 대화법: 행동 - 결과 - 감정 (예를들어 너의 어떤 행동이 결과가 이렇게되서 내 기분이 어떻다.) ~하는 것을 제안합니다. ~하는게 어떨까요 ? &lt;- 오픈 커뮤니케이션 물어보면 대답을 한다. 제안 한 것을 안한다고 대답하기도 편하고 건설적인 피드백을 하라 동료들 간의 코드 리뷰 경쟁 유발 시키는 것이 아닌 팀의 생산성을 높이려는 것 코드 리뷰를 자신의 코드에 대한 비판이 아니라 학습의 과정으로 인지하면 전체적으로 프로젝트의 성공에 기여함 건설적인 피드백은 개발자들이 그들의 실수에서 배우고 역량을 증대하도록 동기부여함 건설적인 피드백을 못하겠으면 차라리 아무 말도 안하는 것이 오히려 나음 진정한 칭찬을 해라 대부분의 리뷰어가 잘못된 부분에만 집중하는데 하지만 리뷰는 긍정적 행위 강화를 위한 값진 기회이기도 하다. PR에서 좋은 변경이 있을 때마다 “오 이런 API가 있나요. 정말 유용해요” “정말 좋은 해결책이네요.” 저자가 주니어 혹은 신규 입사자라면 리뷰에 매우 민감하고 방어적일 수 있음 진심어린 칭찬은 리뷰어가 잔인한 감시자가 아니라 도와주려는 팀동료라는 것을 보여서 이런 긴장감을 낮춘다. 피드백은 명령이 아니라 요청으로 표현해라 우리는 일상에서 동료에게 명령하지 않는다 하지만 리뷰에서는 강압적인 명령이 종종 발견되고 한다. ex. 이 클래스를 별도의 파일로 분리하라 -&gt; 이 클래스를 별도의 파일로 분리할 수 있을까요? 의견이 아니라 원칙에 기반하여 피드백하라 저자에게 의견을 줄 때는 “제안하는 변경”과 “변경의 이유”를 모두 설명하라 ex. 이 클래스를 2개로 분리해야 해요 -&gt; 지금 이 클래스는 파일 다운르도와 파싱의 2가지 책임을 가지고 있어요. 다운로더와 파서 2개의 클래스로 분리하여 SRP를 준수하는 것이 어떨까요 ? SW는 과학인 동시에 예술 항상 원칙에 기반하여 정확히 뭐가 잘못 되었는지 언급할 수는 없다. 단지 그냥 보기 싫거나 직관적이지 않을 수 있다 무엇을 할 수 있을지 객관적으로 설명하라 ex. 이 코드는 혼란스럽네요(너?) -&gt; 나는 이 코드를 이해하기 어렵네요 (I Message) 라고 말하게 되면 왜 이해하기 어려울까 고민할 수도 있고 물어 볼수도 있다. 반복적인 패턴에 대해서 피드백을 제한하라 저자의 실수가 동일한 패턴임을 식별 했다면 모든 경우를 언급하지는 말라 동일 패턴에 대해서 2~3개 정도의 예를 언급하라 그 이상은 저자에게 개별 사례가 아니라 패턴에 대해서 수정을 요구하라 교착상태 시 교착 상태를 적극적으로 처리해라 교착 상태로 향하는지 나타내는 표식 토론의 톤이 점차 팽팽해지고 공격적으로 됨 라운드당 커멘트가 줄어들지 않는 경향을 보임 너무 많은 커멘트에 저항이 보임 코드 리뷰의 최악의 결과는 교착상태(Stalemate) 커멘트를 반영하지 않으니 승인 거부 저자는 커멘트 반영을 거부 만나서 애기하라 화상 혹은 만나서 논의(특히 복잡한 리뷰) 텍스트 기반 의사소통은 상대가 인간이라는 것을 잊게 함 인정하거나 Escalate하라 교착상태가 길어지면 관계가 나빠짐 그냥 승인하는 비용(Agree to disagree - 갈등 해결책) 저수준 코드를 무심코 승인하면 SW 품질이 낮아질 수 있음 동료와 너무 다퉈서 함께 일하지 않게 된다면 고수준의 품질을 얻을 기회가 사라짐(적당하면 타협하자) 인정이 불가한 경우 저자에게 논의를 팀장이나 테크 리더에게 Escalation 다른 리뷰어에게 할당 교착상태로 부터 회복 상황을 관리자와 논의하라 휴식을 가져라. 가능하다면 안정될 때까지 PR을 서로 보내지 마라 갈등 해결책을 학습하라 설계 리뷰를 고려하라 코드 리뷰 때 설계 리뷰 때 논의되었어야 할 사항을 논쟁하는가? 설계 리뷰는 있었나? 아주 심각하지 않다면 그냥 인정하고 좋은 관계로 동료와의 협업을 지속해라 Agree to disagree 코드 리뷰를 하는 아주 재밌는 방법 PR을 작성한 사람과 짝 프로그래밍을 하며 어떻게 고치는 게 좋은지 보여주고 다시 Revert한 다음에 PR을 작성한 사람이 스스로 개선할 수 있도록 기회를 준다. 이렇게 하면 짝이 20분 걸린 거를 PR을 작성한 사람은 2시간이 걸릴 수도 있지만 그렇게 해야 스스로 하는 법을 배우고 적용할 수 있게 된다. 이렇게 할 때 제일 중요한 건 결정은 저자가 하는 것이다. 우리는 “완벽한 설계”가 아니라 “할 수 있는 최고의 설계”를 추구하는 것이다. 그렇기 때문에 팀 정신을 유지하기 위해 불완전한 해결책을 받아들여야 한다. 모든 설계 결함이 항상 실제로 문제가 되지는 않는다. 그리고 코드 리뷰의 목적은 비난이 아니라 배움이기 때문에 전혀 불편할 필요가 없다. 저자만 배우는 게 아니라 리뷰어들도 배우는 것이다. 리뷰하려는 코드가 그냥 나쁠 때가 있을 수도 있다. 하지만 저자가 그날 어떤 일이 있었을지 모른다. 그렇기 때문에 비난하기 보다는 “XXX님. 이 코드 샘플을 당신의 이력서에 추가할까요?” 라는 농담을 던질 수 있는 동료가 되도록 하자. 신규 입사자, 경험이 부족한 팀원의 코드를 리뷰하면서 이런 상황을 만났다면 좀 더 안내하는 가르침으로 전환하자. 마무리 이 뒤 세미나에는 Appendix로 코드 리뷰 문화 정착의 어려움/극복 방법, 코드 리뷰 효과, 코드리뷰를 잘 하기 위해 필요한 기술들, 자주 하는 QNA등 유익한 내용들이 더 있다. 더 관심있는 사람들은 https://www.youtube.com/watch?v=ssDMIcPBqUE 링크로 가서 1:22:36초 부터 시청하면 된다. 처음으로 이렇게 긴 개발 관련 세미나를 듣고 정리를 해봤는데 정말 도움이 많이 된 것 같다. 유익할 뿐 아니라 정말 재미있어서 시간이 순식간에 간 것 같다. 왜 이제 알았는지…ㅜ 앞으로 자주 세미나를 찾아보고 들을 것 같다. 상황이 되면 온라인 말고 오프라인 세미나도 참석하고 싶다. 사실 위에 정리한 양이 워낙 많아서 후기보단 정리에 더 가까운 거 같다ㅋㅋㅋ. 저것도 진짜 전해주고 싶은 내용만 골라 정리한 것이라서 직접 세미나를 보는 걸 추천한다! 더 유익하고 자세한 내용이 많다고 자신할 수 있다. 이 글을 적으면서 빨리 프로젝트 팀원들에게 전하고 싶은 급한 마음에 글이 잘 적힌지 모르겠다. 마지막으로 이렇게 좋은 강연을 해주신 백명석님께 감사드리고 매끄러운 진행을 해주신 JADE님께도 감사하다고 전하고 싶다! *오타가 있거나 피드백 주실 부분이 있으면 편하게 말씀해 주세요.",
    "tags": "code-review seminar etc",
    "url": "/etc/seminar/2022-06-13-code-review/"
  },{
    "title": "String vs StringBuilder vs StringBuffer",
    "text": "String, StringBuffer, StringBuilder 모두 문자열을 저장하고, 관리하는 클래스인데 이렇게 여러가지를 만들어놓은 이유는 뭘까 String String과 다른 클래스(StringBuffer, StringBuilder)의 차이점은 immutable(불변)과 mutable(변함)에 있다. String 객체는 한번 생성되면 할당된 메모리 공간이 변하지 않는다. 새로운 값을 할당할 때마다 새로 클래스에 대한 객체가 생성된다. 기존에 생성된 String 객체에 또 따른 문자열을 붙여도(String + String) 기존 문자열에 새로운 문자열을 붙이는 것이 아니라, 새로운 String 객체를 만든 후 저장하기 때문에 Garbage Collector가 호출되기 전까지 생성된 각각의 String 객체들은 Heap에 쌓여 메모리 관리에 치명적이다. String 객체는 이러한 이유로 문자열 연산이 많은 경우, 그 성능이 좋지 않다. 하지만, Immutable한 객체는 간단하게 사용가능하고, 동기화에 대해 신경쓰지 않아도 되기때문에(Thread-safe), 내부 데이터를 자유롭게 공유 가능하다. String result = \"\"; result += \"hello\"; result += \" \"; result += \"world\"; System.out.println(result); //출력값: hello world StringBuffer, StringBuilder StringBuffer와 StringBuilder같은 경우 문자열 연산 등으로 기존 객체의 공간이 부족하게 되는 경우, 새로 만드는 것이 아니라 기존의 버퍼 크기를 늘리며 유연하게 동작한다. StringBuffer는 각 메서드별로 Synchronized Keyword가 존재하여, 멀티스레드 환경에서도 동기화를 지원 반면 StringBuilder는 동기화를 보장하지 않는다. 그렇기때문에 멀티스레드 환경이라면 값 동기화 보장을 위해 StringBuffer을 사용, 단일 스레드 환경이면 StringBuilder을 사용하는 것이 좋다.(단일 스레드 환경에서 StringBuffer를 사용해도 되지만 동기화 관련 처리로 인해 StringBuilder가 더 성능이 좋음) StringBuffer stringBuffer = new StringBuffer(); stringBuffer.append(\"hello\"); stringBuffer.append(\" \"); stringBuffer.append(\"world\"); String result = stringBuffer.toString(); System.out.println(result); //출력값: hello world StringBuilder stringBuilder = new StringBuilder(); stringBuilder.append(\"hello\"); stringBuilder.append(\" \"); stringBuilder.append(\"world\"); String result = stringBuilder.toString(); System.out.println(result); //출력값: hello world 정리 String은 짧은 문자열은 더할 경우 사용 StringBuffer는 스레드에 안전한 프로그램이 필요할 때나, 스레드에 안전한지 모를 경우 사용 StringBuilder는 스레드에 안전한지 여부가 관계 없는 프로그램을 개발할 때 사용 Java 버전별 String 객체 변경사항 JDK1.5버전 이전에서는 문자열연산(+, concat)을 할때에는 조합된 문자열을 새로운 메모리에 할당하여 참조함으로 인해 성능상의 이슈가 있었다. 그러나 JDK1.5 버전 이후에는 컴파일 단계에서 String 객체를 사용하더라도 StringBuilder로 컴파일 되도록 변경 그래서 JDK 1.5이후 버전에서는 String 클래스를 활용해도 StringBuilder와 성능상으로 차이가 없어짐. 하지만 반복 루프를 사용해서 문자열을 더할 때에는 객체를 계속 추가한다는 사실에는 변함이 없기때문에 String 클래스를 쓰는 대신, 스레드와 관련이 있으면 StringBuffer, 없으면 StringBuilder를 사용하는 것을 권장 참고 https://12bme.tistory.com/42 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java String StringBuilder StringBuffer etc",
    "url": "/etc/java/2022-06-12-String-StringBuilder-StringBuffer/"
  },{
    "title": "가비지 컬렉션(Garbage Collection)",
    "text": "가비지 컬렉션(Garbage Collection)을 보기 전에 JVM(Java Virtual Machine)을 한번 보고 오시는걸 추천합니다. Garbage Collection Java의 가비지 컬렉터는 그 동작 방식에 따라 매우 다양한 종류가 있지만 공통적으로 다음 2가지 작업을 수행한다. 힙(heap) 내의 객체 중에서 가비지(garbage)를 찾아낸다. 찾아낸 가비지를 처리해서 힙의 메모리를 회수한다. Minor GC 새로 생성된 대부분의 객체(Instance)는 Eden 영역에에 위치한다. Eden영역에서 GC가 한번 발생한 후 살아남은 객체는 Survivor 영역 중 하나로 이동된다. 이 과정을 반복하다가 계속해서 살아남아 있는 객체는 일정시간 참조되고 있다는 뜻이므로 Old영역으로 이동 Major GC Old영역에 있는 모든 객체들을 검사하여 참조되지 않은 객체들을 한꺼번에 삭제한다. 시간이 오래 걸리고 실행 중 프로세스가 정지된다. 이것을 ‘stop-the-world’라고 하는데 Major GC가 발생하면 GC를 실행하는 스레드를 제외한 나머지 스레드는 모두 작업을 멈추고, 작업을 완료한 이후에야 다시 시작한다. GC와 Reachability Java GC는 객체가 가비지인지 판별하기 위해서 reachability라는 개념을 사용한다. 어떤 객체에 유효한 참조가 있으면 ‘reachable’, 없으면 ‘unreachable’로 구별하고, unreachable 객체를 가비지로 간주해 GC를 수행한다. 한 객체는 여러 다른 객체를 참조하고, 참조된 다른 객체들도 마찬가지로 또 다른 객체들을 참조할 수 있으므로 객체들은 참조 사슬을 이룬다. 이런 상황에서 유효한 참조 여부를 파악하려면 항상 유효한 최초의 참조가 있어야 하는데 이를 객체 참조의 root set이라고 한다. JVM에서 메모리 영역인 런타임 데이터 영역(runtime data area)의 구조는 다음과 같다. 런타임 데이터 영역은 위와 같이 스레드가 차지하는 영역들과, 객체를 생성 및 보관하는 하나의 큰 힙, 클래스 정보가 차지하는 영역인 메서드 영역, 크게 세 부분으로 나눌 수 있다. 힙에 있는 객체들에 대한 참조는 다음 4가지 경우중 하나이다. 힙 내의 다른 객체에 의한 참조 Java Stack, 즉 Java 메서드 실행 시에 사용하는 지역 변수와 파라미터들에 의한 참조 Natiave Stack, 즉 JNI(Java Native Inteface)에 의해 생성된 객체에 대한 참조 메서드 영역의 정적 변수에 의한 참조 여기서 2, 3, 4는 root set으로, reachability를 판가름하는 기준이 된다. root set과 힙 내의 객체를 중심으로 다시 그리면 다음과 같다. 위 그림에서 보듯, root set으로부터 시작한 참조 사슬에 속한 객체들은 reachable 객체이고, 이 참조 사슬과 무관한 객체들이 unreachable 객체로 GC 대상이다. 가장 오른쪽 아래 객체처럼 reachable 객체를 참조하더라도, 다른 reachable 객체가 이 객체를 참조하지 않는다면 이 객체는 unreachable 객체이다. 인스턴스가 가비지 컬렉션의 대상이 되었다고 해서 바로 소멸이 되는 것은 아니다. 빈번한 가비지 컬렉션의 실행은 시스템에 부담이 될 수 있기에 성능에 영향을 미치지 않도록 가비지 컬렉션 실행 타이밍은 별도의 알고리즘을 기반으로 계신이 되며, 이 계산결과를 기반으로 가비지 컬렉션이 수행. 참고 https://asfirstalways.tistory.com/159 https://d2.naver.com/helloworld/329631 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "java Garbage-Collection etc",
    "url": "/etc/java/2022-06-07-Garbage-Collection/"
  },{
    "title": "JVM(Java Virtural Machine)",
    "text": "JVM? JVM이란 Java Virtual Machine으로 자바 가상 머신의 약자이다. JVM의 역활은 자바 애플리케이션을 클래스 로더를 통해 읽어 들여 자바 API와 함께 실행하는 것이다. JVM은 JAVA와 OS 사이의 중개자 역활을 수행하여 JAVA가 OS에 구애받지 않고 재사용을 가능하게 해주고 메모리관리, Garbage Collection 등을 수행한다. 그리고 JVM은 스택기반의 가상머신이다. 한정된 메모리를 효율적으로 사용하기위해 알아야 될 필요가 있다. *Garbage Collection은 메모리 관리 기법 중 하나로 메모리 영역 중에서 필요없게 된 영역을 해제하는 기능 자바 실행과정 프로그램이 실행되면 JVM은 OS로부터 이 프로그램이 필요로 하는 메모리를 할당받는다. JVM은 이 메모리를 용도에 따라 여러 영역으로 나누어 관리한다. 자바 컴파일러(javac)가 자바 소스코드(.java)를 읽어들여 자바 바이트코드(.class)로 변환 Class Loader를 통해 class파일들을 JVM으로 로딩 로딩된 class파일들은 Execution engine을 통해 해석 해석된 바이트코드는 Runtime Data Areas에 배치되어 실질적인 수행이 이루어짐. 이러한 실행과정에서 JVM은 필요에 따라 Thread Synchronization과 GC같은 관리작업 수행 JVM 구성 Class Loader(클래스 로더) JVM내로 클래스를 로드하고 링크를 통해 배치하는 작업을 수행하는 모듈이며 런타임에 동적으로 클래스를 로드 Execution Engine(실행 엔진) 클래스 로더가 JVM내의 Runtime Data Area에 바이트 코드를 배치시키고, 이 때 실행엔진에 의해 실행된다. 자바 바이트 코드는 기계가 바로 수행할 수 있는 언어보다는 비교적 인간이 보기 편한 형태로 기술되어 있기 떄문에 JVM내부 에서 기계가 실행할 수 있는 형태로 변경되는데, 이 때 Interpreter방식과 JIT방식이 있다. Interpreter(인터프리터) 실행 엔진은 자바 바이트 코드를 명령어 단위로 읽어서 실행한다. 한 줄씩 수행하기 떄문에 느리다. JIT(Just - In - Time) 인터프리터 방식의 단점을 보완하기 위해 도입된 JIT 컴파일러. 인터프리터 방식으로 실행하다 적절한 시점에 바이트코드 전체를 컴파일하여 네이티브 코드로 변경하고, 이후에는 네이티브 코드로 직접 실행하는 방식이다. 네이티브 코드는 캐시에 보관하기 때문에 한 번 컴파일된 코드는 빠르게 수행한다. 그러나 JIT 컴파일러가 컴파일하는 과정을 바이트코드를 인터프리팅하는 과정보다 오래걸리므로 한 번만 실행되는 코드라면 인터프리팅하는 것이 유리하다. Garbage Collector JVM은 Garbage Collector를 통해 메모리관리 기능을 자동으로 수행. 사용되지 않는 객체를 해제하는 방식으로 메모리를 자동 관리한다. Runtime Data Area 프로그램을 수행하기 위해 OS에서 할당받은 메모리 공간 PC Register Thread가 시작될 때 생성되며, Thread가 어떤 부분을 어떤 명령으로 실행해야할 지에 대한 기록을 하는 부분으로 현재 수행중인 JVM 명령의 주소를 갖는다 JVM Stack 프로그램 실행과정에서 임시로 할당되었다가 메소드를 빠져나가면 바로 소멸되는 특성의 데이터를 저장하기 위한 영역. 각종 형태의 변수나 임시 데이터, 스레드나 메소드의 정보를 저장한다. 메소드 호출 시마다 각각의 스택프레임(그 메서드만을 위한 공간)이 생성되고 끝나면 프레임 별로 삭제를 한다. Native Method Stack 자바 프로그램이 컴파일되어 생성되는 바이트 코드가 아닌 실제 실행할 수 있는 기계어로 작성된 프로그램을 실행시키는 영역. Java Native Interface를 통해 바이트 코드로 전환되어 저장하게 된다. Method Area (= Class area, Static area) 클래스 정보를 처음 메모리 공간에 올릴 때 초기화되는 대상을 저장하기 위한 메모리 공간. 이 공간에는 Runtime Constant Pool이라는 별도의 관리 영역도 함께 존재한다. 이는 상수 자료형을 저장하여 참조하고 중복을 막는 역활을 수행한다. 올라가는 정보의 종류 Field Information: 멤버변수의 이름, 데이터 타입, 접근 제어자에 대한 정보 Method Information: 메소드의 이름, 리턴타입, 매개변수, 접근 제어자에 대한 정보 Type Information: class인지 interface인지 여부, 전체 이름, super class의 전체 이름 Method Area는 클래스 데이터를 위한 공간이라면 Heap영역이 객체를 위한 공간. Heap과 마찬가지로 GC의 관리 대상에 포함된다. Heap(힙 영역) 객체를 저장하는 가상 메모리 공간. new 연산자로 생성된 객체와 배열을 저장한다. New/Young Generation 자바 객체가 생성되자마자 저장되고, 생긴지 얼마 안되는 객체가 저장되는 공간. 시간이 지나 우선순위가 낮아지면 Old영역으로 옮겨진다. Tenured(Old) Generation Young Generation 영역에서 저장되었던 객체 중에서 오래된 객체가 이동되어 저장되는 영역 Permanent Generation Permanent Generation(Java 7이전): 생성된 객체들의 정보의 주소값이 저장되는 공간. 클래스 로더에 의해 로드되는 클래스, 메소드 등에 대한 메타 정보가 저장되는 영역으로 JVM에 의해 사용된다. Reflection을 사용하여 동적으로 클래스가 로딩되는 경우에 사용된다. Reflection 기능을 자주 사용하는 Spring Framework를 이용할 경우 이 영역에 대한 고려가 필요하다. 런타임시 사이즈를 조절할 수 없다. Metaspace(Java 8 이후): Permanent Generation이 Metaspace로 변경 되었다. 기능은 비슷하지만, 주요 차이점으로 동적으로 사이즈를 바꿀 수 있다. 참고 https://asfirstalways.tistory.com/158 https://hoonmaro.tistory.com/19 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JVM java-virtual-machine etc java",
    "url": "/etc/java/2022-06-04-JVM/"
  },{
    "title": "HTTP Header-2(HTTP 헤더)",
    "text": "HTTP header에 대한 매우 상세한 내용으로 이전 글 HTTP Header-1과 이번 글 HTTP Header-2로 나뉘어져 있습니다. 이번글은 HTTP 캐시와 조건부 요청에 관한 글입니다. 캐시가 없을 때 데이터가 변경되지 않아도 계속 네트워크를 통해서 데이터를 다운로드 받아야 한다. 인터넷 네트워크는 매우 느리고 비싸다. 브라우저 로딩 속도가 느리다. 느린 사용자 경험 캐시 적용 캐시 덕분에 캐시 가능 시간동안 네트워크를 사용하지 않아도 된다. 비싼 네트워크 사용량을 줄일 수 있다. 브라우저 로딩 속도가 매우 빠르다. 빠른 사용자 경험 캐시 시간 초과 캐시 유효 시간이 초과하면, 서버를 통해 데이터를 다시 조회하고, 캐시를 갱신 이때 다시 네트워크 다운로드가 발생 검증 헤더와 조건부 요청1 캐시 만료후에도 서버에서 데이터를 변경하지 않았다면 데이터를 전송하는 대신에 저장해 두었던 캐시를 재사용 할 수 있다. 단 클라이언트의 데이터와 서버의 데이터가 같다는 사실을 확인할 수 있는 방법이 필요. 검증 헤더 추가 아래처럼 Last-Modified 헤더를 추가한다. 이후에 캐시 시간이 초과되고 재요청 했을 때 데이터 최종 수정일이 그대로일 경우에는 아래처럼 바디는 비우고 304 Not Modified + 헤더 메타 정보만 응답한다. 정리 캐시 유효 시간이 초과해도, 서버의 데이터가 갱신되지 않으면 304 Not Modified + 헤더 메타 정보만 응답(바디 X) 클라이언트는 서버가 보낸 응답 헤더 정보로 캐시의 메타 정보를 개신 클라이언트는 캐시에 저장되어 있는 데이터 재활용 결과적으로 네트워크 다운로드가 발생하지만 용량이 적은 헤더 정보만 다운로드 검증 헤더와 조건부 요청2 검증 헤더 캐시 데이터와 서버 데이터가 같은지 검증하는 데이터 Last-Modified, ETag 조건부 요청 헤더 검증 헤더로 조건에 따른 분기 If-Modified-Since: Last-Modified 사용 If-None-Match: ETag 사용 조건이 만족하면 200 OK 조건이 만족하지 않으면 304 Not Modified 예시 If-Modified-Since: 이후에 데이터가 수정되었으면? 데이터 미변경 예시 캐시: 2020년 11월 10일 10:00:00 vs 서버: 2020년 11월 10일 10:00:00 304 Not Modified, 헤더 데이터만 전송(BODY 미포함) 전송 용량 0.1M (헤더 0.1M) 데이터 변경 예시 캐시: 2020년 11월 10일 10:00:00 vs 서버: 2020년 11월 10일 11:00:00 200 OK, 모든 데이터 전송(BODY 포함) 전송 용량 1.1M (헤더 0.1M, 바디 1.0M) Last-Modified, If-Modified-Since 단점 1초 미만(0.x초) 단위로 캐시 조정이 불가능 날짜 기반의 로직 사용 데이터를 수정해서 날짜가 다르지만, 같은 데이터를 수정해서 데이터 결과가 똑같은 경우 서버에서 별도의 캐시 로직을 관리하고 싶은 경우 ETag, If-None-Match ETag(Entity Tag) 캐시용 데이터에 임의의 고유한 버전 이름을 달아둠 예) ETag: “v1.0”, ETag: “a2jiodwjekjl3” 데이터가 변경되면 이 이름을 바꾸어서 변경(Hash를 다시 생성) 예) ETag: “aaaaa” -&gt; ETag: “bbbbb” 진짜 단순하게 ETag만 보내서 같으면 유지, 다르면 다시 받기 ETag, If-None-Match 정리 진짜 단순하게 ETag만 서버에 보내서 같으면 유지, 다르면 다시 받기 캐시 제어 로직을 서버에서 완전히 관리 클라이언트는 단순히 이 값을 서버에 제공 캐시와 조건부 요청 헤더 캐시 제어 헤더 Cache-Control: 캐시 제어 Pragma: 캐시 제어(하위 호환) Expires: 캐시 유효 기간(하위 호환) Cache-Control 캐시 지시어(directives) Cache-Control: max-age 캐시 유효 시간, 초 단위 Cache-Control: no-cache 데이터는 캐시해도 되지만, 항상 원(origin) 서버에 검증하고 사용 Cache-Control: no-store 데이터에 민감한 정보가 있으므로 저장하면 안됨(메모리에서 사용하고 최대한 빨리 삭제) Pragma 캐시 제어(하위 호환) Pragma: no-cache HTTP 1.0 하위 호환 지금은 거의 사용 안 하지만 하위 호환 때문에 사용하기도 함. Expires 캐시 만료일 지정(하위 호환) expires: Mon, 01 Jan 1990 00:00:00 GMT 캐시 만료일을 정확한 날짜로 지정 HTTP 1.0 부터 사용 지금은 더 유연한 Cache-Control: max-age 권장 Cache-Control: max-age와 함께 사용하면 Expires는 무시 프록시 캐시 원서버 직접 접근 origin 서버 클라이언트마다 원서버에서 받는 경우 멀기 때문에 오래 걸린다. 프록시 캐시 도입 하지만 중간 서버를 두게 되면 짧게 받아올 수 있음 Cache-Control 캐시 지시어(directives) - 기타 Cache-Control: public 응답이 public 캐시에 저장되어도 됨 Cache-Control: private 응답이 해당 사용자만을 위한 것임, private 캐시에 저장해야 함(기본값) Cache-Control: s-maxage 프록시 캐시에만 적용되는 max-age Age: 60 (HTTP 헤더) 오리진 서버에서 응답 후 프록시 캐시 내에 머문 시간(초) 캐시 무효화 Cache-Control 확실한 캐시 무효화 응답 Cache-Control: no-cache, no-store, must-revalidate Pragma: no-cache HTTP 1.0 하위 호환 캐시 지시어(directives) - 확실한 캐시 무효화 Cache-Control: no-cache 데이터는 캐시해도 되지만, 항상 원 서버에 검증하고 사용 Cache-Control: no-store 데이터에 민감한 정보가 있으므로 저장하면 안됨 (메모리에서 사용하고 최대한 빨리 삭제) Cache-Control: must-revalidate 캐시 만료후 최초 조회시 원 서버에 검증 원 서버 접근 실패시 반드시 오류가 발생해야함 - 504(Gateway Timeout) must-revalidate는 캐시 유효 시간이라면 캐시를 사용함 Pragma: no-cache HTTP 1.0 하위 호환 no-cache vs must-revalidate no-cache 같은 경우는 원 서버에 접근할 수 없는 경우 캐시 서버 설정에 따라 캐시 데이터를 반환할 수도 있다. 하지만 must-revalidate 같은 경우 원 서버에 접근할 수 없는 경우 항상 오류 발생 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP http-header 캐시 study http web",
    "url": "/study/http%20web/2022-05-29-http-header2/"
  },{
    "title": "인터페이스 vs 추상 클래스",
    "text": "추상 메서드(Abstract Method)와 추상 클래스(Abstract Class) 추상 메서드(Abstract Method) 코드가 구현되지 않은 메서드 public abstract String getName(); // 추상 메서드 public abstract String getName() { return \"name\" } //추상 메서드 X, 오류 발생 추상 클래스(Abstract Class) 개념: abstract 키워드로 선언된 클래스 최소 한개의 추상 메서드를 포함하는 경우 반드시 추상 클래스로 선언 추상 메서드가 하나도 없는 경우라도 추상 클래스로 선언 가능 추상 클래스의 구현 서브 클래스에서 슈퍼 클래스의 모든 추상 메서드를 오버라이딩하여 실행가능한 코드로 구현 추상 클래스의 목적 객체(인스턴스)를 생성하기 위함이 아니라 상속을 위한 부모 클래스로 활용하기 위한 것 여러 클래스들의 공통된 부분을 추상화하여 상속받는 클래스에게 구현을 강제화하기 위한 것(메서드의 동작을 구현하는 자식의 책임을 위임) 즉, 추상 클래스의 추상 메서드를 자식 클래스가 구체화하여 그 기능을 확장하는데 목적 // 최소 한개의 추상 메서드 포함 abstract class Appliance { a() {...} void b() {...} abstract public void c(); //추상 메서드 } // 추상 메서드 하나도 없는 경우 abstract class Appliace { a() {...} void b() {...} } // 추상 클래스 구현 class TV extend Appliance { public void c() { System.out.println(\"TV\")} // 추상 메서드 (오버라이딩) void d() { System.out.println(\"티비\")} } 인터페이스(Interface) 개념: 추상 메서드와 상수만을 포함하며, interface 키워드를 사용하여 선언 인터페이스의 구현 인터페이스를 상속받고, 추상 메서드로 작성한 클래스를 모두 구현 implements 키워드를 사용하여 구현 인터페이스의 목적 상속받을 서브 클래스에게 구현할 모든 메서드들의 원형을 모두 알려주어, 클래스가 자신의 목적에 맞게 메서드를 구현하도록 한다. 구현 객체의 같은 동작을 보장하기 위한 목적이 있다. 서로 관련이 없는 클래스에서 공통적으로 사용하는 방식이 필요하지만 기능을 각각 구현할 필요가 있는 경우에 사용 인터페이스의 특징 인터페이스는 상수 필드와 추상 메서드만으로 구성된다. 모든 메서드는 추상 메서드로서, abstract public 속성이며 생략 가능 상수는 public static final 속성이며, 생략하여 선언 가능 인터페이스를 상속받아 새로운 인터페이스를 만들 수 있다. // 인터페이스 interface A { int a = 5; // 상수 필드(public static final int a = 5;과 동일) void b(); // 추상 메서드 (abstract public void b();과 동일) absract public void c(); // 추상 메서드 } // 인터페이스 구현 class Aimpl implements A { public void b() {...} public void c() {...} public int d() {...} // 추가적으로 다른 메서드도 작성 가능 } 추상 클래스 VS 인터페이스 공통점 인스턴스(객체)는 생성할 수 없다 자식클래스가 메서드의 구체적인 동작을 구현하도록 책임을 위임 차이점 서로 다른 목적 추상 클래스는 추상 메서드를 자식 클래스가 구체화하여 그 기능을 확장하는데 목적(상속을 위한 부모 클래스) Appliance(추상 클래스) - TV, Refrigerator 인터페이스는 서로 관련이 없는 클래스에서 공통적으로 사용하는 방식이 필요하지만 기능을 각각 구현할 필요가 있는 경우에 사용(구현 객체의 같은 동작을 보장) Flyable(인터페이스) - Plane, Bird 추상 클래스는 클래스이지만 인터페이스는 클래스가 아니다. 추상 클래스는 단일 상속이지만 인터페이스는 다중 상속이 가능 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "interface abstract-class abstract-method etc java",
    "url": "/etc/java/2022-05-25-interface-vs-abstract/"
  },{
    "title": "객체지향 프로그래밍 vs 절차지향 프로그래밍",
    "text": "절차지향 프로그래밍 실행하고자 하는 절차를 정하고, 이 절차대로 프로그래밍하는 방법 목적을 달성하기 위한 일의 흐름에 중점을 둔다. 객체지향 프로그래밍 실세상의 물체를 객체로 표현하고, 이들 사이의 관계, 상호 작용을 프로그램으로 나타냄 객체를 추출하고 객체들의 관계를 결정하고 이들의 상호 작용에 필요한 함수, 변수를 설계 및 구현 객체지향의 핵심은 연관되어 있는 변수와 메서드를 하나의 그룹으로 묶는것 하나의 클래스를 바탕으로 서로 다른 상태를 가진 인스턴스를 만들면 서로 다른 행동을 하게 된다. 즉, 하나의 클래스가 여러 개의 인스턴스가 될 수 있다는 점이 객체 지향이 제공하는 가장 기본적인 재활용성이라고 할 수 있다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "procedure-oriented-programming object-oriented-programming etc java",
    "url": "/etc/java/2022-05-23-object-vs-procedure/"
  },{
    "title": "클래스 vs 객체 vs 인스턴스",
    "text": "클래스(Class) 객체를 만들어 내기 위한 설계도 혹은 틀 연관되어 있는 변수와 메서드의 집합 // 클래스 public class Animal { ... } 객체(Object) 소프트웨어 세계에 구현할 대상 클래스에 선언된 모양 그대로 생성된 실체 ‘클래스의 인스턴스’라고도 부른다. 객체는 모든 인스턴스를 대표햐는 포괄적인 의미를 갖는다. public class Main { public static void main(String[] args) { Animal cat, dog; // '객체' } } 인스턴스(Instance) 설계도를 바탕으로 소프트웨어 세계에 구현된 구체적인 실체 즉, 객체를 소프트웨어세계에 실체화 하면 그것을 ‘인스턴스’라고 부름 실체화된 인스턴스는 메모리에 할당된다. 인스턴스는 객체에 포함된다고 볼 수 있다. public class Main { public static void main(String[] args) { Animal cat, dog; // 인스턴스화 cat = new Animal(); dog = new Animal(); } } 클래스 VS 객체 클래스는 ‘설계도’, 객체는 ‘설계도로 구현한 모든 대상’을 의미 객체 VS 인스턴스 클래스의 타입으로 선언되었을 때 객체라고 부르고, 그 객체가 메모리에 할당되어 실제 사용될 때 인스턴스라고 부른다. 객체는 현실 세계에 가깝고, 인스턴스는 소프트웨어 세계에 가깝다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "class object instance etc java",
    "url": "/etc/java/2022-05-23-class-vs-object-vs-instance/"
  },{
    "title": "HTTP Header-1(HTTP 헤더)",
    "text": "HTTP header에 대한 매우 상세한 내용으로 이번글 HTTP Header-1과 다음 글 HTTP Header-2로 나뉘어져 있습니다. 이번 글은 HTTP 일반 헤더에 관한 글입니다. HTTP 헤더 header-field = field-name: field-value field-name은 대소문자 구분 없음 HTTP 전송에 필요한 모든 부가정보 예) 메시지 바디의 내용, 메시지 바디의 크기, 압축, 인증, 요청 클라이언트, 서버 정보, 캐시 관리 정보… 필요시 임의의 헤더 추가 가능 HTTP BODY message body - RFC7230(최신) 메시지 본문(message body)을 통해 표현 데이터 전달 메시지 본문 = 페이로드(payload) 표현은 요청이나 응답에서 전달할 실제 데이터 표현 헤더는 표현 데이터를 해석할 수 있는 정보 제공 데이터 유형(html, json), 데이터 길이, 압축 정보 등 표현 Content-Type: 표현 데이터의 형식 Content-Encoding: 표현 데이터의 압축 방식 Content-Language: 표현 데이터의 자연 언어 Content-Length: 표현 데이터의 길이 Content-Type 표현 데이터의 형식 설명 미디어 타입, 문자 인코딩 예) text/html; charset-utf8 application/json image/png Content-Encoding 표현 데이터 인코딩 표면 데이터를 압축하기 위해 사용 데이터를 전달하는 곳에서 압축 후 인코딩 헤더 추가 데이터를 읽는 쪽에서 인코딩 헤더의 정보로 압축 해제 예) gzip deflate identity Content-Language 표현 데이터의 자연 언어 표현 데이터의 자연 언어를 표현 예) ko en en-US Content-Length 표현 데이터의 길이 바이트 단위 Transfer-Encoding(전송 코딩)을 사용하면 Content-Length를 사용하면 안됨 협상(콘텐츠 네고시에이션) 클라이언트가 선호하는 표현 요청 Accept: 클라이언트가 선호하는 미디어 타입 전달 Accept-Charset: 클라이언트가 선호하는 문자 인코딩 Accept-Encoding: 클라이언트가 선호하는 압축 인코딩 Accpet-Langugage: 클라이언트가 선호하는 자연 언어 협상 헤더는 요청 시에만 사용 Accept-Language 적용 협상과 우선순위1 Quality Values(q) 값 사용 0~1, 클수록 높은 우선순위 생략하면 1 Accept-Language: ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7 ko-KR;q=1 (q생략) ko;q=0.9 en-US;q=0.8 en:q=0.7 협상과 우선순위2 구체적인 것이 우선한다. Accept: text/, text/plain, text/plain;format=flowed, */ text/plain;format=flowed text/plain text/* / 협상과 우선순위3 구체적인 것을 기준으로 미디어 타입을 맞춘다. Accept: text/;q=0.3, text/html;q=0.7, text/html;level=1, text/html;level=2;q=0.4, */;q=0.5 |Media Type|Quality| |—|:—:| |text/html;level=1|1| |text/html|0.7| |text/plain|0.3| |image/jpeg|0.5| |text/html;level=2|0.4| |text/html;level=3|0.7| 전송 방식 단순 전송 Content-Length 압축 전송 Content-Encoding 분할 전송 Transfer-Encoding 범위 전송 Range, Content-Range 일반 정보 From 유저 에이전트의 이메일 정보 일반적으로 잘 사용되지 않음 검색 엔진 같은 곳에서, 주로 사용 요청에서 사용 Referer 이전 웹 페이지 주소 현재 요청된 페이지의 이전 웹 페이지 주소 A -&gt; B로 이동하는 경우 B를 요청할 때 Referer: A를 포함해서 요청 Referer를 사용해서 유입 경로 분석 가능 요청에서 사용 User-Agent 유저 에이전트 애플리케이션 정보 user-agent :Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36 클라이언트의 애플리케이션 정보(웹 브라우저 정보, 등등) 통계 정보 어떤 종류의 브라우저에서 장애가 발생하는지 파악 가능 요청에서 사용 Server 요청을 처리하는 ORIGIN 서버의 소프트웨어 정보 Server: Apache/2.2.22 (Debian) server: nginx 응답에서 사용 Date 메시지가 발생한 날짜와 시간 Date: Tue, 15 Nov 1994 08:12:31 GMT 응답에서 사용 특별한 정보 Host 요청한 호스트 정보(도메인) 요청에서 사용 필수 하나의 서버가 여러 도메인을 처리해야 할 때 하나의 IP 주소에 여러 도메인이 적용되어 있을 때 Location 페이지 리다이렉션 웹 브라우저는 3xx 응답의 결과에 Location 헤더가 있으면, Location 위치로 자동 이동(리다이렉트) 201 (Created): Location 값은 요청에 의해 생성된 리소스 URI 3Xx (Redirection): Location 값은 요청을 자동으로 리다이렉션하기 위한 대상 리소스를 가리킴 Allow 허용 가능한 HTTP 메서드 405 (Method Not Allowed) 에서 응답에 포함해야함 Allow: GET, HEAD, PUT Retry-After 유저 에이전트가 다음 요청을 하기까지 기다려야 하는 시간 503(Service Unavailable): 서비스가 언제까지 불능인지 알려줄 수 있음 Retry-After: Fri, 31 Dec 1999 23:59:59 GMT (날짜 표기) Retry-After: 120 (초단위 표기) 인증 Authorization 클라이언트 인증 정보를 서버에 전달 Authorization: Basic xxxxxxxxxxxxx WWW-Authenticate 리소스 접근시 필요한 인증 방법 정의 401 Unauthorized 응답과 함게 사용 WWW-Authenticate: Newauth realm=”apps”, type=1, title=”Login to \"apps\"”, Basic realm=”simple” 쿠키 Set-Cookie: 서버에서 클라이언트로 쿠키 전달(응답) Cookie: 클라이언트가 서버에서 받은 쿠키를 저장하고, HTTP 요청시 서버로 전달 사용처 사용자 로그인 세션 관리 광고 정보 트래킹 쿠키 정보는 항상 서버에 전송됨 네트워크 트래픽 추가 유발 최소한의 정보만 사용(세션 id, 인증 토큰) 서버에 전송하지 않고, 웹 브라우저 내부에 데이터를 저장하고 싶으면 웹스토리지 (localStorage, sessionStorage) 사용 보안에 민감한 데이터는 저장하면 안됨(주민번호, 신용카드 번호 등) 쿠키 - 생명주기 Expires, max-age Set-Cookie: expires=Sat, 26-Dec-2020 04:39:21 GMT 만료일이 되면 쿠키 삭제 Set-Cookie: max-age= 3600 (3600초) 0이나 음수를 지정하면 쿠키 삭제 세션 쿠키: 만료 날짜를 생략하면 브라우저 종료시 까지만 유지 영속 쿠키: 만료 날짜를 입력하면 해당 날짜까지 유지 쿠키 - 도메인 Domain 예)domain=exmaple.org 명시: 명시한 문서 기준 도메인 + 서브 도메인 포함 domain=example.org를 지정해서 쿠키 생성 example.org는 물론이고 dev.example.org도 쿠키 접근 생략: 현재 문서 기준 도메인만 적용 example.org에서 쿠키를 생헝하고 domain 지정을 생략 example.org 에서만 쿠키 접근 dev.example.org는 쿠키 미접근 쿠키 - 경로 Path 예) path=/post 이 경로를 포함한 하위 경로 페이지만 쿠키 접근 일반적으로 path=/ 루트로 지정 예) path=/post 지정 /post -&gt; 가능 /post/1 -&gt; 가능 /post/1/2 -&gt; 가능 /test -&gt; 불가능 쿠키 - 보안 Secure, HttpOnly, SameSite Secure 쿠키는 http, https를 구분하지 않고 전송 Secure를 적용하면 https인 경우에만 전송 HttpOnly XSS 공격 방지 자바스크립트에서 접근 불가(document.cookie) HTTP 전송에만 사용 SameSite XSRF 공격 방지 요청 도메인과 쿠키에 설정된 도메인이 같은 경우만 쿠키 전송 글이 길어지는 관계로 여기서 끊고 다음 글 HTTP Header-2에 캐시와 조건부 요청에 관하여 이어서 작성하겠습니다. 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP http-header study http web",
    "url": "/study/http%20web/2022-05-21-http-header1/"
  },{
    "title": "트리(Tree)",
    "text": "Tree 트리는 스택이나 큐와 같은 선형 구조가 아닌 계층적 관계를 표현하는 비선형 자료구조이다. 트리의 특징 그래프의 한 종류. ‘최소 연결 트리’라고도 불림. 사이클 불가능 노드가 N개인 트리는 항상 N-1개의 간선을 가진다. 한 개의 루트 노드만이 존재하며 모든 자식 노드는 한 개의 부모 노드만을 가진다. 트리 구성 요소 Node (노드): 트리를 구성하고 있는 각각의 요소를 의미 Edge (간선): 노드와 노드를 연결하는 선 Root Node (루트 노드): 최상위에 있는 노드 Terminal Node(= leaf Node, 단말 노드): 하위에 다른 노드가 연결되어 있지 않은 노드 Internal Node (내부노드, 비단말 노드): 단말 노드를 제외한 모든 노드 Binary Tree (이진 트리) 루트 노드를 중심으로 두 개의 서브 트리로 나뉘어 진다. 또한 나뉘어진 두 서브 트리도 계속 모두 이진 트리. 트리에서는 각 층별로 숫자를 매겨서 이를 트리의 Level(레벨)이라고 한다. 레벨의 값은 0 부터 시작해 루트 노드의 레벨은 0이다. 그리고 트리의 최고 레벨을 가리켜 해당 트리의 height(높이) 라고 한다. Perfect Binary Tree (포화 이진 트리) 모든 레벨이 꽉 찬 이진 트리 Complete Binary Tree (완전 이진 트리) 위에서 아래로, 왼쪽에서 오른쪽으로 순서대로 채워진 이진 트리 Full Binary Tree (정 이진 트리) 모든 노드가 0개 혹은 2개의 자식노드만을 갖는 이진 트리 BST (Binary Search Tree) 이진트리의 일종으로 효율적인 탐색을 위한 저장방법. 단 이진 탐색 트리에는 데이터를 저장하는 규칙이 있다. 이진 탐색 트리의 노드에 저장된 키는 유일 부모의 키가 왼쪽 자식 노드의 키보다 크다. 부모의 키가 오른쪽 자식 노드의 키보다 작다. 왼쪽과 오른쪽 서브트리도 이진 탐색 트리 이진 탐색 트리의 탐색 연산은 O(log n)의 시간 복잡도를 갖는다. 최악의 경우 한 쪽으로 만 노드가 추가되는 경우가 발생해 Skewed Tree(편향 트리)가 되면 시간 복잡도가 O(n)이 된다. 이를 해결하기 위해 AVL Tree, Red-Black Tree 같은 Rebalncing 기법이 있다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Tree Binary-Tree Binary-Search-Tree etc data structure, algorithm",
    "url": "/etc/data%20structure,%20algorithm/2022-05-18-Tree/"
  },{
    "title": "해시 테이블(Hash Table)",
    "text": "Hash Table Hash는 내부적으로 배열을 사용하여 저장하는 Key와 Value로 이루어진 자료구조이다. Key는 고유한 값인데 길이가 다양하기 때문에 그대로 저장하면 다양한 길이만큼 저장소 구성이 필요하다. 그래서 hash function을 이용해 Hash로 변경 후 저장한다. Hash Function은 Key를 Hash로 바꿔주는 역활을 하는데 해시 충돌(서로 다른 Key가 같은 Hash가 되는 경우)이 발생할 확률을 최대한 줄이는 함수를 만드는 것이 중요하다. Hash는 hash function의 결과로 저장소에서 Value와 매칭되어 저장된다. Value는 저장소에 최종적으로 저장되는 값으로 키와 매칭되어 저장, 삭제, 접근, 검색이 가능하다. Hash Function 그렇다면 좋은 hash function이란 무엇일까? hash function을 무조건 1:1로 만드는 것보다 충돌을 최소화하는 방향으로 설계하고 발생하는 충돌에 대비해 어떻게 대응할 것인가 가 더 중요하다. 1:1 대응이 되도록하는건 array와 다를바가 없고 메모리낭비가 심하다. 충돌이 많아질 수록 탐색에 필요한 시간이 O(1)에서 O(n)에 가까워진다. 그러므로 좋은 hash function을 선택하는 것은 중요하다. Resolve Conflict 1. Open Addressing 해시 충돌이 발생하면 다른 해시 버킷에 해당 자료를 삽입하는 방식이다. 공개 주소 방식이라고도 불리는 이 알고리즘은 충돌이 발생하면 데이터를 저장할 장소를 찾는다. 최악의 경우 비어있는 버킷을 찾지 못하고 탐색을 시작한 위치까지 되돌아 올 수 있다. 이 과정에서도 여러 방법들이 있다. Linear probing: 순차적으로 탐색하여 비어있는 버킷을 찾을 때까지 진행 Quadratic probing: 2차 함수를 이용해 탐색할 위치를 찾는다. Double hasing probing: 첫번째 해시 함수에서 충돌이 발생하면 2차 해시 함수를 이용해 새로운 주소를 할당. 2. Separate Chaining 일반적으로 Open Addressing의 경우 채운 밀도가 높아질수록 Worst Case 발생 빈도가 더 높아지기 때문에 Separate Chaining 보다 느리다. 반면 Seperate Chaining의 경우 해시 충돌이 잘 발생하지 않도록 보조 해시 함수를 통해 조정할 수 있다면 Worst Case에 가까워 지는 빈도를 줄일 수 있다. Java 7 에서는 Separate Chaining을 사용하여 HashMap을 구현했다. 데이터의 개수에 따라 두 가지 구현 방식이 존재한다. 연결 리스트를 사용하는 방식 (Linked List) 각각의 버킷들을 연결리스트로 만들어 충돌이 발생하면 해당 bucket의 list에 추가 데이터의 개수가 6개 이하일 때 사용 Tree를 사용하는 방식 (Red-Black-Tree) 연결리스트 대신 트리를 사용하는 방식. 데이터의 개수가 8개 이상일 때 사용 트리는 기본적으로 메모리 사용량이 많기 때문에 데이터 개수가 적을 때는 링크드 리스트나 트리와의 성능 차이가 거의 없기 때문에 링크드 리스트를 사용한다. Open Addressing vs Separate Chaining 일단 두 방식 모두 Worst Case가 O(M)으로 같다. 하지만 Open Addressing은 연속된 공간에 저장하기 때문에 Separate Chaining에 비해 캐시 효율이 높다. 따라서 데이터의 개수가 적다면 Open Addressing이 더 성능이 좋을 수 있다. 하지만 배열의 크기가 커질수록 캐시 효율이라는 Open Addressing의 장점은 사라진다. 또한 Operate Addressing의 경우 버킷을 계속해서 사용하기 때문에 Seperate Chaining 방식이 테이블의 확장을 더 늦출 수 있다. 3. Resizing (동적 확장) 해시 버킷의 개수가 적다면 메모리 사용을 아낄 수 있겟지만 해시 충돌로 인해 성능 상 손실이 발생한다. 그래서 저장 공간이 75%가 채워지면 저장 공간을 두 배로 늘린다. Hash Table 장점 적은 리소스로 많은 데이터를 효율적으로 관리 가능 ex) HDD, Cloud에 있는 많은 데이터를 Hash로 매핑하여 관리 가능 O(1)의 빠른 검색, 삽입, 삭제 key와 Hash 연관성이 없어 보안 유리 데이터 캐시 기능 중복 제거 유용 Hash Table 단점 충돌 발생 가능성 해시 함수에 의존 순서 무시 공간 복잡도 증가 Hash Table 시간복잡도   평균 최악 탐색 O(1) O(N) 삽입 O(1) O(N) 삭제 O(1) O(N) 참고: https://d2.naver.com/helloworld/831311 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Hash-table resolve-conflict etc data structure, algorithm",
    "url": "/etc/data%20structure,%20algorithm/2022-05-15-Hash-Table/"
  },{
    "title": "Stack vs Queue",
    "text": "Stack 선형 자료구조의 일종으로 가장 늦게 들어간 원소가 가장 먼저 나온다. 하나씩 차곡차곡 쌓이는 구조로 먼저 Stack에 들어간 원소는 바닥에 깔리게 되고 늦게 들어간 원소는 그 위에 쌓이게 되어 호출 시 가장 위에 있는 원소가 호출되는 구조이다. Last In First Out (LIFO) Queue 선형 자료구조의 일종으로 먼저 들어간 원소가 가장 먼저 나온다. Stack 과는 반대로 먼저 들어간 원소가 맨 앞에서 대기하고 있다가 먼저 나오는 구조이다. First In First Out (FIFO) 2개의 스택을 이용해서 큐 만들기 inbox에 데이터들을 삽입한다. - 1, 2, 3, 4 inbox에 있는 데이터들을 pop(추출)하여 outBox에 push(삽입)한다. - 4, 3, 2, 1 outBox에 있는 데이터를 pop(추출)한다. - 1, 2, 3, 4 순으로 출력 2개의 큐를 이용해서 스택 만들기 2 삽입, main queue - 1, sub queue - 데이터가 삽입될 때 main queue에 데이터들을 sub queue로 그대로 옮겨준다. main queue - 2, sub queue - 1 데이터를 삽입한 후 다시 sub queue에 있던 데이터를 main queue로 옮겨준다. main queue - 2 1, sub queue - main queue에 있는 데이터를 추출한다. 2 1 순으로 출력 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Stack Queue etc data structure, algorithm",
    "url": "/etc/data%20structure,%20algorithm/2022-05-13-Stack-vs-Queue/"
  },{
    "title": "Dynamic Array",
    "text": "동적배열(Dynamic Array) 동적 배열이란 크기가 고정되지 않은 배열을 의미한다. 우리가 평소에 말하는 배열은 크기가 고정된 정적배열이다. 미리 자신이 사용할 배열의 크기를 알면 제일 좋겠지만 알 수 없다. 크기를 너무 크게 잡으면 메모리가 낭비될테고, 그렇다고 크기를 작게 잡으면 매번 새로운 배열에 옮겨 담아야 하니 귀찮다. 이럴때 배열의 크기를 동적으로 늘려서 사용하고 싶을 때 필요하다. 예를들어 크기가 추가로 필요할때마다 resize()를 통해 기존배열의 2배 만큼 재할당 받는다. 시간복잡도   배열 동적 배열 검색 O(1) O(1) 삽입 O(N) O(N) 삭제 O(N) O(N) *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Array Dynamic-Array etc data structure, algorithm",
    "url": "/etc/data%20structure,%20algorithm/2022-05-13-Dynamic-Array/"
  },{
    "title": "Array vs Linked List",
    "text": "Array Array는 논리적 저장 순서와 물리적 저장 순서가 일치한다. 따라서 인덱스(index)로 해당 원소(element)에 접근할 수 있다. 그렇기 때문에 찾고자 하는 원소의 인덱스 값을 알고있으면 Big-O(1)에 해당 원소로 접근가능. 하지만 삭제 또는 삽입의 과정에서는 해당 원소에 접근하여 삭제 또는 삽입을 완료한 뒤(O(1)) 추가적인 작업(O(n))이 필요하다. 배열에서 어느 원소를 삭제한다고 했을 때 그 원소보다 큰 인덱스를 갖는 원소들을 왼쪽으로 옮겨줘야 되고 그 시간 복잡도가 O(n)이 된다. 삽입의 경우도 어느 원소를 추가하고자 한다면 추가하려는 곳의 인덱스 보다 큰 인덱스를 갖는 원소들을 오른쪽으로 옮겨줘야 되기 때문에 O(n)의 시간이 걸린다. Linked List 각각의 원소들은 자기 자신 다음에 어떤 원소인지만을 기억하고 있다. 따라서 이 부분만 다른 값으로 바꿔주면 삭제와 삽입을 O(1)에 해결할 수 있다. 하지만 Linked List도 원하는 위치에 삽입을 하거나 삭제를 하고자 하면 Search 과정이 필요하기 때문에 결국 O(n)이 걸린다. *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Array Linked-List etc data structure, algorithm",
    "url": "/etc/data%20structure,%20algorithm/2022-05-12-Array-vs-Linked-List/"
  },{
    "title": "HTTP 상태코드",
    "text": "상태 코드 클라이언트가 보낸 요청의 처리 상태를 응답에서 알려주는 기능 1xx (Informational): 요청이 수신되어 처리중 2xx (Successful): 요청 정상 처리 3xx (Redirection): 요청을 완료하려면 추가 행동이 필요 4xx (Client Error): 클라이언트 오류, 잘못된 문법등으로 서버가 요청을 수행할 수 없음 5xx (Server Error): 서버 오류, 서버가 정상 요청을 처리하지 못함 만약 모르는 상태 코드가 나타나면? 클라이언트가 인식할 수 없는 상태코드를 서버가 반환하면? 클라이언트는 상위 상태코드로 해석해서 처리 미래에 새로운 상태 코드가 추가되어도 클라이언트를 변경하지 않아도 됨 ex 284 ??? -&gt; 2xx (Successful) 423 ??? -&gt; 4xx (Client Error) 568 ??? -&gt; 5xx (Server Error) 1xx (Informational) 요청이 수신되어 처리중 거의 사용하지 않음 2xx 성공 2xx (Successful) 클라이언트의 요청을 성공적으로 처리 200 OK 201 Created 요청 성공해서 새로운 리소스가 생성됨 202 Accepted 요청이 접수되었으나 처리가 완료되지 않았음 배치 처리 같은 곳에서 사용 예) 요청 접수 후 1시간 뒤에 배치 프로세스가 요청을 처리함 204 No Content 서버가 요청을 성공적으로 수행했지만, 응답 페이로드 본문에 보낼 데이터가 없음 save 버튼의 결과로 아무 내용이 없어도 된다. 결과 내용이 없어도 204 메시지(2xx)만으로 성공을 인식할 수 있다. 3xx - 리다이렉션 요청을 완료하기 위해 유저 에이전트의 추가 조치 필요 웹 브라우저는 3xx 응답의 결과에 Location 헤더가 있으면, Location 위치로 자동 이동(리다이렉트) 리다이렉션 종류 영구 리다이렉션 - 특정 리소스의 URI가 영구적으로 이동 예) /members -&gt; /users 예) /event -&gt; /new-event 일시 리다이렉션 - 일시적인 변경 주문 완료 후 주문 내역 화면으로 이동 PRG: Post/Redrirect/Get 특수 리다이렉션 결과 대신 캐시를 사용 영구 리다이렉션 301, 308 리소스의 URI가 영구적으로 이동 원래의 URL을 사용X, 검색 엔진 등에서도 변경 인지 301 Moved Permanently 리다이렉트시 요청 메서드가 GET으로 변하고, 본문이 제거될 수 있음. 308 Permanent Redirect 301과 기능은 같음 리다이렉트시 요청 메서드와 본문 유지(처음 POST를 보내면 리다이렉트도 POST 유지) 영구 리다이렉션 - 301 영구 리다이렉션 - 308 일시적인 리다이렉션 302, 303, 307 리소스의 URI가 일시적으로 변경 따라서 검색 엔진 등에서 URL을 변경하면 안됨 302 Found 리다이렉트시 요청 메서드가 GET으로 변하고, 본문이 제거될 수 있음 303 See Other 302와 기능은 같음 리다이렉트시 요청 메서드가 GET으로 변경 307 Temporary Redirect 302와 기능은 같음 리다이렉트시 요청 메서드와 본문 유지 PRG: Post/Redirect/Get 일시적인 리다이렉션 - 예시 POST로 주문후에 웹 브라우저를 새로고침하면? 다시 요청되서 중복 주문이 될 수 있다. 그래서 POST로 주문후에 주문 결과 화면을 GET 메서드로 리다이렉트 그래서 어떤걸 써야 될까? 302, 303, 307 302 Found -&gt; GET으로 변할 수 있음 303 See Other -&gt; 메서드가 Get으로 변경 307 Temporary Redirect -&gt; 메서드가 변하면 안됨 처음 302 스펙의 의도는 HTTP 메서드를 유지하는 것 그런데 웹 브라우저들이 대부분 GET으로 바꾸어버림(일부는 다르게 동작) 그래서 모호한 302를 대신하는 명확한 303, 307이 등장 303, 307을 권장하지만 이미 많은 애플리케이션 라이브러리들이 302를 기본값으로 사용 자동 리다이렉션시에 GET으로 변해도 되면 그냥 302를 사용해도 큰 문제 없음 기타 리다이렉션 300 304 300 Multiple Choiches: 사용하지 않음. 304 Not Modified 캐시를 목적으로 사용 클라이언트에게 리소스가 수정되지 않았음을 알려준다. 따라서 클라이언트는 로컬PC에 저장된 캐시를 재사용한다. (캐시로 리다이렉트) 304 응답은 응답에 메시지 바디를 포함하면 안된다. (로컬 캐시를 사용해야 하므로) 조건부 GET, HEAD 요청시 사용 4xx - 클라이언트 오류 클라이언트의 요청에 잘못된 문법등으로 서버가 요청을 수행할 수 없음 오류의 원인이 클라이언트에 있음 클라이언트가 이미 잘못된 요청, 데이터를 보내고 있기 때문에, 똑같은 재시도가 실패함 400 Bad Request 클라이언트가 잘못된 요청을 해서 서버가 요청을 처리할 수 없음 요청 구문, 메시지 등등 오류 클라이언트는 요청 내용을 다시 검토하고, 보내야함 예) 요청 파라미터가 잘못되거나, API 스펙이 맞지 않을 때 401 Unauthorized 클라이언트가 해당 리소스에 대한 인증이 필요함 인증(Authentication) 되지 않음 401 오류 발생시 응답에 WWW-Authenticate 헤더와 함께 인증 방법을 설명 참고 인증(Authentication): 본인이 누구인지 확인, (로그인) 인가(Authorization): 권한부여 (ADMIN 권한처럼 특정 리소스에 접근할 수 있는 권한, 인증이 있어야 인가가 있음) 이름이 Unauthorized이라 권한이 없나 할 수 있지만 사실 인증이 되지 않은것 (이름을 Unauthenticated로 지어야되지 않나?) 403 Forbidden 서버가 요청을 이해했지만 승인을 거부함 주로 인증 자격 증명은 있지만, 접근 권한이 불충분한 경우 예) 어드민 등급이 아닌 사용자가 로그인은 했지만, 어드민 등급의 리소스에 접근하는 경우 404 Not Found 요청 리소스를 찾을 수 없음 요청 리소스가 서버에 없음 또는 클라이언트가 권한이 부족한 리소스에 접근할 때 해당 리소스를 숨기고 싶을 때 5xx - 서버 오류 서버 문제로 오류 발생 서버에 문제가 있기 때문에 재시도 하면 성공할 수도 있음 500 Internal Server Error 서버 문제로 오류 발생, 애매하면 500 오류 503 Service Unavailable 서비스 이용 불가 서버가 일시적인 과부하 또는 예정된 작업으로 잠시 요청을 처리할 수 없음 Retry-After 헤더 필드로 얼마뒤에 복구되는지 보낼 수도 있음 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP http-status study http web",
    "url": "/study/http%20web/2022-05-11-http-status/"
  },{
    "title": "HTTP 메서드 활용",
    "text": "클라이언트에서 서버로 데이터 전송 데이터 전달 방식은 크게 2가지 쿼리 파라미터를 통한 데이터 전송 GET 주로 정렬 필터(검색어) 메시지 바디를 통한 데이터 전송 POST, PUT, PATCH 회원가입, 상품 주문, 리소스 등록, 리소스 변경 클라이언트에서 서버로 데이터 전송 4가지 상황 정적 데이터 조회 이미지, 정적 텍스트 문서 동적 데이터 조회 주로 검색, 게시판 목록에서 정렬 필터(검색어) HTML Form을 통한 데이터 전송 회원가입, 상품 주문, 데이터 변경 HTTP API를 통한 데이터 전송 회원 가입, 상품 주문, 데이터 변경 서버 to 서버, 앱 클라이언트, 웹 클라이언트(Ajax) 정적 데이터 조회 쿼리 파라미터 미사용 이미지, 정적 텍스트 문서 조회는 GET 사용 정적 데이터는 일반적으로 쿼리 파라미터 없이 리소스 경로로 단순하게 조회 가능 동적 데이터 조회 쿼리 파라미터 사용 주로 검색, 게시판 목록에서 정렬 필터(검색어) 조회 조건을 줄여주는 필터, 조회 결과를 정렬하는 정렬 조건에 주로 사용 조회는 GET 사용 GET은 쿼리 파라미터 사용해서 데이터를 전달 HTML Form 데이터 전송 POST 전송 - 저장 HTML Form submit시 POST 전송 예) 회원 가입, 상품 주문, 데이터 변경 Content-Type: application/x-www-form-urlencoded 사용 form의 내용을 메시지 바디를 통해서 전송(key=value) 전송 데이터를 url encoding 처리 예)abc김 -&gt; abc%EA%B9%80 GET 전송 - 조회 HTML Form은 GET 전송도 가능 multipart/form-data Content-Type: multipart/form-data 파일 업로드 같은 바이너리 데이터 전송시 사용 다른 종류의 여러 파일과 폼의 내용 함께 전송 가능(그래서 이름이 multipart) *참고: HTML Form 전송은 GET, POST만 지원 HTTP API 데이터 전송 서버 to 서버 백엔드 시스템 통신 앱 클라이언트 아이폰, 안드로이드 웹 클라이언트 HTML에서 Form 전송 대신 자바 스크립트를 통한 통신에 사용(AJAX) 예) React, VueJs 같은 웹 클라이언트와 API 통신 POST, PUT, PATCH: 메시지 바디를 통해 데이터 전송 GET: 조회, 쿼리 파라미터로 데이터 전달 Content-Type: application/json을 주로 사용 TEXT, XML, JSON 등등 API 설계 예시 게시판 목록 /posts -&gt; GET 게시판 등록 /posts -&gt; POST 게시판 조회 /posts/{id} -&gt; GET 게시판 수정 /posts/{id} -&gt; PATCH, PUT, POST 게시판 삭제 /posts/{id} -&gt; DELETE 만약 순수 HTML FORM만 쓴다면? GET, POST만 지원하므로 제약이 있다. 이럴때 동사로 된 리소스 경로 사용 게시판 목록 /posts -&gt; GET 게시판 등록 폼 /posts/new -&gt; GET 게시판 등록 /posts -&gt; POST 게시판 조회 /posts/{id} -&gt; GET 게시판 수정 폼 /posts/{id}/edit -&gt; GET 게시판 수정 /posts/{id} -&gt; POST 게시판 삭제 /posts/{id}/delete -&gt; POST 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP http-method study http web",
    "url": "/study/http%20web/2022-05-07-http-method-use/"
  },{
    "title": "영속성 전이",
    "text": "영속성 전이: CASCADE 특정 엔티티를 영속 상태로 만들 때 연관된 엔티티도 함께 영속상태로 만들고 싶을 때 ex) 부모 엔티티를 저장할 때 자식 엔티티도 함께 저장. 영속성 전이: 저장 @OneToMany (mappedBy=”parent”, cascade=CascadeType.PERSIST) // Parent Entity //@OneToMany (mappedBy=\"parent\") //Private List&lt;Child&gt; childList = new ArrayList&lt;&gt;(); Child child1 = new Child(); Child child2 = new Child(); Parent parent = new Parent(); parent.addChild(child1); parent.addChild(child2); em.persist(parent); em.persist(child1); em.persist(child2); // Parent Entity에 persist 추가 //@OneToMany (mappedBy=\"parent\", cascade=CascadeType.PERSIST) //Private List&lt;Child&gt; childList = new ArrayList&lt;&gt;(); Child child1 = new Child(); Child child2 = new Child(); Parent parent = new Parent(); parent.addChild(child1); parent.addChild(child2); em.persist(parent); // 부모 영속화하면 자동으로 자식은 영속화 해줌 영속성 전이: CASCADE - 주의! 영속성 전이는 연관관계를 매핑하는 것과 아무 관련이 없음 엔티티를 영속화할 때 연관된 엔티티도 함께 영속화하는 편리함을 제공할 뿐 Parent와 Child Entity의 LifeCycle이 비슷할 때 사용 특정 엔티티가 개인 소유할 때 사용 CASCADE의 종류 ALL: 모두 적용 PERSIST: 영속 REMOVE: 삭제 MERGE: 병합 REFRESH: REFRESH DETACH: DETACH 고아 객체 고아 객체 제거: 부모 엔티티와 연관관계가 끊어진 자식 엔티티를 자동으로 삭제 orphanRemoval = true 고아 객체 - 주의 참조가 제거된 엔티티는 다른 곳에서 참조하지 않는 고아 객체로 보고 삭제하는 기능 참조하는 곳이 하나일 때 사용해야함! 특정 엔티티가 개인 소유할 때 사용 @OneToOne, @OneToMany만 가능 영속성 전이 + 고아 객체, 생명주기 @OneToMany (mappedBy=\"parent\", cascade=CascadeType.PERSIST, orphanRemoval = true) Private List&lt;Child&gt; childList = new ArrayList&lt;&gt;(); CascadeType.ALL + orphanRemovel = true 스스로 생명주기를 관리하는 엔티티는 em.persist()로 영속화, em.remove()로 제거 두 옵션을 모두 활성화 하면 부모 엔티티를 통해서 자식의 생명주기를 관리할 수 있음 CASCADE REMOVE vs orphanRemoval = true 위에 내용들을 공부하다보니 CASCADE중 REMOVE와 orphanRemoval = true의 차이가 뭘까하고 궁금증이 생겼다. 알아본바에 의하면 둘다 부모 엔티티가 삭제되면 자식 엔티티가 삭제되는 것은 동일하다. 하지만 삭제가아니라 관계를 제거하는경우에 차이가 있었다. REMOVE같은 경우 부모 엔티티가 자식 엔티티와의 관계를 제거해도 자식 엔티티는 삭제되지 않고 그대로 남아있다. orphanRemoval = true 같은 경우 부모 엔티티가 자식 엔티티의 관계를 제거하면 자식은 고아로 취급되어 삭제된다. =&gt; 무작정 쓰지 말고 영속성 전이를 사용하든 orphanRemoval을 사용하든 주의하여 사용하자 참고: JPA *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "JPA cascade orphanRemoval study jpa",
    "url": "/study/jpa/2022-05-01-Transitive-Persistence/"
  },{
    "title": "HTTP 메서드",
    "text": "HTTP 메서드 종류 주요 메서드 GET: 리소스 조회 POST: 요청 데이터 처리, 주로 등록에 사용 PUT: 리소스를 대체, 해당 리소스가 없으면 생성 PATCH: 리소스 부분 변경 DELETE: 리소스 삭제 기타 메서드 HEAD: GET과 동일하지만 메시지 부분을 제외하고, 상태 줄과 헤더만 반환 OPTIONS: 대상 리소스에 대한 통신 가능 옵션(메서드)을 설명(주로 CORS에서 사용) CONNECT: 대상 자원으로 식별되는 서버에 대한 터널을 설정 TRACE: 대상 리소스에 대한 경로를 따라 메시지 루프백 테스트를 수행 GET 리소스 조회 서버에 전달하고 싶은 데이터는 query(쿼리 파라미터, 쿼리 스트링)를 통해서 전달 메시지 바디를 사용해서 데이터를 전달할 수 있지만, 지원하지 않는곳이 많아서 권장하지 않음 POST 요청 데이터 처리 메시지 바디를 통해 서버로 요청 데이터 전달 서버는 요청 데이터를 처리 메시지 바디를 통해 들어온 데이터를 처리하는 모든 기능을 수행 주로 전달된 데이터로 신규 리소스 등록, 프로세스 처리에 사용 PUT 리소스를 대체 리소스가 있으면 대체 리소스가 없으면 생성 원래 있던 것에 덮어버림 클라이언트가 리소스를 식별! 클라이언트가 리소스 위치를 알고 URI 지정 POST와 차이점 *리소스를 완전히 대체하기 때문에 주의해야한다. ex) “username” = “ex1”, “age” = 25 일때, PUT 요청으로 “age” = 20을 보낸다면 username은 삭제되고 “age” = 20만 남게 됨. PATCH 리소스 부분 변경 DELETE 리소스 제거 HTTP 메서드의 속성 안전(Safe Methods) 멱등(Idempotent Methods) 캐시가능(Cacheable Methods) 안전(Safe) 호출해도 리소스를 변경하지 않는다. 멱등(Idempotent) f(f(x)) = f(x) 한 번 호출하든 두 번 호출하든 100번 호출하든 결과가 같다. 멱등 메서드 GET: 한번 조회하든, 두 번 조회하든 같은 결과가 조회 PUT: 결과를 대체한다. 따라서 같은 요청을 여러번 해도 최종 결과는 같음 DELETE: 결과를 삭제한다. 같은 요청을 여러번 해도 삭제된 결과는 똑같다. POST: 멱등이 아님! 두 번 호출하면 같은 결제가 중복해서 발생 캐시가능(Cacheable) 응답 결과 리소스를 캐시해서 사용해도 되는가? GET, HEAD, POST, PATCH 캐시 가능 실제로는 GET, HEAD 정도만 캐시로 사용 POST, PATCH는 본문 내용까지 고려해야 되서 어려움 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP http-method study http web",
    "url": "/study/http%20web/2022-04-27-http-method/"
  },{
    "title": "HTTP",
    "text": "모든 것이 HTTP HTTP 메시지에 모든 것을 전송 HTML, TEXT IMAGE, 음성, 영상, 파일 JSON, XML (API) 거의 모든 형태의 데이터 전송 가능 서버간 데이터를 주고 받을 떄도 대부분 HTTP 사용 HTTP 특징 클라이언트 서버 구조 Request Response 구조 클라이언트는 서버에 요청을 보내고, 응답을 대기 서버가 요청에 대한 결과를 만들어서 응답 스테이스리스(Stateless) 서버가 클라이언트의 상태를 보존X 장점: 서버 확장성 높음(스케일 아웃) 단점: 클라이언트가 추가 데이터 전송 스테이트풀(Stateful) 서버가 사용자의 상태정보(client or server)를 기억하고 있다가 활용 장점: 서버에서 클라이언트 세션을 유지할 필요가 없을 때 서버 리소스를 절약 단점: 중간에 서버가 장애가나면?? 비 연결성(connectionless) 연결을 유지하는 모델 연결을 유지하지 않는 모델 HTTP는 기본이 연결을 유지하지 않는 모델 일반적으로 초 단위 이하의 빠른 속도로 응답 1시간 동안 수천명이 서비스를 사용해도 실제 서버에서 동시에 처리하는 요청은 수십개 이하로 매우 작음 서버 자원을 매우 효율적으로 사용할 수 있음 한계와 극복 TCP/IP 연결을 새로 맺어야 함 - 3 way handshake 시간 추가 웹 브라우저로 사이트를 요청하면 HTML 뿐만 아니라 자바스크립트 ,css, 추가 이미지 등등 수 많은 자원이 함께 다운로드 지금은 HTTP 지속 연결(Persistent Connections)로 문제 해결 HTTP/2, HTTP/3에서 더 많은 최적화 HTTP 초기 - 연결, 종료 낭비 HTTP 지속 연결(Persistent Connections) 얼마간 연결을 열어놓고 여러 요청에 재사용함(일정 시간뒤에 종료) HTTP 메시지 HTTP 메시지 구조 요청 메시지 - 시작 라인(start-line) request-line = method SP(공백) request-target SP HTTP-version CRLF(엔터) HTTP 메서드 (GET: 조회) 요청대상 (/search?q=hello&amp;hl=ko) HTTP Version (1.1) 요청 메시지 - HTTP 메서드 종류: GET, POST, PUT, DELETE… 서버가 수행해야 할 동작 지정 GET: 리소스 조회 POST: 요청 내역 처리 요청 메시지 - 요청 대상 absolute-path[?query] (절대경로[?쿼리]) 절대경로= “/” 로 시작하는 경로(다른 유형의 경로지정 방법도 있다.) 요청 메시지 - HTTP 버전 HTTP Version 응답 메시지 - 시작 라인(start-line) status-line = HTTP-version SP status-code SP reason-phrase CRLF HTTP 버전 HTTP 상태 코드: 요청 성공, 실패를 나타냄 200: 성공 400: 클라이언트 요청 오류 500: 서버 내부 오류 이유 문구: 사람이 이해할 수 있는 짧은 상태 코드 설명 글 HTTP 헤더 header-field(field-name): filed-value field-name은 대소문자 구분 없음 HTTP 전송에 필요한 모든 부가정보 예) 메시지 바디의 내용, 메시지 바디의 크기, 압축, 인증, 요청 클라이언트 정보, 서버 애플리케이션 정보, 캐시 관리 정보… 표준 헤더가 많음 필요시 임의의 헤더 추가 가능 HTTP 메시지 바디 실제 전송할 데이터 HTML 문서, 이미지, 영상, JSON 등등 byte로 표현할 수 있는 모든 데이터 전송 가능 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "HTTP Stateful Stateless connectionless HTTP-message study http web",
    "url": "/study/http%20web/2022-04-17-HTTP/"
  },{
    "title": "Uri-Webbrowser",
    "text": "URI (Uniform Resource Identifier) URI는 로케이터(locator), 이름(name) 또는 둘다 추가로 분류될 수 있다. URL(Resource Locator) ex) foo://example.com:8042/over/there?name=ferrent#nose URN(Resource Name) ex) urn:example:animal:ferret:nose URI Uniform: 리소스 식별하는 통일된 방식 Resourec: 자원, URI로 식별할 수 있는 모든 것(제한 없음) Identifier: 다른 항목과 구분하는데 필요한 정보 URL, URN URL - Locator: 리소스가 있는 위치를 지정 URN - Name: 리소스에 이름을 부여 위치는 변할 수 있지만, 이름은 변하지 않는다. URN 이름만으로 실제 리소스를 찾을 수 있는 방법이 보편화 되지 않음 URL 전체 문법 scheme://[userinfo@]host[:port][/path][?query][#fragment] [ ]는 option https://www.google.com:443/search?q=hello&amp;hl-ko 프로토콜(https) 호스트명(www.google.com) 포트 번호(443) 패스(/search) 쿼리 파라미터(q=hello&amp;hl=ko) scheme scheme://[userinfo@]host[:port][/path][?query][#fragment] https://www.google.com:443/search?q=hello&amp;hl-ko 주로 프로토콜 사용 프로토콜: 어떤 방식으로 자원에 접근할 것인가 하는 약속 규칙 예) http, https, ftp 등등 http는 80 포트, https는 443 포트를 주로 사용, 포트는 생략 가능 https는 http에 보안 추가 (HTTP Secure) userinfo scheme://[userinfo@]host[:port][/path][?query][#fragment] https://www.google.com:443/search?q=hello&amp;hl-ko URL에 사용자정보를 포함해서 인증 거의 사용하지 않음 host scheme://[userinfo@]host[:port][/path][?query][#fragment] https://www.google.com:443/search?q=hello&amp;hl-ko 호스트명 도메인명 또는 IP 주소를 직접 사용가능 port scheme://[userinfo@]host[:port][/path][?query][#fragment] https://www.google.com:443/search?q=hello&amp;hl-ko 접속 포트 일반적으로 생략, 생략시 http는 80, https는 443 path scheme://[userinfo@]host[:port][/path][?query][#fragment] https://www.google.com:443/search?q=hello&amp;hl-ko 리소스 경로(path), 계층적 구조 ex) /user, /user/20 query scheme://[userinfo@]host[:port][/path][?query][#fragment] https://www.google.com:443/search?q=hello&amp;hl-ko key=value 형태 ?로 시작, &amp;로 추가 가능 =&gt; ?keyA=value&amp;keyB=valueB query parameter, query string 등으로 불림, 웹서버에 제공하는 파라미터, 문자 형태 fragment scheme://[userinfo@]host[:port][/path][?query][#fragment] https://docs.spring.io/spring-boot/docs/current/reference/html/gettingstarted.html#getting-started-introducing-spring-boot fragment html 내부 북마크 등에 사용 서버에 전송하는 정보 아님 웹 브라우저 요청 흐름 HTTP 요청 메시지 GET/search?q=hello&amp;hl=ko HTTP/1.1 HOST: www.google.com *DNS가 뭔지 모르겠으면 이전글을 참고 패킷 생성 요청, 응답 과정 HTTP 응답 메시지 HTTP/1.1 200 OK Content-Type: text/html;charset=UTF-8 Content-Length: 3423 &lt;html&gt; &lt;body&gt;...&lt;/body&gt; &lt;/html&gt; 결과 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "URI URL URN study http web",
    "url": "/study/http%20web/2022-04-03-Uri-Webbrowser/"
  },{
    "title": "Internet Network",
    "text": "인터넷에서 컴퓨터 둘은 어떻게 통신할까? IP 주소 부여 IP 인터넷 프로토콜 역활 지정한 IP 주소에 데이터 전달 패킷(Packet)이라는 통신 단위로 데이터 전달 IP 패킷 정보 클라이언트 패킷 전달 서버 패킷 전달 IP 프로토콜의 한계 비연결성 패킷을 받을 대상이 없거나 서비스 불능 상태여도 패킷 전송 비신뢰성 중간에 패킷이 사라지면? 패킷이 순서대로 안오면? 프로그램 구분 같은 IP를 사용하는 서버에서 통신하는 애플리케이션이 둘 이상이면? TCP, UDP 인터넷 프로토콜 스택의 4계층 프로토콜 계층 TCP/IP 패킷 정보 TCP 특징 전송 제어 프로토콜(Transmission Control Protocol) 연결지향 - TCP 3 way handshake (가상 연결) 데이터 전달 보증 순서 보증 신뢰할 수 있는 프로토콜 현재는 대부분 TCP 사용 TCP 3 way handshake SYN: 접속 요청 ACK: 요청 수락 3.ACK와 함께 데이터 전송 가능 UDP 특징 사용자 데이터그램 프로토콜(User Datagram Protocol) 연결지향 X 데이터 전달 보증 X 순서 보장 X 데이터 전달 및 순서가 보장되지 않지만, 단순하고 빠름 정리 IP와 거의 같지만 PORT + 체크섬 정도만 추가 애플리케이션에 추가 작업 필요 PORT 같은 IP 내에서 프로세스 구분 0 ~ 65535 할당 가능 0 ~ 1023 : 잘 알려진 포트, 사용하지 않는 것이 좋음 FTP - 20, 21 TELNET - 23 HTTP - 80 HTTPS - 443 DNS IP는 기억하기 어렵다. ex) IP: 200.200.200.2 IP는 변경될 수 있다. 과거 IP: 200.200.200.2 -&gt; 신규 IP: 200.200.200.3 DNS 도메인 네임 시스템(Domain Name System) 도메인 명을 IP 주소로 변환 참고: Http Web *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "http IP TCP UDP PORT DNS study http web",
    "url": "/study/http%20web/2022-03-27-Internet-Network/"
  },{
    "title": "빈 생명주기 콜백",
    "text": "빈 생명주기 콜백 시작 데이터베이스 커넥션 풀이나, 네트워크 소켓처럼 애플리케이션 시작 시점에 필요한 연결을 미리 해두고, 애플리케이션 종료 시점에 연결을 모두 종료하는 작업을 하려면, 객체의 초기화와 종료 작업이 필요하다. 스프링을 통해 어떻게 이러한 작업들을 진행하는지 알아보자. // 실제로 연결 x 단순히 문자만 출력 // connect() 호출해서 연결, 애플리케이션 종료되면 disConnect() 호출 public class NetworkClient { private String url; public NetworkClient() { System.out.println(\"생성자 호출, url = \" + url); connect(); call(\"초기화 연결 메시지\"); } public void setUrl(String url) { this.url = url; } //서비스 시작시 호출 public void connect() { System.out.println(\"connect: \" + url); } public void call(String message) { System.out.println(\"call: \" + url + \" message = \" + message); } //서비스 종료시 호출 public void disconnect() { System.out.println(\"close: \" + url); } } public class BeanLifeCycleTest { @Test public void lifeCycleTest() { ConfigurableApplicationContext ac = new AnnotationConfigApplicationContext(LifeCycleConfig.class); NetworkClient client = ac.getBean(NetworkClient.class); ac.close(); } @Configuration static class LifeCycleConfig { @Bean public NetworkClient networkClient() { NetworkClient networkClient = new NetworkClient(); networkClient.setUrl(\"http://hello-spring.dev\"); return networkClient; } } } 실행하면 다음과 같은 결과가 나온다. 생성자 호출, url = null connect: null call: null message = 초기화 연결 메시지 생성자 부분을 보면 url 정보 없이 connect가 호출되는 것을 확인할 수 있다. 객체를 생성하는 단계에는 url이 없고, 객체를 생성한 다음에 외부에서 수정자 주입을 통해 setUrl()이 호출되어야 url 존재하게 된다. 스프링 빈은 간단하게 다음과 같은 라이프사이클을 가진다. 객체 생성 -&gt; 의존관계 주입 스프링 빈은 객체를 생성하고, 의존관계 주입이 다 끝난 다음에야 필요한 데이터를 사용할 수 있는 준비가 완료된다. 따라서 초기화 작업은 의존관계 주입이 모두 완료되고 난 다음에 호출해야 한다. 그런데 어떻게 개발자가 의존관계 주입이 완료된 시점을 알 수 있을까? 스프링은 의존관계 주입이 완료되면 스프링 빈에게 콜백 메서드를 통해서 초기화 시점을 알려주는 다양한 기능을 제공한다. 또한 스프링은 스프링컨테이너가 종료되기 직전에 소멸 콜백을 준다. 따라서 안전하게 종료 작업을 진행할 수 있다. 스프링 빈의 이벤트 라이프사이클 스프링 컨테이너 생성 -&gt; 스프링 빈 생성 -&gt; 의존관계 주입 -&gt; 초기화 콜백 -&gt; 사용 -&gt; 소멸전 콜백 -&gt; 스프링 종료 *참고: 객체의 생성과 초기화를 분리하자. 생성자는 필수 정보(파라미터)를 받고, 메모리를 할당해서 객체를 생성하는 책임을 가진다. 반면에 초기화는 이렇게 생성된 값을 활용해서 외부 커넥션을 연결하는 등 무거운 동작을 수행한다. 따라서 생성자 안에서 무거운 초기화 작업을 함께 하는 것 보다는 객체를 생성하는 부분과 초기화 하는 부분을 명확하게 나누는 것이 유지보수 관점에서 좋다. 물론 내부 값들만 약간 변경하는 정도는 생성자에서 한번에 처리하는게 더 나을 수 있다. 스프링은 크게 3가지 방법으로 빈 생명주기 콜백을 지원한다. 인터페이스(InitializingBean, DisposableBean) 설정 정보에 초기화 메서드, 종료 메서드 지정 @PostConstruct, @PreDestroy 애노테이션 지원 인터페이스 InitializingBean, DisposableBean public class NetworkClient implements InitializingBean, DisposableBean { private String url; public NetworkClient() { System.out.println(\"생성자 호출, url = \" + url); } public void setUrl(String url) { this.url = url; } //서비스 시작시 호출 public void connect() { System.out.println(\"connect: \" + url); } public void call(String message) { System.out.println(\"call: \" + url + \" message = \" + message); } //서비스 종료시 호출 public void disConnect() { System.out.println(\"close + \" + url); } @Override public void afterPropertiesSet() throws Exception { connect(); call(\"초기화 연결 메시지\"); } @Override public void destroy() throws Exception { disConnect(); } } InitializingBean은 afterPropertiesSet() 메서드로 초기화를 지원 DisposableBean은 destory() 메서드로 소멸을 지원 위의 BeanLifeCycleTest 실행 후 출력 결과 생성자 호출, url = null NetworkClient.afterPropertiesSet connect: http://hello-spring.dev call: http://hello-spring.dev message = 초기화 연결 메시지 13:24:49.043 [main] DEBUG org.springframework.context.annotation AnnotationConfigApplicationContext - Closing NetworkClient.destroy close + http://hello-spring.dev 출력 결과를 보면 초기화 메서드가 주입 완료 후에 적절하게 호출 된 것을 확인 할 수 있다. 그리고 스프링 컨테이터의 종료가 호출되자 소멸 메서드가 호출 된 것도 확인할 수 있다. 초기화, 소멸 인터페이스 단점 이 인터페이스는 스프링 전용 인터페이스다. 해당 코드가 스프링 전용 인터페이스에 의존 초기화, 소멸 메서드의 이름을 변경할 수 없다. 내가 코드를 고칠 수 없는 외부 라이브러리에 적용할 수 없다. *참고: 이 방법은 스프링 초창기에 나온 방법이고, 지금은 다음의 더 나은 방법들이 있어서 거의 사용하지 않는다. 빈 등록 초기화, 소멸 메서드 지정 설정 정보에 @Bean(initMethod = “init”, destoryMethod = “close”)처럼 초기화, 소멸 메서드를 지정할 수 있다. public class NetworkClient { private String url; public NetworkClient() { System.out.println(\"생성자 호출, url = \" + url); } public void setUrl(String url) { this.url = url; } //서비스 시작시 호출 public void connect() { System.out.println(\"connect: \" + url); } public void call(String message) { System.out.println(\"call: \" + url + \" message = \" + message); } //서비스 종료시 호출 public void disConnect() { System.out.println(\"close + \" + url); } public void init() { System.out.println(\"NetworkClient.init\"); connect(); call(\"초기화 연결 메시지\"); } public void close() { System.out.println(\"NetworkClient.close\"); disConnect(); } } 설정정보에 초기화 소멸 메서드 지정 @Configuration static class LifeCycleConfig { @Bean(initMethod = \"init\", destroyMethod = \"close\") public NetworkClient networkClient() { NetworkClient networkClient = new NetworkClient(); networkClient.setUrl(\"http://hello-spring.dev\"); return networkClient; } } 결과 생성자 호출, url = null NetworkClient.init connect: http://hello-spring.dev call: http://hello-spring.dev message = 초기화 연결 메시지 13:33:10.029 [main] DEBUG org.springframework.context.annotation.AnnotationConfigApplicationContext - Closing NetworkClient.close close + http://hello-spring.dev 설정 정보 사용 특징 메서드 이름을 자유롭게 줄 수 있다. 스프링 빈이 스프링 코드에 의존하지 않는다. 코드가 아니라 설정 정보를 사용하기 때문에 코드를 고칠 수 없는 외부 라이브러리에도 초기화, 종료 메서드를 적용할 수 있다. 종료 메서드 추론 @Bean의 destroyMethod 속성에는 특별한 기능이 있다. 라이브러리는 대부분 close, shutdown 이라는 이름의 종료 메서드를 사용한다. @Bean의 destoryMethod는 기본값이 (inferred) (추론)으로 등록되어 있다. 이 추론 기능은 close, shutdown 라는 이름의 메서드를 자동으로 호출해준다. 따라서 직접 스프링 빈으로 등록하면 종료 메서드는 따로 적어주지 않아도 잘 동작한다. 추론 기능을 사용하기 싫으면 destoryMethod=”“처럼 빈 공백을 지정하면 된다. 애노테이션 @PostConstruct, @PreDestory public class NetworkClient { private String url; public NetworkClient() { System.out.println(\"생성자 호출, url = \" + url); } public void setUrl(String url) { this.url = url; } //서비스 시작시 호출 public void connect() { System.out.println(\"connect: \" + url); } public void call(String message) { System.out.println(\"call: \" + url + \" message = \" + message); } //서비스 종료시 호출 public void disConnect() { System.out.println(\"close + \" + url); } @PostConstruct public void init() { System.out.println(\"NetworkClient.init\"); connect(); call(\"초기화 연결 메시지\"); } @PreDestroy public void close() { System.out.println(\"NetworkClient.close\"); disConnect(); } } @Configuration static class LifeCycleConfig { @Bean public NetworkClient networkClient() { NetworkClient networkClient = new NetworkClient(); networkClient.setUrl(\"http://hello-spring.dev\"); return networkClient; } } 실행 결과 생성자 호출, url = null NetworkClient.init connect: http://hello-spring.dev call: http://hello-spring.dev message = 초기화 연결 메시지 19:40:50.269 [main] DEBUG org.springframework.context.annotation.AnnotationConfigApplicationContext - Closing NetworkClient.close close + http://hello-spring.dev @PostConstruct, @PreDestory 이 두 애노테이션을 사용하면 가장 편리하게 초기화와 종료를 실행할 수 있다. @PostConstruct, @PreDestroy 애노테이션 특징 최신 스프링에서 가장 권장하는 방법 애노테이션 하나만 붙이면 되므로 매우 편리 스프링에 종속적인 기술이 아니라 JSR-250라는 자바 표준이다. 따라서 스프링이 아닌 다른 컨테이너에서도 동작한다. 컴포넌트 스캔과 잘 어울린다. 유일한 단점은 외부 라이브러리에는 적용하지 못한다는 것이다. 외부 라이브러리를 초기화, 종료 해야 하면 @Bean의 기능을 사용하자. 정리 @PostConstruct, @PreDestory 애노테이션을 사용하자 코드를 고칠 수 없는 외부라이브러리를 초기화, 종료해야 하면 @Bean의 initMethod, destroyMethod를 사용하자 참고: Spring Core *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring study spring",
    "url": "/study/spring/2022-03-11-Spring(7)/"
  },{
    "title": "의존관계 자동 주입",
    "text": "다양한 의존관계 주입 방법 의존관계 주입은 크게 4가지 방법이 있다. 생성장 주입 수정자 주입(setter 주입) 필드 주입 일반 메서드 주입 생성자 주입 이름 그대로 생성자를 통해서 의존 관계를 주입 받는 방법 특징 생성자 호출시점에 딱 1번만 호출 불변, 필수 의존관계에 사용 @Component public class OrderServiceImpl implements OrderService { private final MemberRepository memberRepository; private final DiscountPolicy discountPolicy; // @Autowired public OrderServiceImpl(MemberRepository memberRepository, DiscountPolicy discountPolicy) { this.memberRepository = memberRepository; this.discountPolicy = discountPolicy; } } 생성자가 딱 1개만 있으면 @Autowired를 생략해도 자동 주입 된다. 수정자 주입(setter 주입) setter라 불리는 필드의 값을 변경하는 수정자 메서드를 통해서 의존관계를 주입하는 방법 특징 선택, 변경 가능성이 있는 의존관계에 사용 @Component public class OrderServiceImpl implements OrderService { private MemberRepository memberRepository; private DiscountPolicy discountPolicy; @Autowired public void setMemberRepository(MemberRepository memberRepository) { this.memberRepository = memberRepository; } @Autowired public void setDiscountPolicy(DiscountPolicy discountPolicy) { this.discountPolicy = discountPolicy; } } *참고: @Autowired의 기본 동작은 주입할 대상이 없으면 오류가 발생한다. 주입할 대상이 없어도 동작하게 하려면 @Autowired(required = false)로 지정 필드 주입 이름 그대로 필드에 바로 주입하는 방법 특징 코드가 간결해서 좋아보이지만 외부에서 변경이 불가능해서 테스트하기 힘들다는 치명적인 단점이 있다. 사용하지 않는걸 추천 애플리케이션의 실제 코드와 관계 없는 테스트 코드나 스프링 설정을 목적으로 하는 @Configuration 같은 곳에서만 특별한 용도로 사용 @Component public class OrderServiceImpl implements OrderService { @Autowired private MemberRepository memberRepository; @Autowired private DiscountPolicy discountPolicy; } *참고: 순수한 자바 테스트 코드에는 당연히 @Autowired가 동작하지 않는다. @SpringBootTest처럼 스프링 컨테이너를 테스트에 통합한 경우에만 가능 일반 메서드 주입 일반 메서드를 통해서 주입 받을 수 있다. 특징 한번에 여러 필드를 주입 받을 수 있다. 일반적으로 잘 사용하지 않는다. @Component public class OrderServiceImpl implements OrderService { private MemberRepository memberRepository; private DiscountPolicy discountPolicy; @Autowired public void init(MemberRepository memberRepository, DiscountPolicy discountPolicy) { this.memberRepository = memberRepository; this.discountPolicy = discountPolicy; } } *참고: 의존관계 자동 주입은 스프링 컨테이너가 관리하는 스프링 빈이어야 동작한다. 스프링 빈이 아닌 클래스에서 @Autowired 코드를 적용해도 아무기능도 동작하지 않음. 생성자 주입을 쓰자 과거에는 수정자 주입과, 필드 주입을 많이 사용했지만, 최근에는 스프링을 포함한 DI 프레임워크 대부분이 생성자 주입을 권장한다. 그 이유는 아래와 같다. 불변 대부분의 의존관계 주입은 한번 일어나면 애플리케이션 종료시점까지 의존관계를 변경할 일이 없다. 오히려 대부분의 의존관계는 애플리케이션 종료 전까지 변하면 안된다. 수정자 주입을 사용하면, setXxx 메서드를 public으로 열어두어야 하기 때문에 누군가 실수로 변경할 수도 있다. 생성자 주입은 객체를 생성할 때 딱 1번만 호출되므로 이후에 호출되는 일이 없다. 따라서 불변하게 설계할 수 있다. 누락 프레임워크 없이 순수한 자바 코드를 단위 테스트 하는 경우에 다음과 같이 수정자 의존관계인 경우 public class OrderServiceImpl implements OrderService { private MemberRepository memberRepository; private DiscountPolicy discountPolicy; @Autowired public void setMemberRepository(MemberRepository memberRepository) { this.memberRepository = memberRepository; } @Autowired public void setDiscountPolicy(DiscountPolicy discountPolicy) { this.discountPolicy = discountPolicy; } //... } @Test void createOrder() { OrderServiceImpl orderService = new OrderServiceImpl(); // OrderServiceImpl 빈칸안에 누락됨 orderService.createOrder(1L, \"itemA\", 10000); } 이렇게 테스트를 수행하면 실행은 되지만, 실행 결과는 NPE(Null Point Exception)이 발생하는데, memberRepository, discountPolicy 모두 의존관계 주입이 누락 되었기 때문이다. 하지만 생성자 주입을 사용해서 다시 위에 테스트를 실행하면 주입 데이터를 누락 했을 때 실행이 되지않고 컴파일 오류가 발생한다. 그래서 IDE에서 바로 어떤 값을 필수로 주입해야 하는지 알 수 있다. final 키워드 추가로 생성자 주입을 사용하면 필드에 final 키워드를 사용할 수 있는데, 생성자에서 혹시라도 값이 설정되지 않는 오류를 컴파일 시점에 막아준다. @Component public class OrderServiceImpl implements OrderService { private final MemberRepository memberRepository; private final DiscountPolicy discountPolicy; @Autowired public OrderServiceImpl(MemberRepository memberRepository, DiscountPolicy discountPolicy) { this.memberRepository = memberRepository; // discountPolicy 누락 } //... } 잘 보면 필수 필드인 discountPolicy에 값을 설정해야 하는데, 이 부분이 누락되었다. 실행하면 실행이 되지않고 컴파일 시점에 다음 오류를 발생시킨다. java: variable discountPolicy might not have been initialized 참고: 수정자 주입을 포함한 나머지 주입 방식은 모두 생성자 이후로 호출되므로, 필드에 final 키워드를 사용할 수 없다. 오직 생성자 주입 방식만 final 키워드를 사용할 수 있다. -&gt; 생성자 주입을 선택하고 필요하면 수정자 주입을 선택하자. 롬복과 최신 트렌드 막상 개발을 해보면, 대부분이 다 불변이고, 그래서 다음과 같이 생성자에 final 키워드를 사용하게 된다. 그런데 생성자도 만들어야 하고, 주입 받은 값을 대입하는 코드도 만들어야 하고.. 편리한 방법은 없을까? 기본 코드 @Component public class OrderServiceImpl implements OrderService { private final MemberRepository memberRepository; private final DiscountPolicy discountPolicy; // @Autowired public OrderServiceImpl(MemberRepository memberRepository, DiscountPolicy discountPolicy) { this.memberRepository = memberRepository; this.discountPolicy = discountPolicy; } } 롬복 적용 @Component @RequiredArgsConstructor public class OrderServiceImpl implements OrderService { private final MemberRepository memberRepository; private final DiscountPolicy discountPolicy; } 롬복 라이브러리가 제공하는 @RequiredArgsConstructor 기능을 사용하면 final이 붙은 필드를 모아서 생성자를 자동으로 만들어준다. 참고: Spring Core *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring study spring",
    "url": "/study/spring/2022-03-08-Spring(6)/"
  },{
    "title": "컴포넌트 스캔",
    "text": "컴포넌트 스캔과 의존관계 자동 주입 시작 지금까지 스프링 빈을 등록할 때는 자바 코드의 @Bean을 이용했다.(또는 XML의 을 사용할 수 도 있음) 등록해야 할 스프링 빈이 수십, 수백개가 되면 일일이 등록하기도 귀찮고, 누락하는 문제도 생긴다. 그래서 스프링은 자동으로 스프링 빈을 등록하는 컴포넌트 스캔이라는 기능 제공 의존관계도 자동으로 주입하는 @Autowired 기능도 제공 @Configuration @ComponentScan public class AppConfig { } 컴포넌트 스캔을 사용하려면 먼저 @ComponentScan을 설정 정보에 붙여주면 된다. 예전의 AppConfig와 달리 @Bean으로 등록한 클래스가 없다. 컴포넌트 스캔은 이름 그대로 @Component 애노테이션이 붙은 클래스를 스캔해서 스프링 빈으로 등록한다. *참고: @Configuration도 컴포넌트 스캔의 대상이 되는데 그 이유는 @Configuration 소스코드를 열어보면 @Component 애노테이션이 붙어있다. 이제 클래스가 컴포넌트 스캔의 대상이 되도록 @Component 애노테이션을 붙여주고 의존관계도 주입해주자. @Component public class MemberServiceImpl implements MemberService { private final MemberRepository memberReepository; @Autowired public MemberServiceImpl(MemberRepository memberRepository) { this.memberRepository = memberRepository; } } 컴포넌트 스캔과 자동 의존관계 주입의 동작순서 1. @ComponentScan @ComponentScan은 @Component가 붙은 모든 클래스를 스프링 빈으로 등록한다. 이때 스프링 빈의 기본 이름은 클래스명을 사용하되 맨 앞글자만 소문자를 사용한다. 빈 이름 기본 전략: MemberServiceImpl 클래스 -&gt; memberServiceImpl 빈 이름 직접 지정: 스프링 빈의 이름을 직접 지정하고 싶으면 @Component(“exmemberService”) 이런식으로 이름을 부여하면 된다. 2. @Autowired 의존관계 자동 주입 생성자에 @Autowired를 지정하면, 스프링 컨테이너가 자동으로 해당 스프링 빈을 찾아서 주입한다. 탐색 위치와 기본 스캔 대상 모든 자바 클래스를 다 컴포넌트 스캔하면 시간이 오래 걸린다. 그래서 꼭 필요한 위치부터 탐색하도록 시작 위치를 지정할 수 있다. @ComponentScan{ basePackages = \"Spring.core\", } basePackages: 탐색할 패키지의 시작 위치를 지정하는데, 이 패키지를 포함해서 하위 패키지를 모두 탐색한다. 만약 지정하지 않으면 @ComponentScan이 붙은 설정 정보 클래스의 패키지가 시작 위치가 된다. 패키지 위치를 지정하지 않고, 설정 정보 클래스의 위치를 프로젝트 최상단에 두는걸 추천. 스프링 부트도 이 방법을 기본으로 제공한다. (@SpringBootApplication이 시작 루트 위치에 있는데 이 설정안에 @ComponentScan이 들어있다.) 컴포넌트 스캔 기본 대상 컴포넌트 스캔은 @Component뿐만 아니라 아래에 있는 것도 대상에 포함한다. @Controller: 스프링 MVC 컨트롤러에서 사용 @Service: 스프링 비즈니스 로직에서 사용 @Repository: 스프링 데이터 접근 계층에서 사용 @Configuration: 스프링 설정 정보에서 사용 컴포넌트 스캔의 용도 뿐만 아니라 다음 애노테이션이 있으면 스프링은 부가 기능을 수행한다. @Controller: 스프링 MVC 컨트롤러로 인식 @Repository: 스프링 데이터 접근 계층으로 인식하고, 데이터 계층의 예외를 스프링 예외로 변환해준다. @Configuration: 스프링 설정 정보로 인식하고, 스프링 빈이 싱글톤으로 유지하도록 추가 처리 @Service: @Service는 스프링에서 따로 특별한 처리를 하지않고, 개발자들이 핵심 비즈니스 로직이 여기에 있겠구나 라고 비즈니스 계층을 인식하는데 도움을 준다. 필터 includeFilters: 컴포넌트 스캔 대상을 추가로 지정한다. excludeFilters: 컴포넌트 스캔에서 제외할 대상을 지정한다. 중복 등록과 충돌 컴포넌트 스캔에서 같은 빈 이름을 등록하면 어떻게 될까? 자동 빈 등록 vs 자동 빈 등록 컴포넌트 스캔에 의해 자동으로 스프링 빈이 등록되는데, 그 이름이 같은 경우 스프링은 오류를 발생시킨다. 수동 빈 등록 vs 자동 빈 등록 이 경우 수동 빈이 자동 빈을 오버라이딩 해버린다.(수동 빈 등록이 우선권) 최근 스프링 부트에서는 수동 빈 등록과 자동 빈 등록이 충돌나면 오류가 발생하도록 기본 값을 바꾸었다. 참고: Spring Core *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring study spring",
    "url": "/study/spring/2022-03-02-Spring(5)/"
  },{
    "title": "싱글톤 컨테이너",
    "text": "웹 애플리케이션은 보통 여러 고객이 동시에 요청을 한다. 스프링 없는 순수한 DI 컨테이너는 요청을 할 때 마다 객체를 새로 생성해 메모리 낭비가 심하다. 해결방안은 해당 객체가 딱 1개만 생성되고, 공유하도록 설계하면 된다. -&gt; 싱글톤 패턴 싱글톤 패턴 클래스의 인스턴스가 딱 1개만 생성되는 것을 보장하는 디자인 패턴이다. 그래서 객체 인스턴스를 2개 이상 생성하지 못하도록 막아야 한다. private 생성자를 사용해서 외부에서 임의로 new 키워드를 사용하지 못하도록 막아야 한다. public class SingletonService { //1. static 영역에 객체를 딱 1개만 생성해둔다. private static final SingletonService instance = new SingletonService(); //2. public으로 열어서 객체 인스터스가 필요하면 이 static 메서드를 통해서만 조회하도록 허용한다. public static SingletonService getInstance() { return instance; } //3. 생성자를 private으로 선언해서 외부에서 new 키워드를 사용한 객체 생성을 못하게 막는다. private SingletonService() { } } 싱글톤 패턴을 구현하는 방법은 다양하다. 여기서는 객체를 미리 생성해두는 방식으로 했다. 싱글톤 패턴을 적용하면 고객의 요청이 올 때마다 객체를 생성하는 것이 아니라, 이미 만들어진 객체를 공유해서 효율적으로 사용할 수 있다. 하지만 싱글톤 패턴은 다음과 같은 문제점들을 가지고 있다. 싱글톤 패턴 문제점 싱글톤 패턴을 구현하는 코드 자체가 많이 들어간다. 의존관계상 클라이언트가 구체 클래스에 의존한다. -&gt; DIP를 위반 클라이언트가 구체 클래스에 의존해서 OCP 원칙을 위반할 가능성이 높다. 테스트하기 어렵다. 내부 속성을 변경하거나 초기화 하기 어렵다. private 생성자로 자식 클래스를 만들기 어렵다. 결론적으로 유연성이 떨어진다. 싱글톤 컨테이너 스프링 컨테이너는 싱글톤 패턴의 문제점을 해결하면서, 객체 인스턴스를 싱글톤(1개만 생성)으로 관리한다. 이전 글에서 학습한 스프링 빈이 바로 싱글톤으로 관리되는 빈이다. 스프링 컨테이너는 싱글턴 패턴을 적용하지 않아도, 객체 인스턴스를 싱글톤으로 관리한다. 스프링 컨테이너는 싱글톤 컨테이너 역활을 한다. 이렇게 싱글톤 객체를 생성하고 관리하는 기능을 싱글톤 레지스트리라고 한다. 스프링 컨테이너의 이런 기능 덕분에 싱글턴 패턴의 모든 단점을 해결하면서 객체를 싱글톤으로 유지할 수 있다. 싱글톤 패턴을 위한 지저분한 코드가 들어가지 않아도 된다. DIP, OCP, 테스트, private 생성자로부터 자유롭게 싱글톤을 사용할 수 있다. *참고: 스프링의 기본 빈 등록 방식은 싱글톤이지만, 싱글톤 방식만 지원하는 것은 아니다. 요청할 때 마다 새로운 객체를 생성해서 반환하는 기능도 제공할 수 있다. 싱글톤 방식의 주의점 싱글톤 패턴이든, 스프링 같은 싱글톤 컨테이너를 사용하든, 객체 인스턴스를 하나만 생성해서 공유하는 싱글톤 방식은 여러 클라이언트가 하나의 같은 객체 인스턴스를 공유하기 때문에 싱글톤 객체는 상태를 유지(stateful)하게 설계하면 안된다. 무상태(stateless)로 설계해야 한다. 특정 클라이언트에 의존적인 필드가 있으면 안된다. 특정 클라이언트가 값을 변경할 수 있는 필드가 있으면 안된다. 가급적 읽기만 가능해야 한다. 필드 대신 자바에서 공유되지 않는, 지역변수, 파라미터, ThreadLocal 등을 사용해야 한다. @Configuration과 싱글톤 @Configuration public class AppConfig{ @Bean public MemberService memberService(){ ... } } @Configuration을 적용하지 않고, @Bean만 적용하면 어떻게 될까? @Configuration을 붙이면 바이트코드를 조작하는 CGLIB 기술을 사용해서 싱글톤을 보장하지만, 만약 @Bean만 적용하면 스프링 빈으로만 등록되고, 싱글톤을 보장하지 않는다. (CGLIB 내부 기술이 궁금하면 찾아보자) 스프링 설정 정보는 항상 @Configuration 사용 참고: Spring Core *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring study spring",
    "url": "/study/spring/2022-02-24-Spring(4)/"
  },{
    "title": "스프링 컨테이너와 스프링 빈",
    "text": "이전 글에서 AppConfig로 MemberService에 의존성을 넣어줄 수 있었다. 스프링 컨테이너에 적용하면서 스프링 컨테이너에 대해 알아보자 스프링 컨테이너 @Configuration public class AppConfig{ @Bean public MemberService memberService() { return new MemberService(new MemoryMemberRepository()); } } // memberService에 join, find 구현되있다고 가정 public class MemberApp { public static void main(String[] args){ // AppConfig appConfig = new AppConfig(); 기존의 방법 // MemberService memberService = appConfig.memberService(); ApplicationContext applicationContext = new AnnotationConfigApplicationContext(AppConfig.class); MemberService memberService = applicationContext.getBean(\"memberService\", MemberService.class); Member member = new Member(1L, \"memberA\", Grade.VIP); memberService.join(member); Member findMember = memberService.findMember(1L); System.out.println(\"new member = \" + member.getName()); System.out.println(\"find Member = \" + findMember.getName()); } } ApplicationContext를 스프링 컨테이너라 한다. 기존에는 개발자가 AppConfig를 사용해서 직접 객체를 생성하고 DI를 했지만(주석 처리한 방법), 이제부터는 스프링 컨테이너를 통해서 사용한다. 스프링 컨테이너는 @Configuration 붙은 AppConfig를 설정(구성) 정보로 사용한다. 여기서 @Bean 이라 적힌 메서드를 모두 호출해서 반환된 객체를 스프링 컨테이너에 등록한다. 이렇게 스프링 컨테이너에 등록된 객체를 스프링 빈이라 한다. 이전에는 개발자가 필요한 객체를 AppConfig를 사용해서 직접 조회했지만, 이제부터는 스프링 컨테이너를 통해서 필요한 스프링 빈(객체)를 찾아야 한다. 스프링 빈은 applicationContext.getBean() 메서드를 사용해서 찾을 수 있다. 코드가 약간 더 복잡해진 것 같은데, 스프링 컨테이너를 사용하면 어떤 장점이 있는지 알아보자. 스프링 컨테이너 생성 스프링 컨테이너가 생성되는 과정을 알아보자. //스프링 컨테이너 생성 ApplicationContext applicationContext = new AnnotationConfigApplicationContext(AppConfig.class); ApplicationContext는 인터페이스이다. newAnnotationConfigApplicationContext(AppConfig.class);는 ApplicationContext 인터페이스의 구현체이다. 스프링 컨테이너는 XML을 기반으로 만들 수도 있고, 애노테이션 기반의 자바 설정 클래스로도 만들 수 있다. 직전의 AppConfig를 사용했던 방식이 애노테이션 기반의 자바 설정 클래스로 스프링 컨테이너를 만든 것 XML 설정 사용 최근에는 스프링 부트를 많이 사용하면서 XML기반의 설정은 잘 사용하지 않는다. 궁금하면 찾아보자 스프링 컨테이너의 생성 과정 1. 스프링 컨테이너 생성 new AnnotationConfigApplicationContext(AppConfig.class) 스프링 컨테이너를 생성할 때는 구성 정보를 지정해주어야 한다. 여기서는 AppConfig.class를 구성 정보로 지정 2. 스프링 빈 등록 빈 이름은 메서드 이름을 사용한다. 빈 이름을 직접 부여할 수 도 있다. @Bean(name = “memberService2”) *주의: 빈 이름은 항상 다른 이름을 부여해야 함. 같은 이름을 부여하면, 다른 빈이 무시되거나, 기존 빈을 덮어 버리거나 설정에 따라 오류가 발생. 3. 스프링 빈 의존관계 설정 스프링 컨테이너는 설정 정보를 참고해서 의존관계를 주입(DI)한다. 다양한 설정 형식 지원 - 자바 코드, XML 스프링 컨테이너는 다양한 형식의 설정 정보를 받아드릴 수 있게 유연하게 설계되어 있다. 자바코드, XML, Groovy 등등 어떻게 이런 다양한 설정 형식을 지원할까? 그 중심에는 BeanDefinition이라는 추상화가 있다. 조금 더 깊이 있게 들어가보자면 AnnotationConfigApplicationContext 는 AnnotatedBeanDefinitionReader 를 사용해서 AppConfig.class 를 읽고 BeanDefinition 을 생성한다. GenericXmlApplicationContext 는 XmlBeanDefinitionReader 를 사용해서 appConfig.xml 설정정보를 읽고 BeanDefinition 을 생성한다. 새로운 형식의 설정 정보가 추가되면, XxxBeanDefinitionReader를 만들어서 BeanDefinition 을 생성하면 된다. 실무에서 BeanDefinition을 직접 정의하거나 사용할 일은 거의 없으므로 깊게 이해하는 것 보단 이렇게 사용하는 구나 하고 넘어가자. 참고: Spring Core *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring study spring",
    "url": "/study/spring/2022-02-18-Spring(3)/"
  },{
    "title": "Spring의 강력한 무기",
    "text": "이전 글에서 스프링은 다음 기술로 다형성과 OCP/DIP를 가능하게 지원한다고 했다. DI(Dependency Injection): 의존관계, 의존성 주입 DI 컨테이너 제공 기술의 적용 코드로 어떻게 DI라는 기술을 적용하는지 보자 기존의 코드를 변경하기 위해선 구현체에 의존하고 있기 때문에 Memory에서 Jdbc로 변경하기위해서는 클라이언트쪽 코드의 변경이 불가피한 상황 public interface MemberRepository{ ... } public class MemoryMemberRepository implements MemberRepository { ... } public class JdbcMemberRepository implements MemberRepository { ... } public class MemberService { // 기존코드 public class MemberService { private MemberRepository memberRepository = new MemoryMemberRepository(); } // 변경코드 public class MemberService { // private MemberRepository memberRepository = new MemoryMemberRepository(); private MemberRepository memberRepository = new JdbcMemberRepository(); } 그럼 어떻게 해야한다는 거야?? 클라이언트 코드인 MemberService는 MemberRepository의 인터페이스 뿐 아니라 구체 클래스도함께 의존중이다. 그래서 구체 클래스를 변경할 때 클라이언트 코드도 함께 변경해야 한다. DIP 위반 -&gt; 추상에만 의존하도록 변경해보자(인터페이스에만 의존) public class MemberService { private MemberRepository memberRepository; } 인터페이스에만 의존하도록 변경했다. 그런데 구현체가 없는데 어떻게 코드를 실행될까??(지금 상태에 실행하면 NPE(null pointer exception)이 발생한다) AppConfig의 등장 애플리케이션의 전체 동작 방식을 구성(config)하기위해 구현 객체를 생성하고, 연결하는 책임을 가지는 별도의 설정 클래스 생성 @Configuration public class AppConfig{ @Bean public MemberService memberService() { return new MemberService(new MemoryMemberRepository()); } } AppConfig에 설정을 구성한다는 뜻의 @Configuration을 붙임 메서드에 @Bean을 붙여준다. 이렇게 하면 스프링 컨테이너에 스프링 빈으로 등록한다. (스프링 컨테이너는 다음 글에서 알아보자.) AppConfig에 실제 동작에 필요한 구현 객체를 생성 MemoryMemberRepository AppConfig는 생성한 객체 인스턴스의 참조(레퍼런스)를 생성자를 통하여 주입(연결)한다. public class MemberService { private MemberRepository memberRepository; public MemberService(MemberRepository memberRepository) { this.memberRepository = memberRepository; } } 설계 변경으로 MemberService는 MemoryRepository를 의존하지 않고 MemberRepository라는 인터페이스에만 의존 MemberService 입장에서 생성자를 통해 어떤 구현 객체가 들어올지(주입될지) 알 수 없다. MemberService의 생성자를 통해서 어떤 구현 객체를 주입할지는 외부(AppConfig)에서 결정 MemberService는 이제 의존관계에 대한 고민은 외부에 맡기고 실행에만 집중하면 됨 객체의 생성과 연결은 AppConfig가 담당 클라이언트인 MemberService 입장에서 보면 의존관계를 마치 외부에서 주입해주는 것 같다고 해서 DI(Dependency Injection) 우리말로 의존관계 주입 또는 의존성 주입이라 한다. AppConfig의 등장으로 애플리케이션의 크게 사용 영역과, 객체를 생성하고 구성(Configuration)하는 영역으로 분리되었다. IoC, DI, 그리고 컨테이너 제어의 역전 IoC(Inversion of Control) 기존 프로그램은 클라이언트 구현 객체가 스스로 필요한 서버 구현 객체를 생성하고, 연결하고, 실행. 한마디로 구현 객체가 프로그램의 제어 흐름을 스스로 조종했다. 반면에 AppConfig가 등장한 이후 구현 객체는 자신의 로직을 실행하는 역활만 담당하고 프로그램의 제어 흐름은 AppConfig가 담당한다. 이렇게 프로그램의 제어 흐름을 직접 제어하는 것이 아니라 외부에서 관리하는 것을 제어의 역전(IoC)이라 한다. IoC 컨테이너, DI 컨테이너 AppConfig처럼 객체를 생성하고 관리하면서 의존관계를 연결해 주는 것을 IOC 컨테이너 또는 Di 컨테이너라 한다. 프레임워크 vs 라이브러리 내가 작성한 코드를 제어하고, 대신 실행하면 그것은 프레임워크 반면에 내가 작성한 코드가 직접 제어의 흐름을 담당한다면 그것은 라이브러리 참고: Spring Core *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring study spring",
    "url": "/study/spring/2022-02-10-Spring(2)/"
  },{
    "title": "Spring Security를 구현해보자",
    "text": "이전 글에서 Spring Security 과정을 이해해보았다. 이번에는 어떻게 적용을 하는지 직접 구현해보면서 알아보자. 이전 글에서 구현하는 방법에 간단하게 구현하는 방법과 직접 AuthenticationProvider을 커스텀하는 방법이 있다고 했는데 먼저 기본적인 방법부터 알아보자. Standard Method (DaoAuthenticationProvider) 표준적이고 가장 일반적인 구현방법은 AuthenticationProvider에 DaoAuthenticationProvider을 사용하는 방법인데 따로 Provider을 구현을 할필요가 없기 때문에 사실상 SecurityConfig 설정하고, Userdetails, UserdetailsService만 구현하면된다. 0. 설정 셋팅 build.gradle에 dependency 추가 dependencies { implementation 'org.springframework.boot:spring-boot-starter-data-jpa' implementation 'org.springframework.boot:spring-boot-starter-security' implementation 'org.springframework.boot:spring-boot-starter-thymeleaf' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.thymeleaf.extras:thymeleaf-extras-springsecurity5' compileOnly 'org.projectlombok:lombok' annotationProcessor 'org.projectlombok:lombok' testImplementation 'org.springframework.boot:spring-boot-starter-test' testImplementation 'org.springframework.security:spring-security-test' } SpringSecurity에 대한 기본적인 설정 추가. @Configuration @EnableWebSecurity public class SecurityConfig extends WebSecurityConfigurerAdapter { // 정적 자원에 대해서는 Security 설정을 적용하지 않음. @Override public void configure(WebSecurity web) { web.ignoring().requestMatchers(PathRequest.toStaticResources().atCommonLocations()); } @Override protected void configure(HttpSecurity http) throws Exception { http.csrf().disable(); http.authorizeRequests() .antMatchers(\"/user/**\").authenticated() // /user/** 요청에 대해 로그인 요구 .antMatchers(\"/admin/**\").access(\"hasRole('ROLE_ADMIN')\") // /admin/** 요청에 대해 ROLE_ADMIN 역활을 가지고 있어야 함 .anyRequest().permitAll() // 나머지 요청은 로그인 요구x .and() .formLogin() //form 기반 로그인 설정 .loginProcessingUrl(\"/login\") // 로그인 form의 action에 들어갈 처리 URI .loginPage(\"/loginForm\") //로그인 페이지 설정 .defaultSuccessUrl(\"/\"); //로그인 성공시 URL } } //PasswordEncoder @Bean public BCryptPasswordEncoder encoderPwd() { return new BCryptPasswordEncoder(); } 코드만 적으면 이해하기가 어려울테니 순서에 맞춰서 과정을 설명하며 코드를 구현해보겠다.(구현관련해서만 설명하니 역활 설명과 상세한 과정 설명은 이전 글을 보자!) 1. 로그인 요청 사용자는 아이디와 비밀번호를 입력해서 로그인 요청을 한다. 이번 예제는 Form 기반으로 요청하는 상황. 2. UserPasswordAuthenticationToken 발급 전송이 오면 AuthenticationFilter이 id, password를 가로채 UserPasswordAuthenticationToken 발급. 여기서 유효성을 검사 하기위해 필터를 커스텀해서 추가시킬수 있지만 복잡해지므로 생략한다. 나중에 필요할 때 추가하면 된다. 3. UsernamePasswordToken을 Authentication Manager에게 전달 AuthenticationFilter은 생성한 UsernamePasswordToken을 Authentication Manager에게 전달한다. 4. UsernamePasswordToken을 Authentication Provider에게 전달 AuthenticationManager는 전달받은 UsernamePasswordToken을 AuthenticationProvider에게 전달하여 실제 인증 과정을 수행한다. AuthenticationProvider은 입력한 값(token에서 꺼낼수 있음)과 5,6,7 과정을 통해 가져온 값(DB에서 가져온 것)을 비교한다. 이 부분이 Custom을 한 구현방법과 차이나는 부분인데 이 방법에선 AuthenticationProvider을 따로 구현할필요가 없다. (AuthenticationProvider에 DaoAuthenticationProvider이 사용된다) 5, 6, 7. DB에서 User정보 UserDetailsService를 통해 UserDetails형태로 가져오기 따로 설명하면 더 헷갈리므로 묶어서 설명하겠다. AuthenticationProvider에서 아이디를 조회하였으면 UserDetailsService로부터 아이디를 기반으로 데이터를 조회해야 한다. UserDetailService는 인터페이스이기 때문에 이를 구현한 클래스를 작성해야 한다. @RequiredArgsConstructor @Service public class UserDetailsServiceImpl implements UserDetailsService { private final UserRepository userRepository; @Override public UserDetails loadUserByUsername(String username) { User user = userRepository.findByUsername(username); if (user == null) { return null; } return new UserDetailsImpl(user); } } UserDetailsServiceImpl의 반환형이 UserDetails이다. User를 담을 UserDetails을 구현해보자 @Data public class UserDetailsImpl implements UserDetails { private User user; public UserDetailsImpl(User user) { this.user = user; } //해당 User의 권한을 리턴하는 곳! @Override public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() { Collection&lt;GrantedAuthority&gt; collection = new ArrayList&lt;&gt;(); collection.add(new GrantedAuthority() { @Override public String getAuthority() { return user.getRole(); } }); return collection; } @Override public String getPassword() { return user.getPassword(); } @Override public String getUsername() { return user.getUsername(); } @Override public boolean isAccountNonExpired() { return true; } @Override public boolean isAccountNonLocked() { return true; } @Override public boolean isCredentialsNonExpired() { return true; } @Override public boolean isEnabled() { return true; } } 8. 인증 처리 후 인증된 토큰을 Authentication Manager에게 반환 이제 AuthenticationProvider(현재 구현체 - DaoAuthenticationProvider)에서 UserDetailsServices를 통해 얻어낸 UserDetails와 입력으로 부터 들어온 비밀번호를 PasswordEncoder를 통해 암호화한 것과 비교하여 유효성을 확인하고 Authentication을 반환해준다. 9. 인증된 토큰을 AuthenticationFilter에게 전달 AuthenticationProvider에서 받은 Authentication을 AuthenticationFilter에게 반환 10. 인증된 토큰을 SecurityContextHolder에 저장 Authentication객체를 SecurityContextHolder에 저장하면 인증이 끝난다. github 코드 Custom Method 위에서 기본적인 방법을 알아봤으니 AuthenticationProvider을 직접 Custom하는 방식을 알아보자. 외부, 타사 서비스(예를들어 Crowd) 같은 것에 대해 인증하기위해서는 Custom한 Authentication Provider를 구현해야한다. Standard방식에서 추가적으로 CustomAuthenticationProvider를 구현하고 Config에 등록만 하면 된다. 4. UsernamePasswordToken을 AuthenticationProvider에게 전달 Standard 방식에서 4, 8번 과정인 AuthenticationProvider 부분만 달라지고 나머지 부분은 윗 부분과 동일하다. AuthenticationManager는 전달받은 UsernamePasswordToken을 AuthenticationProvider에게 전달하여 실제 인증 과정을 수행하며, 실제 인증에 대한 부분은 authenticate 함수에 작성하면 된다. Spring Security에서는 Username으로 DB에서 데이터를 조회한다음, 비밀번호의 일치 여부를 검사하는 방식으로 작동. public class CustomAuthenticationProvider implements AuthenticationProvider { @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { UsernamePasswordAuthenticationToken token = (UsernamePasswordAuthenticationToken) authentication; //입력한 ID, Password 조회 String userEmail = token.getName(); String userPw = (String)token.getCredentials(); // 아래 코드는 8번에서 설명 ... } @Override public boolean supports(Class&lt;?&gt; authentication) { return authentication.equals(UsernamePasswordAuthenticationToken.class); } } 8. 인증 처리 후 인증된 토큰을 AuthenticationManager에게 반환 5, 6, 7 방법으로 UserDetetailsService를 통해 DB에서 조회한 유저 정보와 입력받은 비밀번호가 일치하는지 확인하고, 일치하면 인증된 토큰을 생성하여 반환해주어야 한다. DB에 저장된 유저 비밀번호는 암호화 되어있기 때문에, 입력된 비밀번호를 PasswordEncoder를 통해 암호화하여 DB에서 조회한 사용자의 비밀번호와 매칭되는지 확인한다. @RequiredArgsConstructor public class CustomAuthenticationProvider implements AuthenticationProvider { private final UserDetailsService userDetailsService; private final BCryptPasswordEncoder passwordEncoder; @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { UsernamePasswordAuthenticationToken token = (UsernamePasswordAuthenticationToken) authentication; //입력한 ID, Password 조회 String userId = token.getName(); String userPw = (String)token.getCredentials(); //UserDetailsService를 통해 DB에서 조회한 사용자 UserDetailsImpl dbUser = (UserDetailsImpl) userDetailsService.loadUserByUsername(userId); // 비밀번호 매칭되는지 확인 if (!passwordEncoder.matches(userPw, dbUser.getPassword())) { throw new BadCredentialsException(dbUser.getUsername() + \"Invalid password\"); } return new UsernamePasswordAuthenticationToken(dbUser, userPw, dbUser.getAuthorities()); } @Override public boolean supports(Class&lt;?&gt; authentication) { return authentication.equals(UsernamePasswordAuthenticationToken.class); } } 위와 같이 완성된 CustomAuthenticationProvider를 SecurityConfig에 Bean으로 등록해주고 AuthenticationManager에 넣어주자. @Configuration @EnableWebSecurity @RequiredArgsConstructor public class SecurityConfig extends WebSecurityConfigurerAdapter { private final UserDetailsService userDetailsService; @Bean public BCryptPasswordEncoder encoderPwd() { return new BCryptPasswordEncoder(); } // CustomAuthenticationProvider 빈 등록 @Bean public CustomAuthenticationProvider customAuthenticationProvider() { return new CustomAuthenticationProvider(userDetailsService, encoderPwd()); } @Override public void configure(WebSecurity web) throws Exception { web.ignoring(). requestMatchers(PathRequest.toStaticResources().atCommonLocations()); } @Override protected void configure(HttpSecurity http) throws Exception { http.csrf().disable(); http.authorizeRequests() .antMatchers(\"/user/**\").authenticated() .antMatchers(\"/admin/**\").access(\"hasRole('ROLE_ADMIN')\") .anyRequest().permitAll() .and() .formLogin() .loginProcessingUrl(\"/login\") .loginPage(\"/loginForm\") .defaultSuccessUrl(\"/\"); } //AuthenticationManager에 Provider등록 @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.authenticationProvider(customAuthenticationProvider()); } } 로그인에 성공하고나면 SecurityContextHolder라는 세션을 활용해 로그인이 유지된다. github 코드 참고 https://www.baeldung.com/spring-security-authentication-provider https://docs.spring.io/spring-security/reference/servlet/authentication/architecture.html#servlet-authentication-authenticationmanager https://docs.spring.io/spring-security/reference/servlet/authentication/passwords/dao-authentication-provider.html#servlet-authentication-daoauthenticationprovider https://docs.spring.io/spring-security/site/docs/current/api/org/springframework/security/authentication/dao/DaoAuthenticationProvider.html https://mangkyu.tistory.com/77 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "study spring security",
    "url": "/study/spring%20security/2022-02-06-Spring-Security(2)/"
  },{
    "title": "Spring Security 과정을 이해해보자",
    "text": "인터넷을 보고 프로젝트에 Spring Security를 적용시켜봐도 UserDetails, Principal, AuthenticationProvider 등등.. 이게 도대체 무슨 말이야? 도무지 이해가 가지않았던 스프링 시큐리티. 동작 과정을 처음부터 상세하게 이해하고 적용해보자. 처음 보면 어려운게 당연하니 반복해서 학습하자. +구현하는 방법에는 간단하게 구현하는 것(AuthenticationProvider 직접 구현x)도 있고 직접 커스텀해서 하는 방법 (AuthenticationProvider를 직접 구현)도 있기 때문에 그런 디테일한 부분들은 다음 글에서 설명하겠다. Spring Security? Spring Security는 Java 애플리케이션의 인증과 권한 부여를 제공하는 데 중점을 둔 프레임워크. 보안과 관련해서 많은 기능을 제공해주기 때문에 개발자가 직접 보안 관련 로직을 작성하지 않아도 되는 장점이 있다. Architecture 아래 그림은 Spring Security Architecture이다. (더 이해하기 쉽도록 최대한 숫자에 맞춰 과정을 적어봤다 + 역활 설명) 사용자 로그인을 하면 id, password가 Request에 담아져 보내진다. AuthenticationFilter에서 request가 보낸 id, password를 가로채 인증용 객체(UsernamePasswordAuthenticationToken)로 만든다. 인증을 담당할 AuthenticationManager 인터페이스(구현체 - ProviderManager)에게 인증용 객체를 준다. 실제 인증을 할 AuthenticationProvider에게 다시 인증용 객체를 전달한다. 인증 절차가 시작되면 AuthenticationProvider 인터페이스가 실행 -&gt; DB에 있는 이용자의 정보와 화면에서 입력한 로그인 정보 비교 AuthenticationProvider 인터페이스에서는 authenticate() 메소드를 오버라이딩 하게 되는데 이 메소드의 파라미터인 Authentication으로 화면에서 입력한 로그인 정보를 가져올 수 있다. AuthenticationProvider 인터페이스에서 DB에 있는 이용자의 정보를 가져오려면, UserDetailsService 인터페이스를 사용. UserDetailsService 인터페이스는 화면에서 입력한 이용자의 id(username)를 가지고 loadUserByUsername() 메소드를 호출하여 DB에 있는 이용자의 정보를 UserDetails 형으로 가져온다. 만약 이용자가 존재하지 않으면 예외를 던진다. (UserDetails를 User와 Authentication 사이를 채워주는 Adaptor라고 생각하자) 5,6,7을 통해 가져온 정보(DB를 통해 가져온 이용자정보, 화면에서 입력한 이용자 정보)를 비교하고, 일치하면 Authentication 참조를 리턴하고, 일치 하지 않으면 예외를 던진다. -&gt; 인증이 완료되면 사용자 정보를 가진 Authentication 객체를 SecurityContextHolder에 담은 이후 AuthenticationSuccessHandler을 실행.(실패시 AuthenticationFailureHandler 실행) AuthenticationFilter 설정된 로그인 URL로 오는 요청을 감시하며, 유저 인증 처리 AuthenticationManager를 통한 인증 실행 인증 성공 시 얻은 Authentication 객체를 SecurityContext에 저장 후 AuthenticationSuccessHandler 실행 인증 실패시, AuthenticationFailureHandler 실행 UsernamePasswordAuthenticationToken 사용자의 id가 Principal 역활을 하고, password가 Credential의 역활을 한다. 첫번째 생성자는 인증 전의 객체를 생성한다. 두번째 생성자는 인증이 완료된 객체를 생성한다. public class UsernamePasswordAuthenticationToken extends AbstractAuthenticationToken { // 주로 사용자의 ID에 해당함 private final Object principal; // 주로 사용자의 PW에 해당함 private Object credentials; // 인증 완료 전의 객체 생성 public UsernamePasswordAuthenticationToken(Object principal, Object credentials) { super(null); this.principal = principal; this.credentials = credentials; setAuthenticated(false); } // 인증 완료 후의 객체 생성 public UsernamePasswordAuthenticationToken(Object principal, Object credentials, Collection&lt;? extends GrantedAuthority&gt; authorities) { super(authorities); this.principal = principal; this.credentials = credentials; super.setAuthenticated(true); // must use super, as we override } } } public abstract class AbstractAuthenticationToken implements Authentication, CredentialsContainer { } AuthenticationManager 인증에 대한 부분은 SpringSecurity의 AuthenticationManager를 통해 처리하게 되는데, 실질적으로는 AuthenticationManager에 등록된 AuthenticationProvider에 의해 처리된다. public interface AuthenticationManager { Authentication authenticate(Authentication authentication) throws AuthenticationException; } AuthenticationManger(구현체 - ProviderManager)와 AuthenticationProvider가 헷갈리면 이렇게 생각 해보자. AuthenticationManager가 상급자고 AuthenticationProvider가 부하직원이라고 생각하고 상급자가 부하직원에게 인증이란 일을 시킨다고 생각하면 된다. AuthenticationManger를 구현한 ProviderManager는 실제 인증 과정에 대한 로직을 가지고 있는 AuthenticationProvider를 List로 가지고 있으며, for문을 통해 모든 provider를 조회하면서 authenticate 처리를 한다. public class ProviderManager implements AuthenticationManager, MessageSourceAware, InitializingBean { public List&lt;AuthenticationProvider&gt; getProviders() { return providers; } public Authentication authenticate(Authentication authentication) throws AuthenticationException { Class&lt;? extends Authentication&gt; toTest = authentication.getClass(); AuthenticationException lastException = null; Authentication result = null; boolean debug = logger.isDebugEnabled(); //for문으로 모든 provider를 순회하여 처리하고 result가 나올 때까지 반복한다. for (AuthenticationProvider provider : getProviders()) { .... try { result = provider.authenticate(authentication); if (result != null) { copyDetails(authentication, result); break; } } catch (AccountStatusException e) { prepareException(e, authentication); // SEC-546: Avoid polling additional providers if auth failure is due to // invalid account status throw e; } .... } throw lastException; } } AuthenticationProvider를 직접 커스텀해서 만든 경우 AuthenticationManger에 등록 하는 방법은 WebSecurityConfigurerAdapter를 상속해 만든 SecurityConfig에서 할 수 있다. @Configuration @EnableWebSecurity public class SecurityConfig extends WebSecurityConfigurerAdapter { @Bean public AuthenticationManager getAuthenticationManager() throws Exception { return super.authenticationManagerBean(); } @Bean public CustomAuthenticationProvider customAuthenticationProvider() throws Exception { return new CustomAuthenticationProvider(); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.authenticationProvider(customAuthenticationProvider()); } } AuthenticationProvider AuthenticationProvider에서는 실제 인증에 대한 부분을 처리하는데, 인증 전의 인증용 객체를 받아서 5,6,7,8 과정을 거쳐서 인증이 완료된 객체를 반환하는 역활은 한다. 아래와 같은 AuthenticationProvider 인터페이스를 구현해서 Custom한 AuthenticationProvider을 작성해서 바로 위에 설명한 방법처럼 AuthenticationManager에 등록하면 된다. public interface AuthenticationProvider { // 인증 전의 Authenticaion 객체를 받아서 인증된 Authentication 객체를 반환 Authentication authenticate(Authentication var1) throws AuthenticationException; boolean supports(Class&lt;?&gt; var1); } 커스텀하고싶으면 밑에 형식처럼 원하는 부분을 구현하면 된다. 아래를 보면 5,6,7,8번 과정이 모두 일어나는걸 볼 수 있다. public class CustomAuthenticationProvider implements AuthenticationProvider{ @Autowired private UserDetailsService userDetailsService; @SuppressWarnings(\"unchecked\") @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { // AuthenticaionFilter에서 생성된 토큰으로부터 아이디와 비밀번호를 조회함 String username = (String) authentication.getPrincipal(); String password = (String) authentication.getCredentials(); // UserDetailsService를 통해 DB에서 아이디로 사용자 조회 CustomUserDetails user = (CustomUserDetails) userDetailsService.loadUserByUsername(username); //조회한 것들 비교 if(!matchPassword(password, user.getPassword())) { throw new BadCredentialsException(username); } if(!user.isEnabled()) { throw new BadCredentialsException(username); } return new UsernamePasswordAuthenticationToken(username, password, user.getAuthorities()); } @Override public boolean supports(Class&lt;?&gt; authentication) { return true; } private boolean matchPassword(String loginPwd, String password) { return loginPwd.equals(password); } } Authentication Authentication은 현재 접근하는 주체의 정보와 권한을 담는 인터페이스. Authentication 객체는 SecurityContext에 저장되며 SecurityContextHolder를 통해 SecurityContext에 접근하고 SecurityContext를 통해 Authentication에 접근 할 수 있다. public interface Authentication extends Principal, Serializable { // 현재 사용자의 권한 목록을 가져옴 Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); // credentials(주로 비밀번호)을 가져옴 Object getCredentials(); Object getDetails(); // Principal 객체를 가져옴. Object getPrincipal(); // 인증 여부를 가져옴 boolean isAuthenticated(); // 인증 여부를 설정함 void setAuthenticated(boolean isAuthenticated) throws IllegalArgumentException; } UserDetailsService UserDetailsService 인터페이스는 DB에서 유저 정보를 가져오는 역활 public interface UserDetailsService { UserDetails loadUserByUsername(String var1) throws UsernameNotFoundException; } UserDetails 사용자의 정보를 담는 인터페이스, 구현해서 사용하면 됨 public interface UserDetails extends Serializable { Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); String getPassword(); String getUsername(); boolean isAccountNonExpired(); boolean isAccountNonLocked(); boolean isCredentialsNonExpired(); boolean isEnabled(); } SecurityContextHolder SecurityContextHolder는 보안 주체의 세부 정보를 포함하여 응용프로그램의 현재 보안 컨텍스트에 대한 세부 정보가 저장. SecurityContext는 Authentication을 보관하는 역활을 하며, SecurityContext를 통해 Authentication 객체를 꺼내올 수 있다. 이론 설명은 여기까지입니다. 최대한 쉽게 풀어 쓸려고 말을 많이 붙이다 보니 길어졌는 데 도움이 됐는지 모르겠네요ㅜㅜ 다음 글에서는 구현 과정을 설명하겠습니다! 참고 Spring-Security https://mangkyu.tistory.com/76 https://velog.io/@hellas4/Security-%EA%B8%B0%EB%B3%B8-%EC%9B%90%EB%A6%AC-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0-%EC%9D%B4%EB%A1%A0%ED%8E%B8 *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "study spring security",
    "url": "/study/spring%20security/2022-01-29-Spring-Security(1)/"
  },{
    "title": "왜 Spring을 사용할까?",
    "text": "Spring은 왜 만들었나? 스프링은 자바 언어 기반의 프레임워크 자바 언어의 가장 큰 특징 - 객체 지향 언어 스프링은 객체 지향 언어가 가진 특징을 가장 잘 살려내는 프레임워크 스프링은 좋은 객체 지향 애플리케이션을 개발할 수 있게 도와주는 프레임워크 tmi: 옛날 옛적 EJB(Enterprise Java Beans)라는 기술이 있었지만 너무 어렵고 복잡하고 느렸다. 그래서 더 단순하고 사용하기 좋게만들어서 Spring을 만들게 됬는데… 더 궁금하면 검색해보자 객체 지향 프로그래밍? 자꾸 객체 지향 프로그래밍이라는데 그래서 그게 뭐야?? 객체 지향 프로그래밍은 컴퓨터 프로그램을 명령어의 목록으로 보는 시각에서 벗어나 여러개의 독립된 단위, 즉 개체들의 모임으로 파악하는 것이다. 객체 지향 프로그래밍은 프로그램을 유연하고 변경에 용이하게 만들기 때문에 대규모 소프트웨어 개발에 많이 사용된다. 다형성(Polymorphism) 객체 지향 프로그래밍 특징 중 다형성을 알아보자 다형성이란? 다형성이란 하나의 객체가 여러 가지 타입을 가질 수 있는 것을 의미한다 자동차로 비유를 들어보자 우선 역활과 구현으로 세상을 구분할 필요가 있다. 자동차라는 역활(인터페이스)이 있고 그걸 구현한 K3, 아반떼, 테슬라 모델3(구현체)들이 있다. 생각해보자 만약에 운전자가 자동차가 새로 나올때마다 그 자동차의 운전방식을 알아야 된다면 얼마나 불편한가.. 하지만 위와 같이 설계를 함으로 써 K3, 아반떼, 테슬라 모델3 각각의 운전 방식을 알 필요가 없고 자동차라는 역활의 운전 방식만 알면 된다! 역활과 구현을 분리 역활과 구현으로 구분하면 세상이 단순해지고, 유연해지며 변경도 편리해진다. 장점 클라이언트는 대상의 역활(인터페이스)만 알면 된다. 클라이언트는 구현 대상의 내부 구조를 몰라도 된다. 클라이언트는 구현 대상의 내부 구조가 변경되어도 영향을 받지 않는다. 클라이언트는 구현 대상 자체를 변경해도 영향을 받지 않는다. 자 이걸 자바 언어에서는 어떻게 사용할 수 있을까? 자바 언어의 다형성을 활용 역활 = 인터페이스 구현 = 인터페이스를 구현한 클래스, 구현 객체 MemberRepository interface의 메서드를 각 MemoryMemberRepository, JdbcMemberRepository에 오버라이딩 다형성으로 인터페이스를 구현한 객체를 실행 시점에 유연하게 변경 (MemoryMemberRepository or JdbcMemberRepository) 다형성의 본질 인터페이스를 구현한 객체 인스턴스를 실행 시점에 유연하게 변경할 수 있다. 즉, 클라이언트를 변경하지 않고, 서버의 구현 기능을 유연하게 변경할 수 있다. 확장 가능한 설계 -&gt; 인터페이스를 안정적으로 잘 설계하는 것이 중요 스프링과 객체 지향 스프링은 다형성을 극대화해서 이용할 수 있게 도와준다. 스프링에서 이야기하는 제어의 역전(IoC), 의존관계 주입(DI)은 다형성을 활용해서 역활과 구현을 편리하게 다룰 수 있도록 도와준다. 제어의 역전과 의존관계 주입을 몰라도 걱정하지 말자. 뒷 post들에서 설명할테니 스프링과 객체 지향 설계에 대해 제대로 이해하려면 다형성 외에 한가지 더 알아야 된다. 바로 SOLID… 면접에도 자주 나온다고 하니 잘 공부해두자 좋은 객체 지향 설계의 5가지 원칙(SOLID) 클린코드로 유명한 로버트 마틴이 좋은 객체 지향 설계의 5가지 원칙을 정리 SRP: 단일 책임 원칙(Single Responsibility Principle) OCP: 개방-폐쇄 원칙(Open/Closed Principle) LSP: 리스코프 치환 원칙(Liskov Substitution Principle) ISP: 인터페이스 분리 원칙(Interface segregation Principle) DIP: 의존관계 역전 원칙(Dependency Inversion Principle) SRP 단일 책임 원칙(Single Responsibility Principle) 하나의 클래스는 하나의 책임만 가져야 한다. 그런데 하나의 책임이라는 것은 너무 모호하다. 클 수도 있고, 작을 수 도 있다. 문맥과 상황에 따라 다르다. 중요한 기준은 변경이다. 변경이 있을때 파급 효과가 적으면 단일 책임 원칙을 잘 따르는 것 예) UI 변경, 객체의 생성과 사용을 분리 OCP 개방 폐쇄 원칙(Open/Closed Principle) 소프트웨어 요소는 확장에는 열려 있으나 변경에는 닫혀 있어야 한다. 그런데 언뜻 생각하면 확장을 하려면 당연히 기존코드를 변경해야 확장을 하지 아니면 어떻게 변경해?? 라고 이런 모순적인 말이 어딨나 생각이 든다. 다형성을 잘 활용하면 이 개방 폐쇄 원칙을 지킬 수 있다. 지금까지 배운 역활과 구현의 분리를 써서 인터페이스를 구현한 새로운 클래스를 하나 만들어서 새로운 기능을 구현 예를들어 MemberRepository를 구현한 MemoryMemberRepository만 있다고 할때 거기에 JdbcMemberRepository를 새로 구현하더라도 전혀 기존의 코드를 변경하지 않고 확장을 할 수 있다. 그런데 문제점이 있다 // 기존코드 public class MemberService { private MemberRepository memberRepository = new MemoryMemberRepository(); } // 변경코드 public class MemberService { // private MemberRepository memberRepository = new MemoryMemberRepository(); private MemberRepository memberRepository = new JdbcMemberRepository(); } MemberService에서 구현 클래스를 직접 선택할 때 MemberRepository m = new MemoryMemberRepository(); //기존 코드 MemberRepository m = new JdbcMemberRepository(); //변경 코드 구현 객체를 변경하려면 기존의 코드를 변경해야 한다. 분명 다형성을 사용했지만 OCP 원칙을 지킬 수 없다. 어떻게 해결? 객체를 생성하고, 별도의 연관관계를 맺어주는 설정자가 필요 그게 바로 Spring! (어떻게 적용하는지는 뒤로가면서 천천히 알아보자) LSP 리스코프 치환 원칙(Liskov Substitution Principle) 프로그램의 객체는 프로그램의 정확성을 깨뜨리지 않으면서 하위의 인스턴스로 변경이 가능해야 한다. 다형성에서 하위 클래스는 인터페이스 규약을 다 지켜야 한다는 것, 다형성을 지원하기 위한 원칙, 인터페이스를 구현한 구현체는 믿고 사용하려면, 이 원칙이 필요 예) 자동차 인터페이스의 엑셀은 앞으로 가라는 기능, 뒤로 가게 구현하면 LSP 위반, 느리더라도 앞으로 가야한다. ISP 인터페이스 분리 원칙(Interface Segregation Principle) 특정 클라이언트를 위한 인터페이스 여러개가 범용의 인터페이스 하나보다 낫다 자동차 인터페이스 -&gt; 운전 인터페이스, 정비 인터페이스 사용자 클라이언트 -&gt; 운전자 클라이언트, 정비사 클라이언트 분리하면 정비 인터페이스 자체가 변해도 운전자 클라이언트에 영향을 주지 않음 인터페이스가 더 명확해지고 대체 가능성이 높아진다. DIP 의존관계 역전 원칙(Dependency Inversion Principle) 프로그래머는 추상화에 의존해야지 구현체에 의존하면 안된다. 쉽게 말해서 구현 클래스에 의존하지 말고 인터페이스에 의존하라는 뜻 클라이언트가 인터페이스에 의존해야 유연하게 구현체를 변경할 수 있다. 구현체에 의존하게 되면 변경이 매우 어려워짐. 그런데 OCP에서 설명한 MemberService는 인터페이스에 의존하지만, 구현 클래스도 동시에 의존한다. (의존한다는 것은 내가 저 코드를 안다는 것) DIP 위반 -&gt; 객체 지향의 핵심은 다형성이라는데 다형성 만으로는 OCP, DIP를 지킬 수 없다. 뭔가 더 필요하다… 객체 지향 설계와 스프링 스프링 공부하러 왔는데 스프링 얘기는 언제해..? 지금까지는 스프링을 위한 빌드업이라 보면 된다. 스프링은 다음 기술로 다형성 + OCP/DIP를 가능하게 지원 DI(Dipendency Injection): 의존관계, 의존성 주입 DI 컨테이너 제공 클라이언트의 코드 변경 없이 기능 확장 빌드업은 끝났으니 다음 글부터 본격적으로 설명하겠다. 정리 모든 설계에 역활과 구현을 분리하자. 이상적으로는 모든 설계에 인터페이스를 부여하자 하지만 실무에서는 굉장히 고민이 되는게 인터페이스를 도입하면 추상화라는 비용이 발생한다. 기능을 확장할 가능성이 없다면, 구체 클래스를 직접 사용하고, 향후 꼭 필요할 때 리팩터링해서 인터페이스를 도입하는 것도 방법이다. 참고: Spring Core *틀린 부분이 있으면 언제든지 말씀해 주시면 공부해서 수정하겠습니다.",
    "tags": "Java Spring study spring",
    "url": "/study/spring/2022-01-26-Spring(1)/"
  }]};
